{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9713bd72-4d21-4068-9b4c-c68a6f7b560f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.24.1)\n",
      "Collecting pyarrow>=12.0.0 (from datasets)\n",
      "  Downloading pyarrow-15.0.2-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Downloading pandas-2.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
      "Collecting tqdm>=4.62.1 (from datasets)\n",
      "  Downloading tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2023.4.0)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
      "Collecting huggingface-hub>=0.19.4 (from datasets)\n",
      "  Downloading huggingface_hub-0.22.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "INFO: pip is looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting huggingface-hub>=0.19.4 (from datasets)\n",
      "  Downloading huggingface_hub-0.22.1-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.22.0-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.21.4-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.21.3-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.21.2-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.21.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.21.0-py3-none-any.whl.metadata (13 kB)\n",
      "INFO: pip is still looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading huggingface_hub-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.20.2-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.20.1-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.20.0-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "Collecting fsspec[http]<=2024.2.0,>=2023.1.0 (from datasets)\n",
      "  Downloading fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.9/170.9 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-15.0.2-cp310-cp310-manylinux_2_28_x86_64.whl (38.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.3/38.3 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.5/239.5 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m505.5/505.5 kB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.6/301.6 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytz, xxhash, tzdata, tqdm, pyarrow-hotfix, pyarrow, multidict, fsspec, frozenlist, dill, async-timeout, yarl, pandas, multiprocess, huggingface-hub, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "Successfully installed aiohttp-3.9.3 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.18.0 dill-0.3.8 frozenlist-1.4.1 fsspec-2024.2.0 huggingface-hub-0.22.2 multidict-6.0.5 multiprocess-0.70.16 pandas-2.2.1 pyarrow-15.0.2 pyarrow-hotfix-0.6 pytz-2024.1 tqdm-4.66.2 tzdata-2024.1 xxhash-3.4.1 yarl-1.9.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daa01040-5d6f-4d48-801e-4f8b6362c71d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16a0a86bc6a2439c98f68bef0a00dd76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/5.16k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 120M/120M [00:01<00:00, 67.5MB/s] \n",
      "Downloading data: 100%|██████████| 23.9M/23.9M [00:00<00:00, 57.6MB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d20cefc48874c54ae8f6802fd4d9b68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4da86cbc5954a0d8991e5d24d20149f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "cifar10_ds = load_dataset(\"cifar10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f798c36-75f9-4622-ae0c-a4a459380ddd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting contrastive\n",
      "  Downloading contrastive-1.2.0.tar.gz (9.0 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from contrastive) (1.24.1)\n",
      "Collecting scikit-learn (from contrastive)\n",
      "  Downloading scikit_learn-1.4.1.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting matplotlib (from contrastive)\n",
      "  Downloading matplotlib-3.8.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->contrastive)\n",
      "  Downloading contourpy-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->contrastive)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->contrastive)\n",
      "  Downloading fonttools-4.50.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (159 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.4/159.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib->contrastive)\n",
      "  Downloading kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->contrastive) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib->contrastive) (9.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib->contrastive) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->contrastive) (2.8.2)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn->contrastive)\n",
      "  Downloading scipy-1.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.2.0 (from scikit-learn->contrastive)\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn->contrastive)\n",
      "  Downloading threadpoolctl-3.4.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib->contrastive) (1.16.0)\n",
      "Downloading matplotlib-3.8.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.4.1.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (305 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.2/305.2 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.50.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.4.0-py3-none-any.whl (17 kB)\n",
      "Building wheels for collected packages: contrastive\n",
      "  Building wheel for contrastive (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for contrastive: filename=contrastive-1.2.0-py3-none-any.whl size=6899 sha256=9533409bfb4627e701e409c37f1822f0ea7cd419ba3ca5cdc64261f2542d31a2\n",
      "  Stored in directory: /root/.cache/pip/wheels/b4/2a/ad/111239c0d6a0b248f0ee4b903b6b44cc9cdfc1cfcff81ebba2\n",
      "Successfully built contrastive\n",
      "Installing collected packages: threadpoolctl, scipy, kiwisolver, joblib, fonttools, cycler, contourpy, scikit-learn, matplotlib, contrastive\n",
      "Successfully installed contourpy-1.2.1 contrastive-1.2.0 cycler-0.12.1 fonttools-4.50.0 joblib-1.3.2 kiwisolver-1.4.5 matplotlib-3.8.3 scikit-learn-1.4.1.post1 scipy-1.13.0 threadpoolctl-3.4.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install contrastive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3581d7f1-7263-4121-8da4-dde4ac7c7d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['img', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['img', 'label'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(cifar10_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "538cabd4-306a-49d4-b3ee-08416214594f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d25529df-0494-425f-8e19-369b478f6a6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=32x32>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = 10\n",
    "cifar10_ds[\"train\"][index][\"img\"].show()\n",
    "cifar10_ds[\"train\"][index][\"img\"].size\n",
    "cifar10_ds[\"train\"][index][\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cebec81f-7d58-4e3a-a2ae-0776bde09300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3072,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(cifar10_ds[\"train\"][index][\"img\"]).flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b8ac0d3-c7ce-4578-a0e2-7c65969677c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0478b98-05e6-4883-8ec6-741cf07079ee",
   "metadata": {},
   "source": [
    "### Demo Quickstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8574e6f4-a37d-4998-b0b8-fde06837faa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(391, 3072)\n",
      "(391,)\n",
      "(1609, 3072)\n"
     ]
    }
   ],
   "source": [
    "X_target = list()\n",
    "X_label = list()\n",
    "X_background = list()\n",
    "TARGET_LABEL_LIST = [0, 1]\n",
    "for i in random.sample(range(10000), 2000):\n",
    "    label = cifar10_ds[\"train\"][i][\"label\"]\n",
    "    img_flat = np.asarray(cifar10_ds[\"train\"][i][\"img\"]).flatten()\n",
    "    if label in TARGET_LABEL_LIST:\n",
    "        X_target.append(img_flat)\n",
    "        X_label.append(label)\n",
    "    else:\n",
    "        X_background.append(img_flat)\n",
    "\n",
    "X_target = np.asarray(X_target)\n",
    "X_label = np.asarray(X_label)\n",
    "X_background = np.asarray(X_background)\n",
    "\n",
    "print(X_target.shape)\n",
    "print(X_label.shape)\n",
    "print(X_background.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dba91d9-aa2f-45d3-b84f-eeab63318104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(391, 2)\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_target)\n",
    "print(X_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7b5cf3e-fdd5-484f-88e8-7fc6e46875de",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcontrastive\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CPCA\n\u001b[1;32m      3\u001b[0m cpca \u001b[38;5;241m=\u001b[39m CPCA()\n\u001b[0;32m----> 4\u001b[0m X_cpca \u001b[38;5;241m=\u001b[39m \u001b[43mcpca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_target\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_background\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/contrastive/__init__.py:58\u001b[0m, in \u001b[0;36mCPCA.fit_transform\u001b[0;34m(self, foreground, background, plot, gui, alpha_selection, n_alphas, max_log_alpha, n_alphas_to_return, active_labels, colors, legend, alpha_value, return_alphas)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, foreground, background, plot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, gui\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, alpha_selection\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m, n_alphas\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m,  max_log_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, n_alphas_to_return\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, active_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, legend\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, alpha_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, return_alphas\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(foreground, background)\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha_selection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha_selection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mn_alphas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_alphas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_log_alpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_log_alpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_alphas_to_return\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_alphas_to_return\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgui\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgui\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactive_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactive_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlegend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlegend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_alphas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_alphas\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03m    Computes the covariance matrices of the foreground and background datasets\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03m        but n_features > 1,000, a preliminary round of PCA is automatically performed to reduce the dimensionality to 1,000.\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/contrastive/__init__.py:242\u001b[0m, in \u001b[0;36mCPCA.transform\u001b[0;34m(self, dataset, alpha_selection, n_alphas, max_log_alpha, n_alphas_to_return, plot, gui, active_labels, colors, legend, alpha_value, return_alphas)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (alpha_selection\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 242\u001b[0m         transformed_data, best_alphas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautomated_cpca\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_alphas_to_return\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_alphas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_log_alpha\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m         alpha_values \u001b[38;5;241m=\u001b[39m best_alphas\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m (alpha_selection\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/contrastive/__init__.py:265\u001b[0m, in \u001b[0;36mCPCA.automated_cpca\u001b[0;34m(self, dataset, n_alphas_to_return, n_alphas, max_log_alpha)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mautomated_cpca\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset, n_alphas_to_return, n_alphas, max_log_alpha):\n\u001b[0;32m--> 265\u001b[0m     best_alphas, all_alphas, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_spectral_alphas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_alphas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_log_alpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_alphas_to_return\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m     best_alphas \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(([\u001b[38;5;241m0\u001b[39m], best_alphas)) \u001b[38;5;66;03m#one of the alphas is always alpha=0\u001b[39;00m\n\u001b[1;32m    267\u001b[0m     data_to_plot \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/contrastive/__init__.py:307\u001b[0m, in \u001b[0;36mCPCA.find_spectral_alphas\u001b[0;34m(self, n_alphas, max_log_alpha, n_alphas_to_return)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_spectral_alphas\u001b[39m(\u001b[38;5;28mself\u001b[39m, n_alphas, max_log_alpha, n_alphas_to_return):\n\u001b[0;32m--> 307\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_affinity_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_log_alpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_alphas\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m     affinity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maffinity_matrix\n\u001b[1;32m    309\u001b[0m     spectral \u001b[38;5;241m=\u001b[39m cluster\u001b[38;5;241m.\u001b[39mSpectralClustering(n_clusters\u001b[38;5;241m=\u001b[39mn_alphas_to_return, affinity\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecomputed\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/contrastive/__init__.py:333\u001b[0m, in \u001b[0;36mCPCA.create_affinity_matrix\u001b[0;34m(self, max_log_alpha, n_alphas)\u001b[0m\n\u001b[1;32m    331\u001b[0m affinity \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39midentity(k) \u001b[38;5;66;03m#it gets doubled\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m alpha \u001b[38;5;129;01min\u001b[39;00m alphas:\n\u001b[0;32m--> 333\u001b[0m     space \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpca_alpha\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    334\u001b[0m     q, r \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mqr(space)\n\u001b[1;32m    335\u001b[0m     subspaces\u001b[38;5;241m.\u001b[39mappend(q)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/contrastive/__init__.py:293\u001b[0m, in \u001b[0;36mCPCA.cpca_alpha\u001b[0;34m(self, dataset, alpha)\u001b[0m\n\u001b[1;32m    291\u001b[0m n_components \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components\n\u001b[1;32m    292\u001b[0m sigma \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfg_cov \u001b[38;5;241m-\u001b[39m alpha\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbg_cov\n\u001b[0;32m--> 293\u001b[0m w, v \u001b[38;5;241m=\u001b[39m \u001b[43mLA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meig\u001b[49m\u001b[43m(\u001b[49m\u001b[43msigma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m eig_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margpartition(w, \u001b[38;5;241m-\u001b[39mn_components)[\u001b[38;5;241m-\u001b[39mn_components:]\n\u001b[1;32m    295\u001b[0m eig_idx \u001b[38;5;241m=\u001b[39m eig_idx[np\u001b[38;5;241m.\u001b[39margsort(\u001b[38;5;241m-\u001b[39mw[eig_idx])]\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36meig\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/linalg/linalg.py:1304\u001b[0m, in \u001b[0;36meig\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m   1301\u001b[0m extobj \u001b[38;5;241m=\u001b[39m get_linalg_error_extobj(\n\u001b[1;32m   1302\u001b[0m     _raise_linalgerror_eigenvalues_nonconvergence)\n\u001b[1;32m   1303\u001b[0m signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD->DD\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isComplexType(t) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md->DD\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m-> 1304\u001b[0m w, vt \u001b[38;5;241m=\u001b[39m \u001b[43m_umath_linalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meig\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m isComplexType(t) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(w\u001b[38;5;241m.\u001b[39mimag \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m):\n\u001b[1;32m   1307\u001b[0m     w \u001b[38;5;241m=\u001b[39m w\u001b[38;5;241m.\u001b[39mreal\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from contrastive import CPCA\n",
    "\n",
    "cpca = CPCA()\n",
    "X_cpca = cpca.fit_transform(X_target[:, :], X_background[:, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c0bfda-cd65-4c1a-b50c-6395b1a97a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cpca = np.real(np.asarray(X_cpca))\n",
    "print(X_cpca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fcd2b95-e8e6-4a11-9c1a-7290b74fa7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.655 +/- 0.026\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "scores = cross_val_score(knn, X_pca, X_label, cv=5)\n",
    "print(f\"Accuracy: {round(np.mean(scores), 3)} +/- {round(np.std(scores), 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50e02369-f437-4af5-bdc4-5a0d7b640510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(397, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cpca[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f879f03b-4d63-4615-9993-cbe1529df518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.652 +/- 0.025\n",
      "Accuracy: 0.554 +/- 0.081\n",
      "Accuracy: 0.579 +/- 0.068\n",
      "Accuracy: 0.549 +/- 0.029\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "for i in range(4):\n",
    "    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    scores = cross_val_score(knn, X_cpca[i], X_label, cv=5)\n",
    "    print(f\"Accuracy: {round(np.mean(scores), 3)} +/- {round(np.std(scores), 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0afd2a9-9213-4b60-9c0d-40b5fb7bbff6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample complete\n",
      "label: [1, 0]\n",
      "(377, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.631 +/- 0.028\n",
      "PCA scores\n",
      "Accuracy: 0.578 +/- 0.031\n",
      "cPCA scores\n",
      "Accuracy: 0.576 +/- 0.022\n",
      "Accuracy: 0.631 +/- 0.028\n",
      "Accuracy: 0.61 +/- 0.061\n",
      "Accuracy: 0.549 +/- 0.041\n",
      "\n",
      "sample complete\n",
      "label: [2, 0]\n",
      "(406, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.754 +/- 0.052\n",
      "PCA scores\n",
      "Accuracy: 0.626 +/- 0.033\n",
      "cPCA scores\n",
      "Accuracy: 0.631 +/- 0.05\n",
      "Accuracy: 0.564 +/- 0.058\n",
      "Accuracy: 0.502 +/- 0.028\n",
      "Accuracy: 0.51 +/- 0.061\n",
      "\n",
      "sample complete\n",
      "label: [2, 1]\n",
      "(399, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.629 +/- 0.022\n",
      "PCA scores\n",
      "Accuracy: 0.596 +/- 0.03\n",
      "cPCA scores\n",
      "Accuracy: 0.624 +/- 0.041\n",
      "Accuracy: 0.644 +/- 0.044\n",
      "Accuracy: 0.669 +/- 0.043\n",
      "Accuracy: 0.566 +/- 0.088\n",
      "\n",
      "sample complete\n",
      "label: [3, 0]\n",
      "(382, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.754 +/- 0.038\n",
      "PCA scores\n",
      "Accuracy: 0.652 +/- 0.053\n",
      "cPCA scores\n",
      "Accuracy: 0.641 +/- 0.049\n",
      "Accuracy: 0.704 +/- 0.051\n",
      "Accuracy: 0.508 +/- 0.031\n",
      "Accuracy: 0.49 +/- 0.042\n",
      "\n",
      "sample complete\n",
      "label: [3, 1]\n",
      "(386, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.728 +/- 0.069\n",
      "PCA scores\n",
      "Accuracy: 0.591 +/- 0.065\n",
      "cPCA scores\n",
      "Accuracy: 0.625 +/- 0.043\n",
      "Accuracy: 0.562 +/- 0.053\n",
      "Accuracy: 0.63 +/- 0.06\n",
      "Accuracy: 0.526 +/- 0.036\n",
      "\n",
      "sample complete\n",
      "label: [3, 2]\n",
      "(400, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.605 +/- 0.036\n",
      "PCA scores\n",
      "Accuracy: 0.552 +/- 0.051\n",
      "cPCA scores\n",
      "Accuracy: 0.552 +/- 0.063\n",
      "Accuracy: 0.595 +/- 0.044\n",
      "Accuracy: 0.535 +/- 0.032\n",
      "Accuracy: 0.542 +/- 0.038\n",
      "\n",
      "sample complete\n",
      "label: [4, 0]\n",
      "(387, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.773 +/- 0.056\n",
      "PCA scores\n",
      "Accuracy: 0.693 +/- 0.052\n",
      "cPCA scores\n",
      "Accuracy: 0.641 +/- 0.049\n",
      "Accuracy: 0.612 +/- 0.04\n",
      "Accuracy: 0.47 +/- 0.032\n",
      "Accuracy: 0.499 +/- 0.063\n",
      "\n",
      "sample complete\n",
      "label: [4, 1]\n",
      "(367, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.657 +/- 0.048\n",
      "PCA scores\n",
      "Accuracy: 0.687 +/- 0.053\n",
      "cPCA scores\n",
      "Accuracy: 0.69 +/- 0.068\n",
      "Accuracy: 0.755 +/- 0.012\n",
      "Accuracy: 0.575 +/- 0.064\n",
      "Accuracy: 0.537 +/- 0.059\n",
      "\n",
      "sample complete\n",
      "label: [4, 2]\n",
      "(449, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.583 +/- 0.034\n",
      "PCA scores\n",
      "Accuracy: 0.543 +/- 0.023\n",
      "cPCA scores\n",
      "Accuracy: 0.541 +/- 0.064\n",
      "Accuracy: 0.543 +/- 0.024\n",
      "Accuracy: 0.483 +/- 0.028\n",
      "Accuracy: 0.543 +/- 0.052\n",
      "\n",
      "sample complete\n",
      "label: [4, 3]\n",
      "(436, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.617 +/- 0.056\n",
      "PCA scores\n",
      "Accuracy: 0.516 +/- 0.028\n",
      "cPCA scores\n",
      "Accuracy: 0.491 +/- 0.031\n",
      "Accuracy: 0.555 +/- 0.027\n",
      "Accuracy: 0.495 +/- 0.07\n",
      "Accuracy: 0.493 +/- 0.045\n",
      "\n",
      "sample complete\n",
      "label: [5, 0]\n",
      "(383, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.812 +/- 0.039\n",
      "PCA scores\n",
      "Accuracy: 0.737 +/- 0.053\n",
      "cPCA scores\n",
      "Accuracy: 0.729 +/- 0.039\n",
      "Accuracy: 0.621 +/- 0.051\n",
      "Accuracy: 0.517 +/- 0.045\n",
      "Accuracy: 0.473 +/- 0.018\n",
      "\n",
      "sample complete\n",
      "label: [5, 1]\n",
      "(380, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.713 +/- 0.042\n",
      "PCA scores\n",
      "Accuracy: 0.634 +/- 0.047\n",
      "cPCA scores\n",
      "Accuracy: 0.637 +/- 0.038\n",
      "Accuracy: 0.689 +/- 0.047\n",
      "Accuracy: 0.547 +/- 0.056\n",
      "Accuracy: 0.521 +/- 0.046\n",
      "\n",
      "sample complete\n",
      "label: [5, 2]\n",
      "(365, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.614 +/- 0.041\n",
      "PCA scores\n",
      "Accuracy: 0.641 +/- 0.049\n",
      "cPCA scores\n",
      "Accuracy: 0.622 +/- 0.061\n",
      "Accuracy: 0.534 +/- 0.023\n",
      "Accuracy: 0.474 +/- 0.073\n",
      "Accuracy: 0.482 +/- 0.058\n",
      "\n",
      "sample complete\n",
      "label: [5, 3]\n",
      "(382, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.539 +/- 0.034\n",
      "PCA scores\n",
      "Accuracy: 0.453 +/- 0.024\n",
      "cPCA scores\n",
      "Accuracy: 0.474 +/- 0.047\n",
      "Accuracy: 0.49 +/- 0.04\n",
      "Accuracy: 0.479 +/- 0.019\n",
      "Accuracy: 0.506 +/- 0.064\n",
      "\n",
      "sample complete\n",
      "label: [5, 4]\n",
      "(404, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.629 +/- 0.059\n",
      "PCA scores\n",
      "Accuracy: 0.475 +/- 0.024\n",
      "cPCA scores\n",
      "Accuracy: 0.478 +/- 0.05\n",
      "Accuracy: 0.646 +/- 0.033\n",
      "Accuracy: 0.559 +/- 0.033\n",
      "Accuracy: 0.468 +/- 0.042\n",
      "\n",
      "sample complete\n",
      "label: [6, 0]\n",
      "(393, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.842 +/- 0.034\n",
      "PCA scores\n",
      "Accuracy: 0.72 +/- 0.065\n",
      "cPCA scores\n",
      "Accuracy: 0.705 +/- 0.039\n",
      "Accuracy: 0.807 +/- 0.028\n",
      "Accuracy: 0.649 +/- 0.042\n",
      "Accuracy: 0.532 +/- 0.038\n",
      "\n",
      "sample complete\n",
      "label: [6, 1]\n",
      "(397, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.642 +/- 0.008\n",
      "PCA scores\n",
      "Accuracy: 0.658 +/- 0.02\n",
      "cPCA scores\n",
      "Accuracy: 0.642 +/- 0.028\n",
      "Accuracy: 0.71 +/- 0.024\n",
      "Accuracy: 0.59 +/- 0.042\n",
      "Accuracy: 0.531 +/- 0.05\n",
      "\n",
      "sample complete\n",
      "label: [6, 2]\n",
      "(415, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.607 +/- 0.039\n",
      "PCA scores\n",
      "Accuracy: 0.516 +/- 0.034\n",
      "cPCA scores\n",
      "Accuracy: 0.499 +/- 0.016\n",
      "Accuracy: 0.61 +/- 0.04\n",
      "Accuracy: 0.499 +/- 0.02\n",
      "Accuracy: 0.525 +/- 0.029\n",
      "\n",
      "sample complete\n",
      "label: [6, 3]\n",
      "(400, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.61 +/- 0.027\n",
      "PCA scores\n",
      "Accuracy: 0.562 +/- 0.049\n",
      "cPCA scores\n",
      "Accuracy: 0.562 +/- 0.042\n",
      "Accuracy: 0.612 +/- 0.032\n",
      "Accuracy: 0.495 +/- 0.067\n",
      "Accuracy: 0.543 +/- 0.033\n",
      "\n",
      "sample complete\n",
      "label: [6, 4]\n",
      "(393, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.603 +/- 0.049\n",
      "PCA scores\n",
      "Accuracy: 0.562 +/- 0.029\n",
      "cPCA scores\n",
      "Accuracy: 0.608 +/- 0.048\n",
      "Accuracy: 0.519 +/- 0.021\n",
      "Accuracy: 0.606 +/- 0.06\n",
      "Accuracy: 0.534 +/- 0.021\n",
      "\n",
      "sample complete\n",
      "label: [6, 5]\n",
      "(396, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.659 +/- 0.021\n",
      "PCA scores\n",
      "Accuracy: 0.634 +/- 0.075\n",
      "cPCA scores\n",
      "Accuracy: 0.644 +/- 0.033\n",
      "Accuracy: 0.558 +/- 0.045\n",
      "Accuracy: 0.515 +/- 0.04\n",
      "Accuracy: 0.556 +/- 0.033\n",
      "\n",
      "sample complete\n",
      "label: [7, 0]\n",
      "(388, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.699 +/- 0.033\n",
      "PCA scores\n",
      "Accuracy: 0.645 +/- 0.048\n",
      "cPCA scores\n",
      "Accuracy: 0.663 +/- 0.047\n",
      "Accuracy: 0.57 +/- 0.045\n",
      "Accuracy: 0.575 +/- 0.054\n",
      "Accuracy: 0.552 +/- 0.033\n",
      "\n",
      "sample complete\n",
      "label: [7, 1]\n",
      "(390, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.715 +/- 0.038\n",
      "PCA scores\n",
      "Accuracy: 0.528 +/- 0.045\n",
      "cPCA scores\n",
      "Accuracy: 0.556 +/- 0.041\n",
      "Accuracy: 0.577 +/- 0.044\n",
      "Accuracy: 0.567 +/- 0.026\n",
      "Accuracy: 0.518 +/- 0.013\n",
      "\n",
      "sample complete\n",
      "label: [7, 2]\n",
      "(402, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.654 +/- 0.03\n",
      "PCA scores\n",
      "Accuracy: 0.565 +/- 0.022\n",
      "cPCA scores\n",
      "Accuracy: 0.582 +/- 0.027\n",
      "Accuracy: 0.612 +/- 0.045\n",
      "Accuracy: 0.512 +/- 0.04\n",
      "Accuracy: 0.545 +/- 0.07\n",
      "\n",
      "sample complete\n",
      "label: [7, 3]\n",
      "(405, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.644 +/- 0.031\n",
      "PCA scores\n",
      "Accuracy: 0.516 +/- 0.061\n",
      "cPCA scores\n",
      "Accuracy: 0.509 +/- 0.054\n",
      "Accuracy: 0.523 +/- 0.01\n",
      "Accuracy: 0.514 +/- 0.02\n",
      "Accuracy: 0.511 +/- 0.049\n",
      "\n",
      "sample complete\n",
      "label: [7, 4]\n",
      "(398, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.621 +/- 0.027\n",
      "PCA scores\n",
      "Accuracy: 0.555 +/- 0.045\n",
      "cPCA scores\n",
      "Accuracy: 0.565 +/- 0.05\n",
      "Accuracy: 0.548 +/- 0.025\n",
      "Accuracy: 0.548 +/- 0.053\n",
      "Accuracy: 0.518 +/- 0.02\n",
      "\n",
      "sample complete\n",
      "label: [7, 5]\n",
      "(423, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.676 +/- 0.018\n",
      "PCA scores\n",
      "Accuracy: 0.582 +/- 0.059\n",
      "cPCA scores\n",
      "Accuracy: 0.541 +/- 0.041\n",
      "Accuracy: 0.546 +/- 0.03\n",
      "Accuracy: 0.53 +/- 0.041\n",
      "Accuracy: 0.532 +/- 0.056\n",
      "\n",
      "sample complete\n",
      "label: [7, 6]\n",
      "(403, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.643 +/- 0.029\n",
      "PCA scores\n",
      "Accuracy: 0.596 +/- 0.051\n",
      "cPCA scores\n",
      "Accuracy: 0.596 +/- 0.048\n",
      "Accuracy: 0.623 +/- 0.033\n",
      "Accuracy: 0.481 +/- 0.041\n",
      "Accuracy: 0.501 +/- 0.031\n",
      "\n",
      "sample complete\n",
      "label: [8, 0]\n",
      "(401, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.661 +/- 0.064\n",
      "PCA scores\n",
      "Accuracy: 0.604 +/- 0.037\n",
      "cPCA scores\n",
      "Accuracy: 0.606 +/- 0.048\n",
      "Accuracy: 0.584 +/- 0.072\n",
      "Accuracy: 0.491 +/- 0.029\n",
      "Accuracy: 0.486 +/- 0.04\n",
      "\n",
      "sample complete\n",
      "label: [8, 1]\n",
      "(376, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.625 +/- 0.02\n",
      "PCA scores\n",
      "Accuracy: 0.585 +/- 0.031\n",
      "cPCA scores\n",
      "Accuracy: 0.577 +/- 0.033\n",
      "Accuracy: 0.598 +/- 0.055\n",
      "Accuracy: 0.567 +/- 0.036\n",
      "Accuracy: 0.553 +/- 0.037\n",
      "\n",
      "sample complete\n",
      "label: [8, 2]\n",
      "(409, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.831 +/- 0.037\n",
      "PCA scores\n",
      "Accuracy: 0.721 +/- 0.021\n",
      "cPCA scores\n",
      "Accuracy: 0.719 +/- 0.024\n",
      "Accuracy: 0.579 +/- 0.073\n",
      "Accuracy: 0.496 +/- 0.049\n",
      "Accuracy: 0.491 +/- 0.05\n",
      "\n",
      "sample complete\n",
      "label: [8, 3]\n",
      "(393, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.825 +/- 0.027\n",
      "PCA scores\n",
      "Accuracy: 0.741 +/- 0.042\n",
      "cPCA scores\n",
      "Accuracy: 0.723 +/- 0.047\n",
      "Accuracy: 0.799 +/- 0.057\n",
      "Accuracy: 0.53 +/- 0.052\n",
      "Accuracy: 0.534 +/- 0.022\n",
      "\n",
      "sample complete\n",
      "label: [8, 4]\n",
      "(383, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.838 +/- 0.053\n",
      "PCA scores\n",
      "Accuracy: 0.715 +/- 0.057\n",
      "cPCA scores\n",
      "Accuracy: 0.734 +/- 0.072\n",
      "Accuracy: 0.76 +/- 0.051\n",
      "Accuracy: 0.494 +/- 0.038\n",
      "Accuracy: 0.559 +/- 0.047\n",
      "\n",
      "sample complete\n",
      "label: [8, 5]\n",
      "(398, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.852 +/- 0.031\n",
      "PCA scores\n",
      "Accuracy: 0.676 +/- 0.026\n",
      "cPCA scores\n",
      "Accuracy: 0.678 +/- 0.031\n",
      "Accuracy: 0.636 +/- 0.036\n",
      "Accuracy: 0.522 +/- 0.034\n",
      "Accuracy: 0.515 +/- 0.025\n",
      "\n",
      "sample complete\n",
      "label: [8, 6]\n",
      "(428, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.879 +/- 0.034\n",
      "PCA scores\n",
      "Accuracy: 0.792 +/- 0.053\n",
      "cPCA scores\n",
      "Accuracy: 0.785 +/- 0.059\n",
      "Accuracy: 0.554 +/- 0.024\n",
      "Accuracy: 0.561 +/- 0.054\n",
      "Accuracy: 0.516 +/- 0.032\n",
      "\n",
      "sample complete\n",
      "label: [8, 7]\n",
      "(390, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.764 +/- 0.028\n",
      "PCA scores\n",
      "Accuracy: 0.667 +/- 0.026\n",
      "cPCA scores\n",
      "Accuracy: 0.672 +/- 0.033\n",
      "Accuracy: 0.762 +/- 0.037\n",
      "Accuracy: 0.585 +/- 0.019\n",
      "Accuracy: 0.513 +/- 0.047\n",
      "\n",
      "sample complete\n",
      "label: [9, 0]\n",
      "(401, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.628 +/- 0.031\n",
      "PCA scores\n",
      "Accuracy: 0.648 +/- 0.032\n",
      "cPCA scores\n",
      "Accuracy: 0.661 +/- 0.026\n",
      "Accuracy: 0.676 +/- 0.027\n",
      "Accuracy: 0.553 +/- 0.051\n",
      "Accuracy: 0.534 +/- 0.029\n",
      "\n",
      "sample complete\n",
      "label: [9, 1]\n",
      "(396, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.644 +/- 0.049\n",
      "PCA scores\n",
      "Accuracy: 0.614 +/- 0.074\n",
      "cPCA scores\n",
      "Accuracy: 0.614 +/- 0.047\n",
      "Accuracy: 0.568 +/- 0.022\n",
      "Accuracy: 0.54 +/- 0.019\n",
      "Accuracy: 0.52 +/- 0.05\n",
      "\n",
      "sample complete\n",
      "label: [9, 2]\n",
      "(399, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.667 +/- 0.047\n",
      "PCA scores\n",
      "Accuracy: 0.712 +/- 0.029\n",
      "cPCA scores\n",
      "Accuracy: 0.704 +/- 0.046\n",
      "Accuracy: 0.607 +/- 0.02\n",
      "Accuracy: 0.577 +/- 0.04\n",
      "Accuracy: 0.569 +/- 0.041\n",
      "\n",
      "sample complete\n",
      "label: [9, 3]\n",
      "(412, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.755 +/- 0.046\n",
      "PCA scores\n",
      "Accuracy: 0.704 +/- 0.023\n",
      "cPCA scores\n",
      "Accuracy: 0.697 +/- 0.025\n",
      "Accuracy: 0.527 +/- 0.054\n",
      "Accuracy: 0.556 +/- 0.016\n",
      "Accuracy: 0.558 +/- 0.039\n",
      "\n",
      "sample complete\n",
      "label: [9, 4]\n",
      "(377, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.698 +/- 0.031\n",
      "PCA scores\n",
      "Accuracy: 0.703 +/- 0.036\n",
      "cPCA scores\n",
      "Accuracy: 0.735 +/- 0.032\n",
      "Accuracy: 0.703 +/- 0.019\n",
      "Accuracy: 0.592 +/- 0.042\n",
      "Accuracy: 0.586 +/- 0.048\n",
      "\n",
      "sample complete\n",
      "label: [9, 5]\n",
      "(391, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.716 +/- 0.012\n",
      "PCA scores\n",
      "Accuracy: 0.749 +/- 0.037\n",
      "cPCA scores\n",
      "Accuracy: 0.737 +/- 0.041\n",
      "Accuracy: 0.629 +/- 0.022\n",
      "Accuracy: 0.606 +/- 0.033\n",
      "Accuracy: 0.606 +/- 0.041\n",
      "\n",
      "sample complete\n",
      "label: [9, 6]\n",
      "(410, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.698 +/- 0.055\n",
      "PCA scores\n",
      "Accuracy: 0.715 +/- 0.049\n",
      "cPCA scores\n",
      "Accuracy: 0.702 +/- 0.035\n",
      "Accuracy: 0.615 +/- 0.048\n",
      "Accuracy: 0.49 +/- 0.042\n",
      "Accuracy: 0.507 +/- 0.033\n",
      "\n",
      "sample complete\n",
      "label: [9, 7]\n",
      "(385, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.706 +/- 0.039\n",
      "PCA scores\n",
      "Accuracy: 0.657 +/- 0.023\n",
      "cPCA scores\n",
      "Accuracy: 0.688 +/- 0.018\n",
      "Accuracy: 0.681 +/- 0.049\n",
      "Accuracy: 0.525 +/- 0.033\n",
      "Accuracy: 0.545 +/- 0.037\n",
      "\n",
      "sample complete\n",
      "label: [9, 8]\n",
      "(406, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.611 +/- 0.015\n",
      "PCA scores\n",
      "Accuracy: 0.586 +/- 0.011\n",
      "cPCA scores\n",
      "Accuracy: 0.581 +/- 0.029\n",
      "Accuracy: 0.579 +/- 0.035\n",
      "Accuracy: 0.571 +/- 0.043\n",
      "Accuracy: 0.537 +/- 0.044\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_list = list()\n",
    "\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        tmp_dict = {\n",
    "            \"no_preprocessing\": None,\n",
    "            \"PCA\": None,\n",
    "            \"cPCA-0\": None,\n",
    "            \"cPCA-1\": None,\n",
    "            \"cPCA-2\": None,\n",
    "            \"cPCA-3\": None,\n",
    "            \"name\": f\"{i} AND {j}\",\n",
    "        }\n",
    "        X_target = list()\n",
    "        X_label = list()\n",
    "        X_background = list()\n",
    "        if i > j:\n",
    "            TARGET_LABEL_LIST = [i, j]\n",
    "            for l in random.sample(range(10000), 2000):\n",
    "                label = cifar10_ds[\"train\"][l][\"label\"]\n",
    "                img_flat = np.asarray(cifar10_ds[\"train\"][l][\"img\"]).flatten()\n",
    "                if label in TARGET_LABEL_LIST:\n",
    "                    X_target.append(img_flat)\n",
    "                    X_label.append(label)\n",
    "                else:\n",
    "                    X_background.append(img_flat)\n",
    "            X_target = np.asarray(X_target)\n",
    "            X_label = np.asarray(X_label)\n",
    "            X_background = np.asarray(X_background)\n",
    "            print(\"sample complete\")\n",
    "            print(f\"label: {TARGET_LABEL_LIST}\")\n",
    "            pca = PCA(n_components=2)\n",
    "            X_pca = pca.fit_transform(X_target)\n",
    "            print(X_pca.shape)\n",
    "            print(\"PCA complete\")\n",
    "            cpca = CPCA()\n",
    "            X_cpca = cpca.fit_transform(X_target[:, :], X_background[:, :])\n",
    "            X_cpca = np.real(X_cpca)\n",
    "            print(\"CPCA complete\")\n",
    "            print(\"No preprocessing score\")\n",
    "            knn = KNeighborsClassifier(n_neighbors=5)\n",
    "            scores = cross_val_score(knn, X_target, X_label, cv=5)\n",
    "            print(\n",
    "                f\"Accuracy: {round(np.mean(scores), 3)} +/- {round(np.std(scores), 3)}\"\n",
    "            )\n",
    "            tmp_dict[\"no_preprocessing\"] = (\n",
    "                f\"{round(np.mean(scores), 3)} +/- {round(np.std(scores), 3)}\"\n",
    "            )\n",
    "            print(\"PCA scores\")\n",
    "            knn = KNeighborsClassifier(n_neighbors=5)\n",
    "            scores = cross_val_score(knn, X_pca, X_label, cv=5)\n",
    "            print(\n",
    "                f\"Accuracy: {round(np.mean(scores), 3)} +/- {round(np.std(scores), 3)}\"\n",
    "            )\n",
    "            tmp_dict[\"PCA\"] = (\n",
    "                f\"{round(np.mean(scores), 3)} +/- {round(np.std(scores), 3)}\"\n",
    "            )\n",
    "            print(\"cPCA scores\")\n",
    "            for k in range(4):\n",
    "                knn = KNeighborsClassifier(n_neighbors=5)\n",
    "                scores = cross_val_score(knn, X_cpca[k], X_label, cv=5)\n",
    "                print(\n",
    "                    f\"Accuracy: {round(np.mean(scores), 3)} +/- {round(np.std(scores), 3)}\"\n",
    "                )\n",
    "                tmp_dict[\"cPCA-\" + str(k)] = (\n",
    "                    f\"{round(np.mean(scores), 3)} +/- {round(np.std(scores), 3)}\"\n",
    "                )\n",
    "            print()\n",
    "            results_list.append(tmp_dict)\n",
    "            # raise KeyError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "316c7fda-b899-4785-bde9-71acdff22cc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_preprocessing</th>\n",
       "      <th>PCA</th>\n",
       "      <th>cPCA-0</th>\n",
       "      <th>cPCA-1</th>\n",
       "      <th>cPCA-2</th>\n",
       "      <th>cPCA-3</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.631 +/- 0.028</td>\n",
       "      <td>0.578 +/- 0.031</td>\n",
       "      <td>0.576 +/- 0.022</td>\n",
       "      <td>0.631 +/- 0.028</td>\n",
       "      <td>0.61 +/- 0.061</td>\n",
       "      <td>0.549 +/- 0.041</td>\n",
       "      <td>1 AND 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.754 +/- 0.052</td>\n",
       "      <td>0.626 +/- 0.033</td>\n",
       "      <td>0.631 +/- 0.05</td>\n",
       "      <td>0.564 +/- 0.058</td>\n",
       "      <td>0.502 +/- 0.028</td>\n",
       "      <td>0.51 +/- 0.061</td>\n",
       "      <td>2 AND 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.629 +/- 0.022</td>\n",
       "      <td>0.596 +/- 0.03</td>\n",
       "      <td>0.624 +/- 0.041</td>\n",
       "      <td>0.644 +/- 0.044</td>\n",
       "      <td>0.669 +/- 0.043</td>\n",
       "      <td>0.566 +/- 0.088</td>\n",
       "      <td>2 AND 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.754 +/- 0.038</td>\n",
       "      <td>0.652 +/- 0.053</td>\n",
       "      <td>0.641 +/- 0.049</td>\n",
       "      <td>0.704 +/- 0.051</td>\n",
       "      <td>0.508 +/- 0.031</td>\n",
       "      <td>0.49 +/- 0.042</td>\n",
       "      <td>3 AND 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.728 +/- 0.069</td>\n",
       "      <td>0.591 +/- 0.065</td>\n",
       "      <td>0.625 +/- 0.043</td>\n",
       "      <td>0.562 +/- 0.053</td>\n",
       "      <td>0.63 +/- 0.06</td>\n",
       "      <td>0.526 +/- 0.036</td>\n",
       "      <td>3 AND 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.605 +/- 0.036</td>\n",
       "      <td>0.552 +/- 0.051</td>\n",
       "      <td>0.552 +/- 0.063</td>\n",
       "      <td>0.595 +/- 0.044</td>\n",
       "      <td>0.535 +/- 0.032</td>\n",
       "      <td>0.542 +/- 0.038</td>\n",
       "      <td>3 AND 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.773 +/- 0.056</td>\n",
       "      <td>0.693 +/- 0.052</td>\n",
       "      <td>0.641 +/- 0.049</td>\n",
       "      <td>0.612 +/- 0.04</td>\n",
       "      <td>0.47 +/- 0.032</td>\n",
       "      <td>0.499 +/- 0.063</td>\n",
       "      <td>4 AND 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.657 +/- 0.048</td>\n",
       "      <td>0.687 +/- 0.053</td>\n",
       "      <td>0.69 +/- 0.068</td>\n",
       "      <td>0.755 +/- 0.012</td>\n",
       "      <td>0.575 +/- 0.064</td>\n",
       "      <td>0.537 +/- 0.059</td>\n",
       "      <td>4 AND 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.583 +/- 0.034</td>\n",
       "      <td>0.543 +/- 0.023</td>\n",
       "      <td>0.541 +/- 0.064</td>\n",
       "      <td>0.543 +/- 0.024</td>\n",
       "      <td>0.483 +/- 0.028</td>\n",
       "      <td>0.543 +/- 0.052</td>\n",
       "      <td>4 AND 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.617 +/- 0.056</td>\n",
       "      <td>0.516 +/- 0.028</td>\n",
       "      <td>0.491 +/- 0.031</td>\n",
       "      <td>0.555 +/- 0.027</td>\n",
       "      <td>0.495 +/- 0.07</td>\n",
       "      <td>0.493 +/- 0.045</td>\n",
       "      <td>4 AND 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.812 +/- 0.039</td>\n",
       "      <td>0.737 +/- 0.053</td>\n",
       "      <td>0.729 +/- 0.039</td>\n",
       "      <td>0.621 +/- 0.051</td>\n",
       "      <td>0.517 +/- 0.045</td>\n",
       "      <td>0.473 +/- 0.018</td>\n",
       "      <td>5 AND 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.713 +/- 0.042</td>\n",
       "      <td>0.634 +/- 0.047</td>\n",
       "      <td>0.637 +/- 0.038</td>\n",
       "      <td>0.689 +/- 0.047</td>\n",
       "      <td>0.547 +/- 0.056</td>\n",
       "      <td>0.521 +/- 0.046</td>\n",
       "      <td>5 AND 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.614 +/- 0.041</td>\n",
       "      <td>0.641 +/- 0.049</td>\n",
       "      <td>0.622 +/- 0.061</td>\n",
       "      <td>0.534 +/- 0.023</td>\n",
       "      <td>0.474 +/- 0.073</td>\n",
       "      <td>0.482 +/- 0.058</td>\n",
       "      <td>5 AND 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.539 +/- 0.034</td>\n",
       "      <td>0.453 +/- 0.024</td>\n",
       "      <td>0.474 +/- 0.047</td>\n",
       "      <td>0.49 +/- 0.04</td>\n",
       "      <td>0.479 +/- 0.019</td>\n",
       "      <td>0.506 +/- 0.064</td>\n",
       "      <td>5 AND 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.629 +/- 0.059</td>\n",
       "      <td>0.475 +/- 0.024</td>\n",
       "      <td>0.478 +/- 0.05</td>\n",
       "      <td>0.646 +/- 0.033</td>\n",
       "      <td>0.559 +/- 0.033</td>\n",
       "      <td>0.468 +/- 0.042</td>\n",
       "      <td>5 AND 4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.842 +/- 0.034</td>\n",
       "      <td>0.72 +/- 0.065</td>\n",
       "      <td>0.705 +/- 0.039</td>\n",
       "      <td>0.807 +/- 0.028</td>\n",
       "      <td>0.649 +/- 0.042</td>\n",
       "      <td>0.532 +/- 0.038</td>\n",
       "      <td>6 AND 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.642 +/- 0.008</td>\n",
       "      <td>0.658 +/- 0.02</td>\n",
       "      <td>0.642 +/- 0.028</td>\n",
       "      <td>0.71 +/- 0.024</td>\n",
       "      <td>0.59 +/- 0.042</td>\n",
       "      <td>0.531 +/- 0.05</td>\n",
       "      <td>6 AND 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.607 +/- 0.039</td>\n",
       "      <td>0.516 +/- 0.034</td>\n",
       "      <td>0.499 +/- 0.016</td>\n",
       "      <td>0.61 +/- 0.04</td>\n",
       "      <td>0.499 +/- 0.02</td>\n",
       "      <td>0.525 +/- 0.029</td>\n",
       "      <td>6 AND 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.61 +/- 0.027</td>\n",
       "      <td>0.562 +/- 0.049</td>\n",
       "      <td>0.562 +/- 0.042</td>\n",
       "      <td>0.612 +/- 0.032</td>\n",
       "      <td>0.495 +/- 0.067</td>\n",
       "      <td>0.543 +/- 0.033</td>\n",
       "      <td>6 AND 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.603 +/- 0.049</td>\n",
       "      <td>0.562 +/- 0.029</td>\n",
       "      <td>0.608 +/- 0.048</td>\n",
       "      <td>0.519 +/- 0.021</td>\n",
       "      <td>0.606 +/- 0.06</td>\n",
       "      <td>0.534 +/- 0.021</td>\n",
       "      <td>6 AND 4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.659 +/- 0.021</td>\n",
       "      <td>0.634 +/- 0.075</td>\n",
       "      <td>0.644 +/- 0.033</td>\n",
       "      <td>0.558 +/- 0.045</td>\n",
       "      <td>0.515 +/- 0.04</td>\n",
       "      <td>0.556 +/- 0.033</td>\n",
       "      <td>6 AND 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.699 +/- 0.033</td>\n",
       "      <td>0.645 +/- 0.048</td>\n",
       "      <td>0.663 +/- 0.047</td>\n",
       "      <td>0.57 +/- 0.045</td>\n",
       "      <td>0.575 +/- 0.054</td>\n",
       "      <td>0.552 +/- 0.033</td>\n",
       "      <td>7 AND 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.715 +/- 0.038</td>\n",
       "      <td>0.528 +/- 0.045</td>\n",
       "      <td>0.556 +/- 0.041</td>\n",
       "      <td>0.577 +/- 0.044</td>\n",
       "      <td>0.567 +/- 0.026</td>\n",
       "      <td>0.518 +/- 0.013</td>\n",
       "      <td>7 AND 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.654 +/- 0.03</td>\n",
       "      <td>0.565 +/- 0.022</td>\n",
       "      <td>0.582 +/- 0.027</td>\n",
       "      <td>0.612 +/- 0.045</td>\n",
       "      <td>0.512 +/- 0.04</td>\n",
       "      <td>0.545 +/- 0.07</td>\n",
       "      <td>7 AND 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.644 +/- 0.031</td>\n",
       "      <td>0.516 +/- 0.061</td>\n",
       "      <td>0.509 +/- 0.054</td>\n",
       "      <td>0.523 +/- 0.01</td>\n",
       "      <td>0.514 +/- 0.02</td>\n",
       "      <td>0.511 +/- 0.049</td>\n",
       "      <td>7 AND 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.621 +/- 0.027</td>\n",
       "      <td>0.555 +/- 0.045</td>\n",
       "      <td>0.565 +/- 0.05</td>\n",
       "      <td>0.548 +/- 0.025</td>\n",
       "      <td>0.548 +/- 0.053</td>\n",
       "      <td>0.518 +/- 0.02</td>\n",
       "      <td>7 AND 4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.676 +/- 0.018</td>\n",
       "      <td>0.582 +/- 0.059</td>\n",
       "      <td>0.541 +/- 0.041</td>\n",
       "      <td>0.546 +/- 0.03</td>\n",
       "      <td>0.53 +/- 0.041</td>\n",
       "      <td>0.532 +/- 0.056</td>\n",
       "      <td>7 AND 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.643 +/- 0.029</td>\n",
       "      <td>0.596 +/- 0.051</td>\n",
       "      <td>0.596 +/- 0.048</td>\n",
       "      <td>0.623 +/- 0.033</td>\n",
       "      <td>0.481 +/- 0.041</td>\n",
       "      <td>0.501 +/- 0.031</td>\n",
       "      <td>7 AND 6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.661 +/- 0.064</td>\n",
       "      <td>0.604 +/- 0.037</td>\n",
       "      <td>0.606 +/- 0.048</td>\n",
       "      <td>0.584 +/- 0.072</td>\n",
       "      <td>0.491 +/- 0.029</td>\n",
       "      <td>0.486 +/- 0.04</td>\n",
       "      <td>8 AND 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.625 +/- 0.02</td>\n",
       "      <td>0.585 +/- 0.031</td>\n",
       "      <td>0.577 +/- 0.033</td>\n",
       "      <td>0.598 +/- 0.055</td>\n",
       "      <td>0.567 +/- 0.036</td>\n",
       "      <td>0.553 +/- 0.037</td>\n",
       "      <td>8 AND 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.831 +/- 0.037</td>\n",
       "      <td>0.721 +/- 0.021</td>\n",
       "      <td>0.719 +/- 0.024</td>\n",
       "      <td>0.579 +/- 0.073</td>\n",
       "      <td>0.496 +/- 0.049</td>\n",
       "      <td>0.491 +/- 0.05</td>\n",
       "      <td>8 AND 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.825 +/- 0.027</td>\n",
       "      <td>0.741 +/- 0.042</td>\n",
       "      <td>0.723 +/- 0.047</td>\n",
       "      <td>0.799 +/- 0.057</td>\n",
       "      <td>0.53 +/- 0.052</td>\n",
       "      <td>0.534 +/- 0.022</td>\n",
       "      <td>8 AND 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.838 +/- 0.053</td>\n",
       "      <td>0.715 +/- 0.057</td>\n",
       "      <td>0.734 +/- 0.072</td>\n",
       "      <td>0.76 +/- 0.051</td>\n",
       "      <td>0.494 +/- 0.038</td>\n",
       "      <td>0.559 +/- 0.047</td>\n",
       "      <td>8 AND 4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.852 +/- 0.031</td>\n",
       "      <td>0.676 +/- 0.026</td>\n",
       "      <td>0.678 +/- 0.031</td>\n",
       "      <td>0.636 +/- 0.036</td>\n",
       "      <td>0.522 +/- 0.034</td>\n",
       "      <td>0.515 +/- 0.025</td>\n",
       "      <td>8 AND 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.879 +/- 0.034</td>\n",
       "      <td>0.792 +/- 0.053</td>\n",
       "      <td>0.785 +/- 0.059</td>\n",
       "      <td>0.554 +/- 0.024</td>\n",
       "      <td>0.561 +/- 0.054</td>\n",
       "      <td>0.516 +/- 0.032</td>\n",
       "      <td>8 AND 6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.764 +/- 0.028</td>\n",
       "      <td>0.667 +/- 0.026</td>\n",
       "      <td>0.672 +/- 0.033</td>\n",
       "      <td>0.762 +/- 0.037</td>\n",
       "      <td>0.585 +/- 0.019</td>\n",
       "      <td>0.513 +/- 0.047</td>\n",
       "      <td>8 AND 7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.628 +/- 0.031</td>\n",
       "      <td>0.648 +/- 0.032</td>\n",
       "      <td>0.661 +/- 0.026</td>\n",
       "      <td>0.676 +/- 0.027</td>\n",
       "      <td>0.553 +/- 0.051</td>\n",
       "      <td>0.534 +/- 0.029</td>\n",
       "      <td>9 AND 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.644 +/- 0.049</td>\n",
       "      <td>0.614 +/- 0.074</td>\n",
       "      <td>0.614 +/- 0.047</td>\n",
       "      <td>0.568 +/- 0.022</td>\n",
       "      <td>0.54 +/- 0.019</td>\n",
       "      <td>0.52 +/- 0.05</td>\n",
       "      <td>9 AND 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.667 +/- 0.047</td>\n",
       "      <td>0.712 +/- 0.029</td>\n",
       "      <td>0.704 +/- 0.046</td>\n",
       "      <td>0.607 +/- 0.02</td>\n",
       "      <td>0.577 +/- 0.04</td>\n",
       "      <td>0.569 +/- 0.041</td>\n",
       "      <td>9 AND 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.755 +/- 0.046</td>\n",
       "      <td>0.704 +/- 0.023</td>\n",
       "      <td>0.697 +/- 0.025</td>\n",
       "      <td>0.527 +/- 0.054</td>\n",
       "      <td>0.556 +/- 0.016</td>\n",
       "      <td>0.558 +/- 0.039</td>\n",
       "      <td>9 AND 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.698 +/- 0.031</td>\n",
       "      <td>0.703 +/- 0.036</td>\n",
       "      <td>0.735 +/- 0.032</td>\n",
       "      <td>0.703 +/- 0.019</td>\n",
       "      <td>0.592 +/- 0.042</td>\n",
       "      <td>0.586 +/- 0.048</td>\n",
       "      <td>9 AND 4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.716 +/- 0.012</td>\n",
       "      <td>0.749 +/- 0.037</td>\n",
       "      <td>0.737 +/- 0.041</td>\n",
       "      <td>0.629 +/- 0.022</td>\n",
       "      <td>0.606 +/- 0.033</td>\n",
       "      <td>0.606 +/- 0.041</td>\n",
       "      <td>9 AND 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.698 +/- 0.055</td>\n",
       "      <td>0.715 +/- 0.049</td>\n",
       "      <td>0.702 +/- 0.035</td>\n",
       "      <td>0.615 +/- 0.048</td>\n",
       "      <td>0.49 +/- 0.042</td>\n",
       "      <td>0.507 +/- 0.033</td>\n",
       "      <td>9 AND 6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.706 +/- 0.039</td>\n",
       "      <td>0.657 +/- 0.023</td>\n",
       "      <td>0.688 +/- 0.018</td>\n",
       "      <td>0.681 +/- 0.049</td>\n",
       "      <td>0.525 +/- 0.033</td>\n",
       "      <td>0.545 +/- 0.037</td>\n",
       "      <td>9 AND 7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.611 +/- 0.015</td>\n",
       "      <td>0.586 +/- 0.011</td>\n",
       "      <td>0.581 +/- 0.029</td>\n",
       "      <td>0.579 +/- 0.035</td>\n",
       "      <td>0.571 +/- 0.043</td>\n",
       "      <td>0.537 +/- 0.044</td>\n",
       "      <td>9 AND 8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   no_preprocessing              PCA           cPCA-0           cPCA-1  \\\n",
       "0   0.631 +/- 0.028  0.578 +/- 0.031  0.576 +/- 0.022  0.631 +/- 0.028   \n",
       "1   0.754 +/- 0.052  0.626 +/- 0.033   0.631 +/- 0.05  0.564 +/- 0.058   \n",
       "2   0.629 +/- 0.022   0.596 +/- 0.03  0.624 +/- 0.041  0.644 +/- 0.044   \n",
       "3   0.754 +/- 0.038  0.652 +/- 0.053  0.641 +/- 0.049  0.704 +/- 0.051   \n",
       "4   0.728 +/- 0.069  0.591 +/- 0.065  0.625 +/- 0.043  0.562 +/- 0.053   \n",
       "5   0.605 +/- 0.036  0.552 +/- 0.051  0.552 +/- 0.063  0.595 +/- 0.044   \n",
       "6   0.773 +/- 0.056  0.693 +/- 0.052  0.641 +/- 0.049   0.612 +/- 0.04   \n",
       "7   0.657 +/- 0.048  0.687 +/- 0.053   0.69 +/- 0.068  0.755 +/- 0.012   \n",
       "8   0.583 +/- 0.034  0.543 +/- 0.023  0.541 +/- 0.064  0.543 +/- 0.024   \n",
       "9   0.617 +/- 0.056  0.516 +/- 0.028  0.491 +/- 0.031  0.555 +/- 0.027   \n",
       "10  0.812 +/- 0.039  0.737 +/- 0.053  0.729 +/- 0.039  0.621 +/- 0.051   \n",
       "11  0.713 +/- 0.042  0.634 +/- 0.047  0.637 +/- 0.038  0.689 +/- 0.047   \n",
       "12  0.614 +/- 0.041  0.641 +/- 0.049  0.622 +/- 0.061  0.534 +/- 0.023   \n",
       "13  0.539 +/- 0.034  0.453 +/- 0.024  0.474 +/- 0.047    0.49 +/- 0.04   \n",
       "14  0.629 +/- 0.059  0.475 +/- 0.024   0.478 +/- 0.05  0.646 +/- 0.033   \n",
       "15  0.842 +/- 0.034   0.72 +/- 0.065  0.705 +/- 0.039  0.807 +/- 0.028   \n",
       "16  0.642 +/- 0.008   0.658 +/- 0.02  0.642 +/- 0.028   0.71 +/- 0.024   \n",
       "17  0.607 +/- 0.039  0.516 +/- 0.034  0.499 +/- 0.016    0.61 +/- 0.04   \n",
       "18   0.61 +/- 0.027  0.562 +/- 0.049  0.562 +/- 0.042  0.612 +/- 0.032   \n",
       "19  0.603 +/- 0.049  0.562 +/- 0.029  0.608 +/- 0.048  0.519 +/- 0.021   \n",
       "20  0.659 +/- 0.021  0.634 +/- 0.075  0.644 +/- 0.033  0.558 +/- 0.045   \n",
       "21  0.699 +/- 0.033  0.645 +/- 0.048  0.663 +/- 0.047   0.57 +/- 0.045   \n",
       "22  0.715 +/- 0.038  0.528 +/- 0.045  0.556 +/- 0.041  0.577 +/- 0.044   \n",
       "23   0.654 +/- 0.03  0.565 +/- 0.022  0.582 +/- 0.027  0.612 +/- 0.045   \n",
       "24  0.644 +/- 0.031  0.516 +/- 0.061  0.509 +/- 0.054   0.523 +/- 0.01   \n",
       "25  0.621 +/- 0.027  0.555 +/- 0.045   0.565 +/- 0.05  0.548 +/- 0.025   \n",
       "26  0.676 +/- 0.018  0.582 +/- 0.059  0.541 +/- 0.041   0.546 +/- 0.03   \n",
       "27  0.643 +/- 0.029  0.596 +/- 0.051  0.596 +/- 0.048  0.623 +/- 0.033   \n",
       "28  0.661 +/- 0.064  0.604 +/- 0.037  0.606 +/- 0.048  0.584 +/- 0.072   \n",
       "29   0.625 +/- 0.02  0.585 +/- 0.031  0.577 +/- 0.033  0.598 +/- 0.055   \n",
       "30  0.831 +/- 0.037  0.721 +/- 0.021  0.719 +/- 0.024  0.579 +/- 0.073   \n",
       "31  0.825 +/- 0.027  0.741 +/- 0.042  0.723 +/- 0.047  0.799 +/- 0.057   \n",
       "32  0.838 +/- 0.053  0.715 +/- 0.057  0.734 +/- 0.072   0.76 +/- 0.051   \n",
       "33  0.852 +/- 0.031  0.676 +/- 0.026  0.678 +/- 0.031  0.636 +/- 0.036   \n",
       "34  0.879 +/- 0.034  0.792 +/- 0.053  0.785 +/- 0.059  0.554 +/- 0.024   \n",
       "35  0.764 +/- 0.028  0.667 +/- 0.026  0.672 +/- 0.033  0.762 +/- 0.037   \n",
       "36  0.628 +/- 0.031  0.648 +/- 0.032  0.661 +/- 0.026  0.676 +/- 0.027   \n",
       "37  0.644 +/- 0.049  0.614 +/- 0.074  0.614 +/- 0.047  0.568 +/- 0.022   \n",
       "38  0.667 +/- 0.047  0.712 +/- 0.029  0.704 +/- 0.046   0.607 +/- 0.02   \n",
       "39  0.755 +/- 0.046  0.704 +/- 0.023  0.697 +/- 0.025  0.527 +/- 0.054   \n",
       "40  0.698 +/- 0.031  0.703 +/- 0.036  0.735 +/- 0.032  0.703 +/- 0.019   \n",
       "41  0.716 +/- 0.012  0.749 +/- 0.037  0.737 +/- 0.041  0.629 +/- 0.022   \n",
       "42  0.698 +/- 0.055  0.715 +/- 0.049  0.702 +/- 0.035  0.615 +/- 0.048   \n",
       "43  0.706 +/- 0.039  0.657 +/- 0.023  0.688 +/- 0.018  0.681 +/- 0.049   \n",
       "44  0.611 +/- 0.015  0.586 +/- 0.011  0.581 +/- 0.029  0.579 +/- 0.035   \n",
       "\n",
       "             cPCA-2           cPCA-3     name  \n",
       "0    0.61 +/- 0.061  0.549 +/- 0.041  1 AND 0  \n",
       "1   0.502 +/- 0.028   0.51 +/- 0.061  2 AND 0  \n",
       "2   0.669 +/- 0.043  0.566 +/- 0.088  2 AND 1  \n",
       "3   0.508 +/- 0.031   0.49 +/- 0.042  3 AND 0  \n",
       "4     0.63 +/- 0.06  0.526 +/- 0.036  3 AND 1  \n",
       "5   0.535 +/- 0.032  0.542 +/- 0.038  3 AND 2  \n",
       "6    0.47 +/- 0.032  0.499 +/- 0.063  4 AND 0  \n",
       "7   0.575 +/- 0.064  0.537 +/- 0.059  4 AND 1  \n",
       "8   0.483 +/- 0.028  0.543 +/- 0.052  4 AND 2  \n",
       "9    0.495 +/- 0.07  0.493 +/- 0.045  4 AND 3  \n",
       "10  0.517 +/- 0.045  0.473 +/- 0.018  5 AND 0  \n",
       "11  0.547 +/- 0.056  0.521 +/- 0.046  5 AND 1  \n",
       "12  0.474 +/- 0.073  0.482 +/- 0.058  5 AND 2  \n",
       "13  0.479 +/- 0.019  0.506 +/- 0.064  5 AND 3  \n",
       "14  0.559 +/- 0.033  0.468 +/- 0.042  5 AND 4  \n",
       "15  0.649 +/- 0.042  0.532 +/- 0.038  6 AND 0  \n",
       "16   0.59 +/- 0.042   0.531 +/- 0.05  6 AND 1  \n",
       "17   0.499 +/- 0.02  0.525 +/- 0.029  6 AND 2  \n",
       "18  0.495 +/- 0.067  0.543 +/- 0.033  6 AND 3  \n",
       "19   0.606 +/- 0.06  0.534 +/- 0.021  6 AND 4  \n",
       "20   0.515 +/- 0.04  0.556 +/- 0.033  6 AND 5  \n",
       "21  0.575 +/- 0.054  0.552 +/- 0.033  7 AND 0  \n",
       "22  0.567 +/- 0.026  0.518 +/- 0.013  7 AND 1  \n",
       "23   0.512 +/- 0.04   0.545 +/- 0.07  7 AND 2  \n",
       "24   0.514 +/- 0.02  0.511 +/- 0.049  7 AND 3  \n",
       "25  0.548 +/- 0.053   0.518 +/- 0.02  7 AND 4  \n",
       "26   0.53 +/- 0.041  0.532 +/- 0.056  7 AND 5  \n",
       "27  0.481 +/- 0.041  0.501 +/- 0.031  7 AND 6  \n",
       "28  0.491 +/- 0.029   0.486 +/- 0.04  8 AND 0  \n",
       "29  0.567 +/- 0.036  0.553 +/- 0.037  8 AND 1  \n",
       "30  0.496 +/- 0.049   0.491 +/- 0.05  8 AND 2  \n",
       "31   0.53 +/- 0.052  0.534 +/- 0.022  8 AND 3  \n",
       "32  0.494 +/- 0.038  0.559 +/- 0.047  8 AND 4  \n",
       "33  0.522 +/- 0.034  0.515 +/- 0.025  8 AND 5  \n",
       "34  0.561 +/- 0.054  0.516 +/- 0.032  8 AND 6  \n",
       "35  0.585 +/- 0.019  0.513 +/- 0.047  8 AND 7  \n",
       "36  0.553 +/- 0.051  0.534 +/- 0.029  9 AND 0  \n",
       "37   0.54 +/- 0.019    0.52 +/- 0.05  9 AND 1  \n",
       "38   0.577 +/- 0.04  0.569 +/- 0.041  9 AND 2  \n",
       "39  0.556 +/- 0.016  0.558 +/- 0.039  9 AND 3  \n",
       "40  0.592 +/- 0.042  0.586 +/- 0.048  9 AND 4  \n",
       "41  0.606 +/- 0.033  0.606 +/- 0.041  9 AND 5  \n",
       "42   0.49 +/- 0.042  0.507 +/- 0.033  9 AND 6  \n",
       "43  0.525 +/- 0.033  0.545 +/- 0.037  9 AND 7  \n",
       "44  0.571 +/- 0.043  0.537 +/- 0.044  9 AND 8  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(results_list)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd7205e6-69f0-4e44-838d-ee01f9692163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_5e087_row0_col0, #T_5e087_row0_col1, #T_5e087_row0_col2, #T_5e087_row0_col3, #T_5e087_row0_col4, #T_5e087_row0_col5, #T_5e087_row1_col0, #T_5e087_row1_col1, #T_5e087_row1_col2, #T_5e087_row1_col3, #T_5e087_row1_col4, #T_5e087_row1_col5, #T_5e087_row2_col0, #T_5e087_row2_col1, #T_5e087_row2_col2, #T_5e087_row2_col5, #T_5e087_row3_col0, #T_5e087_row3_col1, #T_5e087_row3_col2, #T_5e087_row3_col3, #T_5e087_row3_col4, #T_5e087_row3_col5, #T_5e087_row4_col0, #T_5e087_row4_col1, #T_5e087_row4_col2, #T_5e087_row4_col3, #T_5e087_row4_col4, #T_5e087_row4_col5, #T_5e087_row5_col0, #T_5e087_row5_col1, #T_5e087_row5_col2, #T_5e087_row5_col3, #T_5e087_row5_col4, #T_5e087_row5_col5, #T_5e087_row6_col0, #T_5e087_row6_col1, #T_5e087_row6_col2, #T_5e087_row6_col3, #T_5e087_row6_col4, #T_5e087_row6_col5, #T_5e087_row7_col0, #T_5e087_row7_col4, #T_5e087_row7_col5, #T_5e087_row8_col0, #T_5e087_row8_col1, #T_5e087_row8_col2, #T_5e087_row8_col3, #T_5e087_row8_col4, #T_5e087_row8_col5, #T_5e087_row9_col0, #T_5e087_row9_col1, #T_5e087_row9_col2, #T_5e087_row9_col3, #T_5e087_row9_col4, #T_5e087_row9_col5, #T_5e087_row10_col0, #T_5e087_row10_col1, #T_5e087_row10_col2, #T_5e087_row10_col3, #T_5e087_row10_col4, #T_5e087_row10_col5, #T_5e087_row11_col0, #T_5e087_row11_col1, #T_5e087_row11_col2, #T_5e087_row11_col3, #T_5e087_row11_col4, #T_5e087_row11_col5, #T_5e087_row12_col0, #T_5e087_row12_col3, #T_5e087_row12_col4, #T_5e087_row12_col5, #T_5e087_row13_col0, #T_5e087_row13_col1, #T_5e087_row13_col2, #T_5e087_row13_col3, #T_5e087_row13_col4, #T_5e087_row13_col5, #T_5e087_row14_col0, #T_5e087_row14_col1, #T_5e087_row14_col2, #T_5e087_row14_col4, #T_5e087_row14_col5, #T_5e087_row15_col0, #T_5e087_row15_col1, #T_5e087_row15_col2, #T_5e087_row15_col3, #T_5e087_row15_col4, #T_5e087_row15_col5, #T_5e087_row16_col0, #T_5e087_row16_col2, #T_5e087_row16_col4, #T_5e087_row16_col5, #T_5e087_row17_col0, #T_5e087_row17_col1, #T_5e087_row17_col2, #T_5e087_row17_col4, #T_5e087_row17_col5, #T_5e087_row18_col0, #T_5e087_row18_col1, #T_5e087_row18_col2, #T_5e087_row18_col4, #T_5e087_row18_col5, #T_5e087_row19_col0, #T_5e087_row19_col1, #T_5e087_row19_col3, #T_5e087_row19_col5, #T_5e087_row20_col0, #T_5e087_row20_col1, #T_5e087_row20_col2, #T_5e087_row20_col3, #T_5e087_row20_col4, #T_5e087_row20_col5, #T_5e087_row21_col0, #T_5e087_row21_col1, #T_5e087_row21_col2, #T_5e087_row21_col3, #T_5e087_row21_col4, #T_5e087_row21_col5, #T_5e087_row22_col0, #T_5e087_row22_col1, #T_5e087_row22_col2, #T_5e087_row22_col3, #T_5e087_row22_col4, #T_5e087_row22_col5, #T_5e087_row23_col0, #T_5e087_row23_col1, #T_5e087_row23_col2, #T_5e087_row23_col3, #T_5e087_row23_col4, #T_5e087_row23_col5, #T_5e087_row24_col0, #T_5e087_row24_col1, #T_5e087_row24_col2, #T_5e087_row24_col3, #T_5e087_row24_col4, #T_5e087_row24_col5, #T_5e087_row25_col0, #T_5e087_row25_col1, #T_5e087_row25_col2, #T_5e087_row25_col3, #T_5e087_row25_col4, #T_5e087_row25_col5, #T_5e087_row26_col0, #T_5e087_row26_col1, #T_5e087_row26_col2, #T_5e087_row26_col3, #T_5e087_row26_col4, #T_5e087_row26_col5, #T_5e087_row27_col0, #T_5e087_row27_col1, #T_5e087_row27_col2, #T_5e087_row27_col3, #T_5e087_row27_col4, #T_5e087_row27_col5, #T_5e087_row28_col0, #T_5e087_row28_col1, #T_5e087_row28_col2, #T_5e087_row28_col3, #T_5e087_row28_col4, #T_5e087_row28_col5, #T_5e087_row29_col0, #T_5e087_row29_col1, #T_5e087_row29_col2, #T_5e087_row29_col3, #T_5e087_row29_col4, #T_5e087_row29_col5, #T_5e087_row30_col0, #T_5e087_row30_col1, #T_5e087_row30_col2, #T_5e087_row30_col3, #T_5e087_row30_col4, #T_5e087_row30_col5, #T_5e087_row31_col0, #T_5e087_row31_col1, #T_5e087_row31_col2, #T_5e087_row31_col3, #T_5e087_row31_col4, #T_5e087_row31_col5, #T_5e087_row32_col0, #T_5e087_row32_col1, #T_5e087_row32_col2, #T_5e087_row32_col3, #T_5e087_row32_col4, #T_5e087_row32_col5, #T_5e087_row33_col0, #T_5e087_row33_col1, #T_5e087_row33_col2, #T_5e087_row33_col3, #T_5e087_row33_col4, #T_5e087_row33_col5, #T_5e087_row34_col0, #T_5e087_row34_col1, #T_5e087_row34_col2, #T_5e087_row34_col3, #T_5e087_row34_col4, #T_5e087_row34_col5, #T_5e087_row35_col0, #T_5e087_row35_col1, #T_5e087_row35_col2, #T_5e087_row35_col3, #T_5e087_row35_col4, #T_5e087_row35_col5, #T_5e087_row36_col0, #T_5e087_row36_col4, #T_5e087_row36_col5, #T_5e087_row37_col0, #T_5e087_row37_col1, #T_5e087_row37_col2, #T_5e087_row37_col3, #T_5e087_row37_col4, #T_5e087_row37_col5, #T_5e087_row38_col0, #T_5e087_row38_col3, #T_5e087_row38_col4, #T_5e087_row38_col5, #T_5e087_row39_col0, #T_5e087_row39_col1, #T_5e087_row39_col2, #T_5e087_row39_col3, #T_5e087_row39_col4, #T_5e087_row39_col5, #T_5e087_row40_col0, #T_5e087_row40_col4, #T_5e087_row40_col5, #T_5e087_row41_col0, #T_5e087_row41_col3, #T_5e087_row41_col4, #T_5e087_row41_col5, #T_5e087_row42_col0, #T_5e087_row42_col3, #T_5e087_row42_col4, #T_5e087_row42_col5, #T_5e087_row43_col0, #T_5e087_row43_col1, #T_5e087_row43_col2, #T_5e087_row43_col3, #T_5e087_row43_col4, #T_5e087_row43_col5, #T_5e087_row44_col0, #T_5e087_row44_col1, #T_5e087_row44_col2, #T_5e087_row44_col3, #T_5e087_row44_col4, #T_5e087_row44_col5 {\n",
       "  background-color: white;\n",
       "}\n",
       "#T_5e087_row2_col3, #T_5e087_row2_col4, #T_5e087_row7_col1, #T_5e087_row7_col2, #T_5e087_row7_col3, #T_5e087_row12_col1, #T_5e087_row12_col2, #T_5e087_row14_col3, #T_5e087_row16_col1, #T_5e087_row16_col3, #T_5e087_row17_col3, #T_5e087_row18_col3, #T_5e087_row19_col2, #T_5e087_row19_col4, #T_5e087_row36_col1, #T_5e087_row36_col2, #T_5e087_row36_col3, #T_5e087_row38_col1, #T_5e087_row38_col2, #T_5e087_row40_col1, #T_5e087_row40_col2, #T_5e087_row40_col3, #T_5e087_row41_col1, #T_5e087_row41_col2, #T_5e087_row42_col1, #T_5e087_row42_col2 {\n",
       "  background-color: rgb(100,255,100);\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_5e087\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_5e087_level0_col0\" class=\"col_heading level0 col0\" >no_preprocessing</th>\n",
       "      <th id=\"T_5e087_level0_col1\" class=\"col_heading level0 col1\" >PCA</th>\n",
       "      <th id=\"T_5e087_level0_col2\" class=\"col_heading level0 col2\" >cPCA-0</th>\n",
       "      <th id=\"T_5e087_level0_col3\" class=\"col_heading level0 col3\" >cPCA-1</th>\n",
       "      <th id=\"T_5e087_level0_col4\" class=\"col_heading level0 col4\" >cPCA-2</th>\n",
       "      <th id=\"T_5e087_level0_col5\" class=\"col_heading level0 col5\" >cPCA-3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >name</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row0\" class=\"row_heading level0 row0\" >1 AND 0</th>\n",
       "      <td id=\"T_5e087_row0_col0\" class=\"data row0 col0\" >0.631 +/- 0.028</td>\n",
       "      <td id=\"T_5e087_row0_col1\" class=\"data row0 col1\" >0.578 +/- 0.031</td>\n",
       "      <td id=\"T_5e087_row0_col2\" class=\"data row0 col2\" >0.576 +/- 0.022</td>\n",
       "      <td id=\"T_5e087_row0_col3\" class=\"data row0 col3\" >0.631 +/- 0.028</td>\n",
       "      <td id=\"T_5e087_row0_col4\" class=\"data row0 col4\" >0.61 +/- 0.061</td>\n",
       "      <td id=\"T_5e087_row0_col5\" class=\"data row0 col5\" >0.549 +/- 0.041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row1\" class=\"row_heading level0 row1\" >2 AND 0</th>\n",
       "      <td id=\"T_5e087_row1_col0\" class=\"data row1 col0\" >0.754 +/- 0.052</td>\n",
       "      <td id=\"T_5e087_row1_col1\" class=\"data row1 col1\" >0.626 +/- 0.033</td>\n",
       "      <td id=\"T_5e087_row1_col2\" class=\"data row1 col2\" >0.631 +/- 0.05</td>\n",
       "      <td id=\"T_5e087_row1_col3\" class=\"data row1 col3\" >0.564 +/- 0.058</td>\n",
       "      <td id=\"T_5e087_row1_col4\" class=\"data row1 col4\" >0.502 +/- 0.028</td>\n",
       "      <td id=\"T_5e087_row1_col5\" class=\"data row1 col5\" >0.51 +/- 0.061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row2\" class=\"row_heading level0 row2\" >2 AND 1</th>\n",
       "      <td id=\"T_5e087_row2_col0\" class=\"data row2 col0\" >0.629 +/- 0.022</td>\n",
       "      <td id=\"T_5e087_row2_col1\" class=\"data row2 col1\" >0.596 +/- 0.03</td>\n",
       "      <td id=\"T_5e087_row2_col2\" class=\"data row2 col2\" >0.624 +/- 0.041</td>\n",
       "      <td id=\"T_5e087_row2_col3\" class=\"data row2 col3\" >0.644 +/- 0.044</td>\n",
       "      <td id=\"T_5e087_row2_col4\" class=\"data row2 col4\" >0.669 +/- 0.043</td>\n",
       "      <td id=\"T_5e087_row2_col5\" class=\"data row2 col5\" >0.566 +/- 0.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row3\" class=\"row_heading level0 row3\" >3 AND 0</th>\n",
       "      <td id=\"T_5e087_row3_col0\" class=\"data row3 col0\" >0.754 +/- 0.038</td>\n",
       "      <td id=\"T_5e087_row3_col1\" class=\"data row3 col1\" >0.652 +/- 0.053</td>\n",
       "      <td id=\"T_5e087_row3_col2\" class=\"data row3 col2\" >0.641 +/- 0.049</td>\n",
       "      <td id=\"T_5e087_row3_col3\" class=\"data row3 col3\" >0.704 +/- 0.051</td>\n",
       "      <td id=\"T_5e087_row3_col4\" class=\"data row3 col4\" >0.508 +/- 0.031</td>\n",
       "      <td id=\"T_5e087_row3_col5\" class=\"data row3 col5\" >0.49 +/- 0.042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row4\" class=\"row_heading level0 row4\" >3 AND 1</th>\n",
       "      <td id=\"T_5e087_row4_col0\" class=\"data row4 col0\" >0.728 +/- 0.069</td>\n",
       "      <td id=\"T_5e087_row4_col1\" class=\"data row4 col1\" >0.591 +/- 0.065</td>\n",
       "      <td id=\"T_5e087_row4_col2\" class=\"data row4 col2\" >0.625 +/- 0.043</td>\n",
       "      <td id=\"T_5e087_row4_col3\" class=\"data row4 col3\" >0.562 +/- 0.053</td>\n",
       "      <td id=\"T_5e087_row4_col4\" class=\"data row4 col4\" >0.63 +/- 0.06</td>\n",
       "      <td id=\"T_5e087_row4_col5\" class=\"data row4 col5\" >0.526 +/- 0.036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row5\" class=\"row_heading level0 row5\" >3 AND 2</th>\n",
       "      <td id=\"T_5e087_row5_col0\" class=\"data row5 col0\" >0.605 +/- 0.036</td>\n",
       "      <td id=\"T_5e087_row5_col1\" class=\"data row5 col1\" >0.552 +/- 0.051</td>\n",
       "      <td id=\"T_5e087_row5_col2\" class=\"data row5 col2\" >0.552 +/- 0.063</td>\n",
       "      <td id=\"T_5e087_row5_col3\" class=\"data row5 col3\" >0.595 +/- 0.044</td>\n",
       "      <td id=\"T_5e087_row5_col4\" class=\"data row5 col4\" >0.535 +/- 0.032</td>\n",
       "      <td id=\"T_5e087_row5_col5\" class=\"data row5 col5\" >0.542 +/- 0.038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row6\" class=\"row_heading level0 row6\" >4 AND 0</th>\n",
       "      <td id=\"T_5e087_row6_col0\" class=\"data row6 col0\" >0.773 +/- 0.056</td>\n",
       "      <td id=\"T_5e087_row6_col1\" class=\"data row6 col1\" >0.693 +/- 0.052</td>\n",
       "      <td id=\"T_5e087_row6_col2\" class=\"data row6 col2\" >0.641 +/- 0.049</td>\n",
       "      <td id=\"T_5e087_row6_col3\" class=\"data row6 col3\" >0.612 +/- 0.04</td>\n",
       "      <td id=\"T_5e087_row6_col4\" class=\"data row6 col4\" >0.47 +/- 0.032</td>\n",
       "      <td id=\"T_5e087_row6_col5\" class=\"data row6 col5\" >0.499 +/- 0.063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row7\" class=\"row_heading level0 row7\" >4 AND 1</th>\n",
       "      <td id=\"T_5e087_row7_col0\" class=\"data row7 col0\" >0.657 +/- 0.048</td>\n",
       "      <td id=\"T_5e087_row7_col1\" class=\"data row7 col1\" >0.687 +/- 0.053</td>\n",
       "      <td id=\"T_5e087_row7_col2\" class=\"data row7 col2\" >0.69 +/- 0.068</td>\n",
       "      <td id=\"T_5e087_row7_col3\" class=\"data row7 col3\" >0.755 +/- 0.012</td>\n",
       "      <td id=\"T_5e087_row7_col4\" class=\"data row7 col4\" >0.575 +/- 0.064</td>\n",
       "      <td id=\"T_5e087_row7_col5\" class=\"data row7 col5\" >0.537 +/- 0.059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row8\" class=\"row_heading level0 row8\" >4 AND 2</th>\n",
       "      <td id=\"T_5e087_row8_col0\" class=\"data row8 col0\" >0.583 +/- 0.034</td>\n",
       "      <td id=\"T_5e087_row8_col1\" class=\"data row8 col1\" >0.543 +/- 0.023</td>\n",
       "      <td id=\"T_5e087_row8_col2\" class=\"data row8 col2\" >0.541 +/- 0.064</td>\n",
       "      <td id=\"T_5e087_row8_col3\" class=\"data row8 col3\" >0.543 +/- 0.024</td>\n",
       "      <td id=\"T_5e087_row8_col4\" class=\"data row8 col4\" >0.483 +/- 0.028</td>\n",
       "      <td id=\"T_5e087_row8_col5\" class=\"data row8 col5\" >0.543 +/- 0.052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row9\" class=\"row_heading level0 row9\" >4 AND 3</th>\n",
       "      <td id=\"T_5e087_row9_col0\" class=\"data row9 col0\" >0.617 +/- 0.056</td>\n",
       "      <td id=\"T_5e087_row9_col1\" class=\"data row9 col1\" >0.516 +/- 0.028</td>\n",
       "      <td id=\"T_5e087_row9_col2\" class=\"data row9 col2\" >0.491 +/- 0.031</td>\n",
       "      <td id=\"T_5e087_row9_col3\" class=\"data row9 col3\" >0.555 +/- 0.027</td>\n",
       "      <td id=\"T_5e087_row9_col4\" class=\"data row9 col4\" >0.495 +/- 0.07</td>\n",
       "      <td id=\"T_5e087_row9_col5\" class=\"data row9 col5\" >0.493 +/- 0.045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row10\" class=\"row_heading level0 row10\" >5 AND 0</th>\n",
       "      <td id=\"T_5e087_row10_col0\" class=\"data row10 col0\" >0.812 +/- 0.039</td>\n",
       "      <td id=\"T_5e087_row10_col1\" class=\"data row10 col1\" >0.737 +/- 0.053</td>\n",
       "      <td id=\"T_5e087_row10_col2\" class=\"data row10 col2\" >0.729 +/- 0.039</td>\n",
       "      <td id=\"T_5e087_row10_col3\" class=\"data row10 col3\" >0.621 +/- 0.051</td>\n",
       "      <td id=\"T_5e087_row10_col4\" class=\"data row10 col4\" >0.517 +/- 0.045</td>\n",
       "      <td id=\"T_5e087_row10_col5\" class=\"data row10 col5\" >0.473 +/- 0.018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row11\" class=\"row_heading level0 row11\" >5 AND 1</th>\n",
       "      <td id=\"T_5e087_row11_col0\" class=\"data row11 col0\" >0.713 +/- 0.042</td>\n",
       "      <td id=\"T_5e087_row11_col1\" class=\"data row11 col1\" >0.634 +/- 0.047</td>\n",
       "      <td id=\"T_5e087_row11_col2\" class=\"data row11 col2\" >0.637 +/- 0.038</td>\n",
       "      <td id=\"T_5e087_row11_col3\" class=\"data row11 col3\" >0.689 +/- 0.047</td>\n",
       "      <td id=\"T_5e087_row11_col4\" class=\"data row11 col4\" >0.547 +/- 0.056</td>\n",
       "      <td id=\"T_5e087_row11_col5\" class=\"data row11 col5\" >0.521 +/- 0.046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row12\" class=\"row_heading level0 row12\" >5 AND 2</th>\n",
       "      <td id=\"T_5e087_row12_col0\" class=\"data row12 col0\" >0.614 +/- 0.041</td>\n",
       "      <td id=\"T_5e087_row12_col1\" class=\"data row12 col1\" >0.641 +/- 0.049</td>\n",
       "      <td id=\"T_5e087_row12_col2\" class=\"data row12 col2\" >0.622 +/- 0.061</td>\n",
       "      <td id=\"T_5e087_row12_col3\" class=\"data row12 col3\" >0.534 +/- 0.023</td>\n",
       "      <td id=\"T_5e087_row12_col4\" class=\"data row12 col4\" >0.474 +/- 0.073</td>\n",
       "      <td id=\"T_5e087_row12_col5\" class=\"data row12 col5\" >0.482 +/- 0.058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row13\" class=\"row_heading level0 row13\" >5 AND 3</th>\n",
       "      <td id=\"T_5e087_row13_col0\" class=\"data row13 col0\" >0.539 +/- 0.034</td>\n",
       "      <td id=\"T_5e087_row13_col1\" class=\"data row13 col1\" >0.453 +/- 0.024</td>\n",
       "      <td id=\"T_5e087_row13_col2\" class=\"data row13 col2\" >0.474 +/- 0.047</td>\n",
       "      <td id=\"T_5e087_row13_col3\" class=\"data row13 col3\" >0.49 +/- 0.04</td>\n",
       "      <td id=\"T_5e087_row13_col4\" class=\"data row13 col4\" >0.479 +/- 0.019</td>\n",
       "      <td id=\"T_5e087_row13_col5\" class=\"data row13 col5\" >0.506 +/- 0.064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row14\" class=\"row_heading level0 row14\" >5 AND 4</th>\n",
       "      <td id=\"T_5e087_row14_col0\" class=\"data row14 col0\" >0.629 +/- 0.059</td>\n",
       "      <td id=\"T_5e087_row14_col1\" class=\"data row14 col1\" >0.475 +/- 0.024</td>\n",
       "      <td id=\"T_5e087_row14_col2\" class=\"data row14 col2\" >0.478 +/- 0.05</td>\n",
       "      <td id=\"T_5e087_row14_col3\" class=\"data row14 col3\" >0.646 +/- 0.033</td>\n",
       "      <td id=\"T_5e087_row14_col4\" class=\"data row14 col4\" >0.559 +/- 0.033</td>\n",
       "      <td id=\"T_5e087_row14_col5\" class=\"data row14 col5\" >0.468 +/- 0.042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row15\" class=\"row_heading level0 row15\" >6 AND 0</th>\n",
       "      <td id=\"T_5e087_row15_col0\" class=\"data row15 col0\" >0.842 +/- 0.034</td>\n",
       "      <td id=\"T_5e087_row15_col1\" class=\"data row15 col1\" >0.72 +/- 0.065</td>\n",
       "      <td id=\"T_5e087_row15_col2\" class=\"data row15 col2\" >0.705 +/- 0.039</td>\n",
       "      <td id=\"T_5e087_row15_col3\" class=\"data row15 col3\" >0.807 +/- 0.028</td>\n",
       "      <td id=\"T_5e087_row15_col4\" class=\"data row15 col4\" >0.649 +/- 0.042</td>\n",
       "      <td id=\"T_5e087_row15_col5\" class=\"data row15 col5\" >0.532 +/- 0.038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row16\" class=\"row_heading level0 row16\" >6 AND 1</th>\n",
       "      <td id=\"T_5e087_row16_col0\" class=\"data row16 col0\" >0.642 +/- 0.008</td>\n",
       "      <td id=\"T_5e087_row16_col1\" class=\"data row16 col1\" >0.658 +/- 0.02</td>\n",
       "      <td id=\"T_5e087_row16_col2\" class=\"data row16 col2\" >0.642 +/- 0.028</td>\n",
       "      <td id=\"T_5e087_row16_col3\" class=\"data row16 col3\" >0.71 +/- 0.024</td>\n",
       "      <td id=\"T_5e087_row16_col4\" class=\"data row16 col4\" >0.59 +/- 0.042</td>\n",
       "      <td id=\"T_5e087_row16_col5\" class=\"data row16 col5\" >0.531 +/- 0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row17\" class=\"row_heading level0 row17\" >6 AND 2</th>\n",
       "      <td id=\"T_5e087_row17_col0\" class=\"data row17 col0\" >0.607 +/- 0.039</td>\n",
       "      <td id=\"T_5e087_row17_col1\" class=\"data row17 col1\" >0.516 +/- 0.034</td>\n",
       "      <td id=\"T_5e087_row17_col2\" class=\"data row17 col2\" >0.499 +/- 0.016</td>\n",
       "      <td id=\"T_5e087_row17_col3\" class=\"data row17 col3\" >0.61 +/- 0.04</td>\n",
       "      <td id=\"T_5e087_row17_col4\" class=\"data row17 col4\" >0.499 +/- 0.02</td>\n",
       "      <td id=\"T_5e087_row17_col5\" class=\"data row17 col5\" >0.525 +/- 0.029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row18\" class=\"row_heading level0 row18\" >6 AND 3</th>\n",
       "      <td id=\"T_5e087_row18_col0\" class=\"data row18 col0\" >0.61 +/- 0.027</td>\n",
       "      <td id=\"T_5e087_row18_col1\" class=\"data row18 col1\" >0.562 +/- 0.049</td>\n",
       "      <td id=\"T_5e087_row18_col2\" class=\"data row18 col2\" >0.562 +/- 0.042</td>\n",
       "      <td id=\"T_5e087_row18_col3\" class=\"data row18 col3\" >0.612 +/- 0.032</td>\n",
       "      <td id=\"T_5e087_row18_col4\" class=\"data row18 col4\" >0.495 +/- 0.067</td>\n",
       "      <td id=\"T_5e087_row18_col5\" class=\"data row18 col5\" >0.543 +/- 0.033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row19\" class=\"row_heading level0 row19\" >6 AND 4</th>\n",
       "      <td id=\"T_5e087_row19_col0\" class=\"data row19 col0\" >0.603 +/- 0.049</td>\n",
       "      <td id=\"T_5e087_row19_col1\" class=\"data row19 col1\" >0.562 +/- 0.029</td>\n",
       "      <td id=\"T_5e087_row19_col2\" class=\"data row19 col2\" >0.608 +/- 0.048</td>\n",
       "      <td id=\"T_5e087_row19_col3\" class=\"data row19 col3\" >0.519 +/- 0.021</td>\n",
       "      <td id=\"T_5e087_row19_col4\" class=\"data row19 col4\" >0.606 +/- 0.06</td>\n",
       "      <td id=\"T_5e087_row19_col5\" class=\"data row19 col5\" >0.534 +/- 0.021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row20\" class=\"row_heading level0 row20\" >6 AND 5</th>\n",
       "      <td id=\"T_5e087_row20_col0\" class=\"data row20 col0\" >0.659 +/- 0.021</td>\n",
       "      <td id=\"T_5e087_row20_col1\" class=\"data row20 col1\" >0.634 +/- 0.075</td>\n",
       "      <td id=\"T_5e087_row20_col2\" class=\"data row20 col2\" >0.644 +/- 0.033</td>\n",
       "      <td id=\"T_5e087_row20_col3\" class=\"data row20 col3\" >0.558 +/- 0.045</td>\n",
       "      <td id=\"T_5e087_row20_col4\" class=\"data row20 col4\" >0.515 +/- 0.04</td>\n",
       "      <td id=\"T_5e087_row20_col5\" class=\"data row20 col5\" >0.556 +/- 0.033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row21\" class=\"row_heading level0 row21\" >7 AND 0</th>\n",
       "      <td id=\"T_5e087_row21_col0\" class=\"data row21 col0\" >0.699 +/- 0.033</td>\n",
       "      <td id=\"T_5e087_row21_col1\" class=\"data row21 col1\" >0.645 +/- 0.048</td>\n",
       "      <td id=\"T_5e087_row21_col2\" class=\"data row21 col2\" >0.663 +/- 0.047</td>\n",
       "      <td id=\"T_5e087_row21_col3\" class=\"data row21 col3\" >0.57 +/- 0.045</td>\n",
       "      <td id=\"T_5e087_row21_col4\" class=\"data row21 col4\" >0.575 +/- 0.054</td>\n",
       "      <td id=\"T_5e087_row21_col5\" class=\"data row21 col5\" >0.552 +/- 0.033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row22\" class=\"row_heading level0 row22\" >7 AND 1</th>\n",
       "      <td id=\"T_5e087_row22_col0\" class=\"data row22 col0\" >0.715 +/- 0.038</td>\n",
       "      <td id=\"T_5e087_row22_col1\" class=\"data row22 col1\" >0.528 +/- 0.045</td>\n",
       "      <td id=\"T_5e087_row22_col2\" class=\"data row22 col2\" >0.556 +/- 0.041</td>\n",
       "      <td id=\"T_5e087_row22_col3\" class=\"data row22 col3\" >0.577 +/- 0.044</td>\n",
       "      <td id=\"T_5e087_row22_col4\" class=\"data row22 col4\" >0.567 +/- 0.026</td>\n",
       "      <td id=\"T_5e087_row22_col5\" class=\"data row22 col5\" >0.518 +/- 0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row23\" class=\"row_heading level0 row23\" >7 AND 2</th>\n",
       "      <td id=\"T_5e087_row23_col0\" class=\"data row23 col0\" >0.654 +/- 0.03</td>\n",
       "      <td id=\"T_5e087_row23_col1\" class=\"data row23 col1\" >0.565 +/- 0.022</td>\n",
       "      <td id=\"T_5e087_row23_col2\" class=\"data row23 col2\" >0.582 +/- 0.027</td>\n",
       "      <td id=\"T_5e087_row23_col3\" class=\"data row23 col3\" >0.612 +/- 0.045</td>\n",
       "      <td id=\"T_5e087_row23_col4\" class=\"data row23 col4\" >0.512 +/- 0.04</td>\n",
       "      <td id=\"T_5e087_row23_col5\" class=\"data row23 col5\" >0.545 +/- 0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row24\" class=\"row_heading level0 row24\" >7 AND 3</th>\n",
       "      <td id=\"T_5e087_row24_col0\" class=\"data row24 col0\" >0.644 +/- 0.031</td>\n",
       "      <td id=\"T_5e087_row24_col1\" class=\"data row24 col1\" >0.516 +/- 0.061</td>\n",
       "      <td id=\"T_5e087_row24_col2\" class=\"data row24 col2\" >0.509 +/- 0.054</td>\n",
       "      <td id=\"T_5e087_row24_col3\" class=\"data row24 col3\" >0.523 +/- 0.01</td>\n",
       "      <td id=\"T_5e087_row24_col4\" class=\"data row24 col4\" >0.514 +/- 0.02</td>\n",
       "      <td id=\"T_5e087_row24_col5\" class=\"data row24 col5\" >0.511 +/- 0.049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row25\" class=\"row_heading level0 row25\" >7 AND 4</th>\n",
       "      <td id=\"T_5e087_row25_col0\" class=\"data row25 col0\" >0.621 +/- 0.027</td>\n",
       "      <td id=\"T_5e087_row25_col1\" class=\"data row25 col1\" >0.555 +/- 0.045</td>\n",
       "      <td id=\"T_5e087_row25_col2\" class=\"data row25 col2\" >0.565 +/- 0.05</td>\n",
       "      <td id=\"T_5e087_row25_col3\" class=\"data row25 col3\" >0.548 +/- 0.025</td>\n",
       "      <td id=\"T_5e087_row25_col4\" class=\"data row25 col4\" >0.548 +/- 0.053</td>\n",
       "      <td id=\"T_5e087_row25_col5\" class=\"data row25 col5\" >0.518 +/- 0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row26\" class=\"row_heading level0 row26\" >7 AND 5</th>\n",
       "      <td id=\"T_5e087_row26_col0\" class=\"data row26 col0\" >0.676 +/- 0.018</td>\n",
       "      <td id=\"T_5e087_row26_col1\" class=\"data row26 col1\" >0.582 +/- 0.059</td>\n",
       "      <td id=\"T_5e087_row26_col2\" class=\"data row26 col2\" >0.541 +/- 0.041</td>\n",
       "      <td id=\"T_5e087_row26_col3\" class=\"data row26 col3\" >0.546 +/- 0.03</td>\n",
       "      <td id=\"T_5e087_row26_col4\" class=\"data row26 col4\" >0.53 +/- 0.041</td>\n",
       "      <td id=\"T_5e087_row26_col5\" class=\"data row26 col5\" >0.532 +/- 0.056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row27\" class=\"row_heading level0 row27\" >7 AND 6</th>\n",
       "      <td id=\"T_5e087_row27_col0\" class=\"data row27 col0\" >0.643 +/- 0.029</td>\n",
       "      <td id=\"T_5e087_row27_col1\" class=\"data row27 col1\" >0.596 +/- 0.051</td>\n",
       "      <td id=\"T_5e087_row27_col2\" class=\"data row27 col2\" >0.596 +/- 0.048</td>\n",
       "      <td id=\"T_5e087_row27_col3\" class=\"data row27 col3\" >0.623 +/- 0.033</td>\n",
       "      <td id=\"T_5e087_row27_col4\" class=\"data row27 col4\" >0.481 +/- 0.041</td>\n",
       "      <td id=\"T_5e087_row27_col5\" class=\"data row27 col5\" >0.501 +/- 0.031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row28\" class=\"row_heading level0 row28\" >8 AND 0</th>\n",
       "      <td id=\"T_5e087_row28_col0\" class=\"data row28 col0\" >0.661 +/- 0.064</td>\n",
       "      <td id=\"T_5e087_row28_col1\" class=\"data row28 col1\" >0.604 +/- 0.037</td>\n",
       "      <td id=\"T_5e087_row28_col2\" class=\"data row28 col2\" >0.606 +/- 0.048</td>\n",
       "      <td id=\"T_5e087_row28_col3\" class=\"data row28 col3\" >0.584 +/- 0.072</td>\n",
       "      <td id=\"T_5e087_row28_col4\" class=\"data row28 col4\" >0.491 +/- 0.029</td>\n",
       "      <td id=\"T_5e087_row28_col5\" class=\"data row28 col5\" >0.486 +/- 0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row29\" class=\"row_heading level0 row29\" >8 AND 1</th>\n",
       "      <td id=\"T_5e087_row29_col0\" class=\"data row29 col0\" >0.625 +/- 0.02</td>\n",
       "      <td id=\"T_5e087_row29_col1\" class=\"data row29 col1\" >0.585 +/- 0.031</td>\n",
       "      <td id=\"T_5e087_row29_col2\" class=\"data row29 col2\" >0.577 +/- 0.033</td>\n",
       "      <td id=\"T_5e087_row29_col3\" class=\"data row29 col3\" >0.598 +/- 0.055</td>\n",
       "      <td id=\"T_5e087_row29_col4\" class=\"data row29 col4\" >0.567 +/- 0.036</td>\n",
       "      <td id=\"T_5e087_row29_col5\" class=\"data row29 col5\" >0.553 +/- 0.037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row30\" class=\"row_heading level0 row30\" >8 AND 2</th>\n",
       "      <td id=\"T_5e087_row30_col0\" class=\"data row30 col0\" >0.831 +/- 0.037</td>\n",
       "      <td id=\"T_5e087_row30_col1\" class=\"data row30 col1\" >0.721 +/- 0.021</td>\n",
       "      <td id=\"T_5e087_row30_col2\" class=\"data row30 col2\" >0.719 +/- 0.024</td>\n",
       "      <td id=\"T_5e087_row30_col3\" class=\"data row30 col3\" >0.579 +/- 0.073</td>\n",
       "      <td id=\"T_5e087_row30_col4\" class=\"data row30 col4\" >0.496 +/- 0.049</td>\n",
       "      <td id=\"T_5e087_row30_col5\" class=\"data row30 col5\" >0.491 +/- 0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row31\" class=\"row_heading level0 row31\" >8 AND 3</th>\n",
       "      <td id=\"T_5e087_row31_col0\" class=\"data row31 col0\" >0.825 +/- 0.027</td>\n",
       "      <td id=\"T_5e087_row31_col1\" class=\"data row31 col1\" >0.741 +/- 0.042</td>\n",
       "      <td id=\"T_5e087_row31_col2\" class=\"data row31 col2\" >0.723 +/- 0.047</td>\n",
       "      <td id=\"T_5e087_row31_col3\" class=\"data row31 col3\" >0.799 +/- 0.057</td>\n",
       "      <td id=\"T_5e087_row31_col4\" class=\"data row31 col4\" >0.53 +/- 0.052</td>\n",
       "      <td id=\"T_5e087_row31_col5\" class=\"data row31 col5\" >0.534 +/- 0.022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row32\" class=\"row_heading level0 row32\" >8 AND 4</th>\n",
       "      <td id=\"T_5e087_row32_col0\" class=\"data row32 col0\" >0.838 +/- 0.053</td>\n",
       "      <td id=\"T_5e087_row32_col1\" class=\"data row32 col1\" >0.715 +/- 0.057</td>\n",
       "      <td id=\"T_5e087_row32_col2\" class=\"data row32 col2\" >0.734 +/- 0.072</td>\n",
       "      <td id=\"T_5e087_row32_col3\" class=\"data row32 col3\" >0.76 +/- 0.051</td>\n",
       "      <td id=\"T_5e087_row32_col4\" class=\"data row32 col4\" >0.494 +/- 0.038</td>\n",
       "      <td id=\"T_5e087_row32_col5\" class=\"data row32 col5\" >0.559 +/- 0.047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row33\" class=\"row_heading level0 row33\" >8 AND 5</th>\n",
       "      <td id=\"T_5e087_row33_col0\" class=\"data row33 col0\" >0.852 +/- 0.031</td>\n",
       "      <td id=\"T_5e087_row33_col1\" class=\"data row33 col1\" >0.676 +/- 0.026</td>\n",
       "      <td id=\"T_5e087_row33_col2\" class=\"data row33 col2\" >0.678 +/- 0.031</td>\n",
       "      <td id=\"T_5e087_row33_col3\" class=\"data row33 col3\" >0.636 +/- 0.036</td>\n",
       "      <td id=\"T_5e087_row33_col4\" class=\"data row33 col4\" >0.522 +/- 0.034</td>\n",
       "      <td id=\"T_5e087_row33_col5\" class=\"data row33 col5\" >0.515 +/- 0.025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row34\" class=\"row_heading level0 row34\" >8 AND 6</th>\n",
       "      <td id=\"T_5e087_row34_col0\" class=\"data row34 col0\" >0.879 +/- 0.034</td>\n",
       "      <td id=\"T_5e087_row34_col1\" class=\"data row34 col1\" >0.792 +/- 0.053</td>\n",
       "      <td id=\"T_5e087_row34_col2\" class=\"data row34 col2\" >0.785 +/- 0.059</td>\n",
       "      <td id=\"T_5e087_row34_col3\" class=\"data row34 col3\" >0.554 +/- 0.024</td>\n",
       "      <td id=\"T_5e087_row34_col4\" class=\"data row34 col4\" >0.561 +/- 0.054</td>\n",
       "      <td id=\"T_5e087_row34_col5\" class=\"data row34 col5\" >0.516 +/- 0.032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row35\" class=\"row_heading level0 row35\" >8 AND 7</th>\n",
       "      <td id=\"T_5e087_row35_col0\" class=\"data row35 col0\" >0.764 +/- 0.028</td>\n",
       "      <td id=\"T_5e087_row35_col1\" class=\"data row35 col1\" >0.667 +/- 0.026</td>\n",
       "      <td id=\"T_5e087_row35_col2\" class=\"data row35 col2\" >0.672 +/- 0.033</td>\n",
       "      <td id=\"T_5e087_row35_col3\" class=\"data row35 col3\" >0.762 +/- 0.037</td>\n",
       "      <td id=\"T_5e087_row35_col4\" class=\"data row35 col4\" >0.585 +/- 0.019</td>\n",
       "      <td id=\"T_5e087_row35_col5\" class=\"data row35 col5\" >0.513 +/- 0.047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row36\" class=\"row_heading level0 row36\" >9 AND 0</th>\n",
       "      <td id=\"T_5e087_row36_col0\" class=\"data row36 col0\" >0.628 +/- 0.031</td>\n",
       "      <td id=\"T_5e087_row36_col1\" class=\"data row36 col1\" >0.648 +/- 0.032</td>\n",
       "      <td id=\"T_5e087_row36_col2\" class=\"data row36 col2\" >0.661 +/- 0.026</td>\n",
       "      <td id=\"T_5e087_row36_col3\" class=\"data row36 col3\" >0.676 +/- 0.027</td>\n",
       "      <td id=\"T_5e087_row36_col4\" class=\"data row36 col4\" >0.553 +/- 0.051</td>\n",
       "      <td id=\"T_5e087_row36_col5\" class=\"data row36 col5\" >0.534 +/- 0.029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row37\" class=\"row_heading level0 row37\" >9 AND 1</th>\n",
       "      <td id=\"T_5e087_row37_col0\" class=\"data row37 col0\" >0.644 +/- 0.049</td>\n",
       "      <td id=\"T_5e087_row37_col1\" class=\"data row37 col1\" >0.614 +/- 0.074</td>\n",
       "      <td id=\"T_5e087_row37_col2\" class=\"data row37 col2\" >0.614 +/- 0.047</td>\n",
       "      <td id=\"T_5e087_row37_col3\" class=\"data row37 col3\" >0.568 +/- 0.022</td>\n",
       "      <td id=\"T_5e087_row37_col4\" class=\"data row37 col4\" >0.54 +/- 0.019</td>\n",
       "      <td id=\"T_5e087_row37_col5\" class=\"data row37 col5\" >0.52 +/- 0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row38\" class=\"row_heading level0 row38\" >9 AND 2</th>\n",
       "      <td id=\"T_5e087_row38_col0\" class=\"data row38 col0\" >0.667 +/- 0.047</td>\n",
       "      <td id=\"T_5e087_row38_col1\" class=\"data row38 col1\" >0.712 +/- 0.029</td>\n",
       "      <td id=\"T_5e087_row38_col2\" class=\"data row38 col2\" >0.704 +/- 0.046</td>\n",
       "      <td id=\"T_5e087_row38_col3\" class=\"data row38 col3\" >0.607 +/- 0.02</td>\n",
       "      <td id=\"T_5e087_row38_col4\" class=\"data row38 col4\" >0.577 +/- 0.04</td>\n",
       "      <td id=\"T_5e087_row38_col5\" class=\"data row38 col5\" >0.569 +/- 0.041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row39\" class=\"row_heading level0 row39\" >9 AND 3</th>\n",
       "      <td id=\"T_5e087_row39_col0\" class=\"data row39 col0\" >0.755 +/- 0.046</td>\n",
       "      <td id=\"T_5e087_row39_col1\" class=\"data row39 col1\" >0.704 +/- 0.023</td>\n",
       "      <td id=\"T_5e087_row39_col2\" class=\"data row39 col2\" >0.697 +/- 0.025</td>\n",
       "      <td id=\"T_5e087_row39_col3\" class=\"data row39 col3\" >0.527 +/- 0.054</td>\n",
       "      <td id=\"T_5e087_row39_col4\" class=\"data row39 col4\" >0.556 +/- 0.016</td>\n",
       "      <td id=\"T_5e087_row39_col5\" class=\"data row39 col5\" >0.558 +/- 0.039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row40\" class=\"row_heading level0 row40\" >9 AND 4</th>\n",
       "      <td id=\"T_5e087_row40_col0\" class=\"data row40 col0\" >0.698 +/- 0.031</td>\n",
       "      <td id=\"T_5e087_row40_col1\" class=\"data row40 col1\" >0.703 +/- 0.036</td>\n",
       "      <td id=\"T_5e087_row40_col2\" class=\"data row40 col2\" >0.735 +/- 0.032</td>\n",
       "      <td id=\"T_5e087_row40_col3\" class=\"data row40 col3\" >0.703 +/- 0.019</td>\n",
       "      <td id=\"T_5e087_row40_col4\" class=\"data row40 col4\" >0.592 +/- 0.042</td>\n",
       "      <td id=\"T_5e087_row40_col5\" class=\"data row40 col5\" >0.586 +/- 0.048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row41\" class=\"row_heading level0 row41\" >9 AND 5</th>\n",
       "      <td id=\"T_5e087_row41_col0\" class=\"data row41 col0\" >0.716 +/- 0.012</td>\n",
       "      <td id=\"T_5e087_row41_col1\" class=\"data row41 col1\" >0.749 +/- 0.037</td>\n",
       "      <td id=\"T_5e087_row41_col2\" class=\"data row41 col2\" >0.737 +/- 0.041</td>\n",
       "      <td id=\"T_5e087_row41_col3\" class=\"data row41 col3\" >0.629 +/- 0.022</td>\n",
       "      <td id=\"T_5e087_row41_col4\" class=\"data row41 col4\" >0.606 +/- 0.033</td>\n",
       "      <td id=\"T_5e087_row41_col5\" class=\"data row41 col5\" >0.606 +/- 0.041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row42\" class=\"row_heading level0 row42\" >9 AND 6</th>\n",
       "      <td id=\"T_5e087_row42_col0\" class=\"data row42 col0\" >0.698 +/- 0.055</td>\n",
       "      <td id=\"T_5e087_row42_col1\" class=\"data row42 col1\" >0.715 +/- 0.049</td>\n",
       "      <td id=\"T_5e087_row42_col2\" class=\"data row42 col2\" >0.702 +/- 0.035</td>\n",
       "      <td id=\"T_5e087_row42_col3\" class=\"data row42 col3\" >0.615 +/- 0.048</td>\n",
       "      <td id=\"T_5e087_row42_col4\" class=\"data row42 col4\" >0.49 +/- 0.042</td>\n",
       "      <td id=\"T_5e087_row42_col5\" class=\"data row42 col5\" >0.507 +/- 0.033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row43\" class=\"row_heading level0 row43\" >9 AND 7</th>\n",
       "      <td id=\"T_5e087_row43_col0\" class=\"data row43 col0\" >0.706 +/- 0.039</td>\n",
       "      <td id=\"T_5e087_row43_col1\" class=\"data row43 col1\" >0.657 +/- 0.023</td>\n",
       "      <td id=\"T_5e087_row43_col2\" class=\"data row43 col2\" >0.688 +/- 0.018</td>\n",
       "      <td id=\"T_5e087_row43_col3\" class=\"data row43 col3\" >0.681 +/- 0.049</td>\n",
       "      <td id=\"T_5e087_row43_col4\" class=\"data row43 col4\" >0.525 +/- 0.033</td>\n",
       "      <td id=\"T_5e087_row43_col5\" class=\"data row43 col5\" >0.545 +/- 0.037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row44\" class=\"row_heading level0 row44\" >9 AND 8</th>\n",
       "      <td id=\"T_5e087_row44_col0\" class=\"data row44 col0\" >0.611 +/- 0.015</td>\n",
       "      <td id=\"T_5e087_row44_col1\" class=\"data row44 col1\" >0.586 +/- 0.011</td>\n",
       "      <td id=\"T_5e087_row44_col2\" class=\"data row44 col2\" >0.581 +/- 0.029</td>\n",
       "      <td id=\"T_5e087_row44_col3\" class=\"data row44 col3\" >0.579 +/- 0.035</td>\n",
       "      <td id=\"T_5e087_row44_col4\" class=\"data row44 col4\" >0.571 +/- 0.043</td>\n",
       "      <td id=\"T_5e087_row44_col5\" class=\"data row44 col5\" >0.537 +/- 0.044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fb1cc1efeb0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def color_format(s):\n",
    "    original_acc = get_acc(s[\"no_preprocessing\"])\n",
    "    # print(original_acc)\n",
    "    is_max = s == s.max()\n",
    "    color_list = []\n",
    "    for i in s:\n",
    "        # print(f'hi: {i}')\n",
    "        if get_acc(i) <= original_acc:\n",
    "            color_list.append(\"background-color: white\")\n",
    "        else:\n",
    "            color_list.append(\"background-color: rgb(100,255,100)\")\n",
    "    return color_list\n",
    "\n",
    "\n",
    "def get_acc(x):\n",
    "    return float(x.split(\"+/-\")[0].strip())\n",
    "\n",
    "\n",
    "# results_df = pd.DataFrame(results_list)\n",
    "df.set_index(\"name\", inplace=True)\n",
    "df.style.apply(color_format, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc695da5-9427-49ea-91b8-71fa48813daa",
   "metadata": {},
   "source": [
    "### Using OpenAI Clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce843fd1-fa77-4f37-9362-2de0f212f6e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbed6484-7b09-42df-b866-fcb27b80a2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"zero-shot-image-classification\", model=\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66b5d3df-e8c3-494a-ad4a-ac8b57657ec8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on ZeroShotImageClassificationPipeline in module transformers.pipelines.zero_shot_image_classification object:\n",
      "\n",
      "class ZeroShotImageClassificationPipeline(transformers.pipelines.base.Pipeline)\n",
      " |  ZeroShotImageClassificationPipeline(**kwargs)\n",
      " |  \n",
      " |  Zero shot image classification pipeline using `CLIPModel`. This pipeline predicts the class of an image when you\n",
      " |  provide an image and a set of `candidate_labels`.\n",
      " |  \n",
      " |  Example:\n",
      " |  \n",
      " |  ```python\n",
      " |  >>> from transformers import pipeline\n",
      " |  \n",
      " |  >>> classifier = pipeline(model=\"google/siglip-so400m-patch14-384\")\n",
      " |  >>> classifier(\n",
      " |  ...     \"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\",\n",
      " |  ...     candidate_labels=[\"animals\", \"humans\", \"landscape\"],\n",
      " |  ... )\n",
      " |  [{'score': 0.965, 'label': 'animals'}, {'score': 0.03, 'label': 'humans'}, {'score': 0.005, 'label': 'landscape'}]\n",
      " |  \n",
      " |  >>> classifier(\n",
      " |  ...     \"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\",\n",
      " |  ...     candidate_labels=[\"black and white\", \"photorealist\", \"painting\"],\n",
      " |  ... )\n",
      " |  [{'score': 0.996, 'label': 'black and white'}, {'score': 0.003, 'label': 'photorealist'}, {'score': 0.0, 'label': 'painting'}]\n",
      " |  ```\n",
      " |  \n",
      " |  Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)\n",
      " |  \n",
      " |  This image classification pipeline can currently be loaded from [`pipeline`] using the following task identifier:\n",
      " |  `\"zero-shot-image-classification\"`.\n",
      " |  \n",
      " |  See the list of available models on\n",
      " |  [huggingface.co/models](https://huggingface.co/models?filter=zero-shot-image-classification).\n",
      " |  \n",
      " |  Arguments:\n",
      " |      model ([`PreTrainedModel`] or [`TFPreTrainedModel`]):\n",
      " |          The model that will be used by the pipeline to make predictions. This needs to be a model inheriting from\n",
      " |          [`PreTrainedModel`] for PyTorch and [`TFPreTrainedModel`] for TensorFlow.\n",
      " |      image_processor ([`BaseImageProcessor`]):\n",
      " |          The image processor that will be used by the pipeline to encode data for the model. This object inherits from\n",
      " |          [`BaseImageProcessor`].\n",
      " |      modelcard (`str` or [`ModelCard`], *optional*):\n",
      " |          Model card attributed to the model for this pipeline.\n",
      " |      framework (`str`, *optional*):\n",
      " |          The framework to use, either `\"pt\"` for PyTorch or `\"tf\"` for TensorFlow. The specified framework must be\n",
      " |          installed.\n",
      " |  \n",
      " |          If no framework is specified, will default to the one currently installed. If no framework is specified and\n",
      " |          both frameworks are installed, will default to the framework of the `model`, or to PyTorch if no model is\n",
      " |          provided.\n",
      " |      task (`str`, defaults to `\"\"`):\n",
      " |          A task-identifier for the pipeline.\n",
      " |      num_workers (`int`, *optional*, defaults to 8):\n",
      " |          When the pipeline will use *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number of\n",
      " |          workers to be used.\n",
      " |      batch_size (`int`, *optional*, defaults to 1):\n",
      " |          When the pipeline will use *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of\n",
      " |          the batch to use, for inference this is not always beneficial, please read [Batching with\n",
      " |          pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching) .\n",
      " |      args_parser ([`~pipelines.ArgumentHandler`], *optional*):\n",
      " |          Reference to the object in charge of parsing supplied pipeline parameters.\n",
      " |      device (`int`, *optional*, defaults to -1):\n",
      " |          Device ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model on\n",
      " |          the associated CUDA device id. You can pass native `torch.device` or a `str` too\n",
      " |      torch_dtype (`str` or `torch.dtype`, *optional*):\n",
      " |          Sent directly as `model_kwargs` (just a simpler shortcut) to use the available precision for this model\n",
      " |          (`torch.float16`, `torch.bfloat16`, ... or `\"auto\"`)\n",
      " |      binary_output (`bool`, *optional*, defaults to `False`):\n",
      " |          Flag indicating if the output the pipeline should happen in a serialized format (i.e., pickle) or as\n",
      " |          the raw output data e.g. text.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      ZeroShotImageClassificationPipeline\n",
      " |      transformers.pipelines.base.Pipeline\n",
      " |      transformers.pipelines.base._ScikitCompat\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __call__(self, images: Union[str, List[str], ForwardRef('Image'), List[ForwardRef('Image')]], **kwargs)\n",
      " |      Assign labels to the image(s) passed as inputs.\n",
      " |      \n",
      " |      Args:\n",
      " |          images (`str`, `List[str]`, `PIL.Image` or `List[PIL.Image]`):\n",
      " |              The pipeline handles three types of images:\n",
      " |      \n",
      " |              - A string containing a http link pointing to an image\n",
      " |              - A string containing a local path to an image\n",
      " |              - An image loaded in PIL directly\n",
      " |      \n",
      " |          candidate_labels (`List[str]`):\n",
      " |              The candidate labels for this image\n",
      " |      \n",
      " |          hypothesis_template (`str`, *optional*, defaults to `\"This is a photo of {}\"`):\n",
      " |              The sentence used in cunjunction with *candidate_labels* to attempt the image classification by\n",
      " |              replacing the placeholder with the candidate_labels. Then likelihood is estimated by using\n",
      " |              logits_per_image\n",
      " |      \n",
      " |          timeout (`float`, *optional*, defaults to None):\n",
      " |              The maximum time in seconds to wait for fetching images from the web. If None, no timeout is set and\n",
      " |              the call may block forever.\n",
      " |      \n",
      " |      Return:\n",
      " |          A list of dictionaries containing result, one dictionary per proposed label. The dictionaries contain the\n",
      " |          following keys:\n",
      " |      \n",
      " |          - **label** (`str`) -- The label identified by the model. It is one of the suggested `candidate_label`.\n",
      " |          - **score** (`float`) -- The score attributed by the model for that label (between 0 and 1).\n",
      " |  \n",
      " |  __init__(self, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  postprocess(self, model_outputs)\n",
      " |      Postprocess will receive the raw outputs of the `_forward` method, generally tensors, and reformat them into\n",
      " |      something more friendly. Generally it will output a list or a dict or results (containing just strings and\n",
      " |      numbers).\n",
      " |  \n",
      " |  preprocess(self, image, candidate_labels=None, hypothesis_template='This is a photo of {}.', timeout=None)\n",
      " |      Preprocess will take the `input_` of a specific pipeline and return a dictionary of everything necessary for\n",
      " |      `_forward` to run properly. It should contain at least one tensor, but might have arbitrary other items.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.pipelines.base.Pipeline:\n",
      " |  \n",
      " |  check_model_type(self, supported_models: Union[List[str], dict])\n",
      " |      Check if the model class is in supported by the pipeline.\n",
      " |      \n",
      " |      Args:\n",
      " |          supported_models (`List[str]` or `dict`):\n",
      " |              The list of models supported by the pipeline, or a dictionary with model class values.\n",
      " |  \n",
      " |  device_placement(self)\n",
      " |      Context Manager allowing tensor allocation on the user-specified device in framework agnostic way.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Context manager\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      # Explicitly ask for tensor allocation on CUDA device :0\n",
      " |      pipe = pipeline(..., device=0)\n",
      " |      with pipe.device_placement():\n",
      " |          # Every framework specific tensor allocation will be done on the request device\n",
      " |          output = pipe(...)\n",
      " |      ```\n",
      " |  \n",
      " |  ensure_tensor_on_device(self, **inputs)\n",
      " |      Ensure PyTorch tensors are on the specified device.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs (keyword arguments that should be `torch.Tensor`, the rest is ignored):\n",
      " |              The tensors to place on `self.device`.\n",
      " |          Recursive on lists **only**.\n",
      " |      \n",
      " |      Return:\n",
      " |          `Dict[str, torch.Tensor]`: The same as `inputs` but on the proper device.\n",
      " |  \n",
      " |  forward(self, model_inputs, **forward_params)\n",
      " |  \n",
      " |  get_inference_context(self)\n",
      " |  \n",
      " |  get_iterator(self, inputs, num_workers: int, batch_size: int, preprocess_params, forward_params, postprocess_params)\n",
      " |  \n",
      " |  iterate(self, inputs, preprocess_params, forward_params, postprocess_params)\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Scikit / Keras interface to transformers' pipelines. This method will forward to __call__().\n",
      " |  \n",
      " |  run_multi(self, inputs, preprocess_params, forward_params, postprocess_params)\n",
      " |  \n",
      " |  run_single(self, inputs, preprocess_params, forward_params, postprocess_params)\n",
      " |  \n",
      " |  save_pretrained(self, save_directory: str, safe_serialization: bool = True)\n",
      " |      Save the pipeline's model and tokenizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          save_directory (`str`):\n",
      " |              A path to the directory where to saved. It will be created if it doesn't exist.\n",
      " |          safe_serialization (`str`):\n",
      " |              Whether to save the model using `safetensors` or the traditional way for PyTorch or Tensorflow.\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Scikit / Keras interface to transformers' pipelines. This method will forward to __call__().\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from transformers.pipelines.base.Pipeline:\n",
      " |  \n",
      " |  default_input_names = None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from transformers.pipelines.base._ScikitCompat:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a0453cfe-b4b1-4044-b493-2e7d80d190ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'airplane', 'score': 0.9744154214859009},\n",
      " {'label': 'bird', 'score': 0.012951435521245003},\n",
      " {'label': 'ship', 'score': 0.008248009718954563},\n",
      " {'label': 'automobile', 'score': 0.0018931111553683877},\n",
      " {'label': 'dog', 'score': 0.000702335499227047},\n",
      " {'label': 'truck', 'score': 0.0006072412943467498},\n",
      " {'label': 'cat', 'score': 0.0004406006191857159},\n",
      " {'label': 'deer', 'score': 0.00033803354017436504},\n",
      " {'label': 'horse', 'score': 0.00031132795265875757},\n",
      " {'label': 'frog', 'score': 9.248455171473324e-05}]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "sample_pred = pipe(\n",
    "    cifar10_ds[\"train\"][0][\"img\"],\n",
    "    candidate_labels=[\n",
    "        \"airplane\",\n",
    "        \"automobile\",\n",
    "        \"bird\",\n",
    "        \"cat\",\n",
    "        \"deer\",\n",
    "        \"dog\",\n",
    "        \"frog\",\n",
    "        \"horse\",\n",
    "        \"ship\",\n",
    "        \"truck\",\n",
    "    ],\n",
    ")\n",
    "pprint(sample_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0edc22aa-eb04-473b-b6c1-d5ec87ad6485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=32x32 at 0x7F0350D42E00>\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(cifar10_ds[\"train\"][0][\"img\"])\n",
    "print(cifar10_ds[\"train\"][0][\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04bcc1f-5878-43d7-9108-3be412d39a25",
   "metadata": {},
   "source": [
    "### Binary Classification Using Clip\n",
    "+ Target dataset is either label 0 (airplane) or 1 (automobile)\n",
    "+ For the background dataset, we want to only use instances where the score for label 0 or 1 is larger than a threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a6a39153-aa0a-403f-adeb-84a242099297",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "\n",
    "inputs = processor(\n",
    "    text=[\"a photo of a cat\", \"a photo of a dog\"],\n",
    "    images=image,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    ")\n",
    "output = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "247f1f16-3d21-4a8d-9f66-b8db81d000be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[49406,   320,  1125,   539,   320,  2368, 49407],\n",
       "        [49406,   320,  1125,   539,   320,  1929, 49407]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ee830f1f-ea1f-4774-b1f7-df394148aa27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits_per_image', 'logits_per_text', 'text_embeds', 'image_embeds', 'text_model_output', 'vision_model_output'])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "18b6b1dd-43de-44da-aa69-608e507812d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPooling(last_hidden_state=tensor([[[-0.1391,  0.1016, -0.2362,  ...,  0.1568,  0.2143,  0.2181],\n",
       "         [ 0.1976,  0.4183, -0.8494,  ...,  0.1030,  1.4616,  0.4321],\n",
       "         [ 0.1630,  0.2004, -1.0608,  ..., -0.0640,  1.3778,  0.5354],\n",
       "         ...,\n",
       "         [ 0.0552, -0.0113, -0.6040,  ...,  0.2935,  1.0303, -0.0316],\n",
       "         [ 0.1673,  0.0491, -0.3833,  ...,  0.3797,  0.9275, -0.0212],\n",
       "         [ 0.1937,  0.2567, -0.4325,  ...,  0.2957,  1.2151, -0.0417]]],\n",
       "       grad_fn=<AddBackward0>), pooler_output=tensor([[-3.5264e-01,  4.3304e-02, -7.5685e-01, -8.6568e-01,  5.1499e-01,\n",
       "         -8.6238e-01,  3.0487e-01,  1.0063e+00,  4.8464e-01,  4.2416e-01,\n",
       "          2.1265e+00, -6.3059e-01, -2.0583e+00,  4.7908e-02,  2.4651e-01,\n",
       "          1.5391e-01,  1.1603e+00,  9.8606e-01,  1.1290e+00, -6.3667e-01,\n",
       "          3.4923e-01,  9.5762e-01,  3.4354e-01,  1.7032e-01,  2.7315e-01,\n",
       "         -1.5358e-01,  3.5460e-01, -3.6673e-01,  2.2665e-01,  1.4825e+00,\n",
       "         -5.5609e+00,  4.7867e-01, -3.6525e-02,  6.9109e-02,  9.1081e-01,\n",
       "          9.7474e-01,  1.3259e+00, -1.4460e-01, -1.0853e+00, -3.7624e-01,\n",
       "          3.5872e-02,  2.3670e+00,  1.3561e-02, -1.2212e-01, -1.5952e+00,\n",
       "          8.5968e-01,  8.5326e-01, -4.8963e-01, -1.0861e+00,  3.1368e-01,\n",
       "          2.6086e-01,  9.9086e-01,  4.4735e-02, -6.0607e-01, -2.5588e-02,\n",
       "         -2.5954e-01,  2.6184e-01,  3.6858e-01,  5.3657e-01, -4.7886e-01,\n",
       "         -9.9796e-01, -5.2461e-01,  7.5558e-01, -8.6403e-01, -8.2988e-02,\n",
       "         -1.2506e-01, -6.2009e-01, -3.4873e-01,  2.8018e-01, -6.4279e-02,\n",
       "          5.8928e-01,  9.2975e-01,  2.3892e-02, -1.7725e+00,  6.8918e-01,\n",
       "          2.3316e-01,  4.8994e-01, -6.8690e-03, -3.9914e-01, -5.4403e-01,\n",
       "          1.2570e+00, -1.6068e-01,  1.0599e+00,  1.1801e+00,  2.0259e-01,\n",
       "          6.1690e-02,  9.2588e-01,  2.9386e-01,  1.1666e-02,  2.0448e-01,\n",
       "          4.6831e-01,  2.3376e-01,  2.5640e+00,  6.9256e-01,  3.3700e-01,\n",
       "          5.3429e-01,  3.2651e-01, -9.4718e-01, -7.1590e-02, -3.3778e-01,\n",
       "         -1.0915e+00,  1.0146e+00,  4.8727e-01,  1.2883e-01,  1.5267e+00,\n",
       "          1.2702e+00,  1.0522e-01,  6.1814e-01,  1.2620e+00, -9.7498e-01,\n",
       "         -8.3319e-01,  1.0517e+00,  1.0112e+00,  5.0059e-01,  6.3184e-01,\n",
       "         -6.1085e-01,  9.8586e-01, -4.7894e+00,  7.0911e-01,  1.2716e+00,\n",
       "          9.4411e-03, -1.3824e+00,  1.3820e+00, -9.4865e-01,  1.3560e+00,\n",
       "         -3.0087e-02,  3.3432e-01,  2.8334e-01, -2.1543e+00, -2.9065e-01,\n",
       "          6.9320e-01,  5.6451e-01,  7.8557e-01,  5.6714e-01,  6.4115e-01,\n",
       "         -4.0370e-01,  9.2214e-01,  3.5810e-01,  4.1939e-02, -5.5594e-01,\n",
       "          7.5722e-01,  2.3432e-01,  5.0104e-01,  5.5496e-02,  2.0550e-01,\n",
       "         -2.1337e-01, -1.9733e-01,  1.1409e+00,  6.9875e-01,  1.7283e+00,\n",
       "         -7.6343e-01, -3.2098e-02, -3.1697e-01, -7.2258e-01,  5.8814e-01,\n",
       "          4.9162e-01,  1.3317e+00, -5.7261e-01,  5.1979e-02,  2.5553e+00,\n",
       "          8.2094e-02,  1.5233e+00, -8.4682e-01, -3.1632e-01,  3.1814e-01,\n",
       "         -1.2002e-02, -4.0892e-01,  2.8272e+00,  9.8166e-01,  1.8623e-03,\n",
       "         -1.1622e+00,  1.7501e-01,  3.4614e-01,  9.1047e-02,  9.7296e-01,\n",
       "          1.0803e-01,  8.7221e-01,  1.5142e+00,  5.8995e-01, -2.5457e-01,\n",
       "          9.0521e-01, -7.8983e-01, -1.6377e+00, -2.0730e+00, -1.4363e-01,\n",
       "          4.6563e-01, -7.8027e-02,  3.7469e-01,  7.6621e-01,  2.4610e-01,\n",
       "          9.8579e-01,  1.2886e+00, -9.8026e-01,  1.2338e+00, -8.0253e-01,\n",
       "          1.0827e-01,  6.4560e-01, -3.6977e-01,  3.5115e-01, -1.9270e-02,\n",
       "          1.8087e+00, -1.1130e+00, -1.0118e+00, -3.1947e-01,  1.2213e-01,\n",
       "         -3.3319e-01, -7.6511e-01, -1.3321e+00,  4.4757e-01, -3.6747e-01,\n",
       "          1.1015e+00,  2.0999e-01,  1.2471e-01, -1.3555e-01, -3.2647e-01,\n",
       "         -5.1078e-01,  1.7271e+00, -1.1488e-01,  1.4335e+00,  2.5693e+00,\n",
       "         -1.3075e+00, -4.7947e-02,  4.6487e-02,  6.6219e-01, -3.8170e-01,\n",
       "          6.4457e-01,  1.4556e+00, -5.9513e-01, -2.5588e-01, -5.0467e-01,\n",
       "         -7.0425e-01, -3.8041e-01,  3.7561e-01, -6.9994e-01, -9.1622e-01,\n",
       "         -1.2066e+00,  1.0132e+00, -1.8947e-01,  1.4076e+00,  3.1970e-01,\n",
       "         -5.2296e-01, -3.1013e-01,  1.1451e+00,  2.1767e-01, -5.1022e-01,\n",
       "          3.6925e-01,  4.8536e-01,  9.7659e-01, -5.2282e-01, -8.7919e-01,\n",
       "         -1.8793e-01,  3.1892e-01, -1.0917e+00,  7.9159e-01, -5.4996e-02,\n",
       "         -1.3775e+00,  8.3843e-01,  1.3465e+00,  1.7894e+00,  3.6971e-02,\n",
       "         -6.7165e-02,  3.2308e-01,  9.1129e-02, -3.3263e-01,  2.9222e-02,\n",
       "          6.2664e-01,  6.3750e-01,  6.3640e-01,  1.4893e+00, -9.3224e-01,\n",
       "          9.9525e-01,  1.9778e+00,  1.1538e+00,  5.7606e-01, -4.5982e-01,\n",
       "         -7.5805e-01,  4.8339e-01, -2.5749e-01,  4.2240e-01, -3.2489e-01,\n",
       "         -1.6969e-01,  1.3746e-01, -9.2024e-01,  1.8619e-03,  1.6877e+00,\n",
       "          1.3015e+00,  6.9480e-01,  6.4740e-01, -1.5567e-02,  3.2907e-01,\n",
       "          3.4862e-01, -1.2985e-01, -4.1677e-01,  7.7095e-02,  2.3429e-01,\n",
       "         -2.8761e-01,  9.5210e-01, -1.0645e-01,  1.8884e-01,  8.5905e-01,\n",
       "          2.1797e-02,  1.9875e+00,  1.3699e+00, -4.0271e-01, -1.5127e-01,\n",
       "          1.5301e+00, -4.9888e-01, -4.6305e-02, -2.3626e+00,  2.0374e+00,\n",
       "          8.4862e-01,  3.5139e-01, -3.7457e-01,  1.6747e+00,  1.1257e+00,\n",
       "          1.8909e-01, -8.5011e-02,  5.7352e-01,  3.5760e-01, -7.8200e-01,\n",
       "         -3.6456e-01,  1.3278e+00,  3.8621e-01,  1.7694e+00, -9.9532e-01,\n",
       "         -7.6960e-01,  2.0353e-01,  8.4319e-01,  5.7375e-01, -1.2118e+00,\n",
       "         -5.8213e-01,  2.2399e-01,  3.1518e-01,  7.8852e-01, -3.0443e-01,\n",
       "          9.1613e-01, -8.4484e+00,  8.0768e-01, -3.1251e-01, -5.2107e-01,\n",
       "          2.2688e+00, -1.0966e+00, -4.2557e-01,  8.1576e-01,  3.9901e-02,\n",
       "          5.6879e-01, -3.4454e-01,  4.9169e-01, -3.7721e-01,  1.9447e+00,\n",
       "         -2.7399e+00, -5.0647e-01, -6.4821e-01, -4.6581e-01, -2.3602e-01,\n",
       "         -4.2327e-01,  2.6264e-01,  1.0275e-01,  5.7667e-01, -3.7875e-01,\n",
       "         -5.4397e-01,  1.5267e+00,  1.6838e+00, -3.2419e-01,  8.0147e-01,\n",
       "         -1.5630e+00, -1.2249e+00,  5.5589e-01,  7.7242e-01,  6.0412e-01,\n",
       "          2.8980e-02, -4.1701e-01,  1.3650e+00,  2.2362e+00,  1.8796e-01,\n",
       "          3.4491e-01, -5.1059e-01, -4.9391e-01,  3.6328e-01,  2.0818e-01,\n",
       "          1.1217e+00, -2.6375e-02,  1.1703e+00, -5.1401e-01,  4.4867e-01,\n",
       "         -1.5572e+00, -9.0921e-01, -3.0844e-01,  3.0260e-01, -3.3211e-02,\n",
       "         -1.7382e-01, -1.1408e+00, -1.6635e-01, -1.3524e+00, -1.8079e-01,\n",
       "          5.8820e-01,  1.7260e+00,  8.5461e-01,  4.3257e-01,  2.5352e-01,\n",
       "          1.0146e+00,  3.3303e-01, -2.0897e+00, -5.4013e-01, -1.5557e-01,\n",
       "         -6.6893e-01,  1.9901e-01,  6.1228e-01,  1.3019e+00,  7.2215e-01,\n",
       "          6.3090e-01, -9.8380e-01, -1.3003e+00, -7.9498e-01,  1.1795e-01,\n",
       "          1.8986e+00,  4.5840e-01,  1.1443e+00, -6.3730e-01, -4.1015e-01,\n",
       "          3.4007e-02,  1.0921e+00,  1.5790e+00,  1.2586e+00, -3.5369e-01,\n",
       "          8.9323e-01, -8.6981e-02,  8.8079e-01,  4.3065e-01,  6.3814e-02,\n",
       "          8.2852e-02, -4.0830e-01,  6.6507e-01,  3.2531e-01, -5.2296e-01,\n",
       "          1.3427e-01,  9.1604e-01,  5.2759e-01,  9.0018e-01,  8.5370e-01,\n",
       "         -8.4900e-01, -4.4072e-01,  4.6849e-01,  1.2493e+00,  7.8004e-01,\n",
       "          1.6397e+00,  6.5921e-01, -2.1974e-01,  6.1917e-01,  2.0731e-01,\n",
       "         -1.7788e-01, -1.4708e-01,  1.3430e+00,  8.6353e-01, -5.2284e-01,\n",
       "          9.7586e-01,  1.9234e-01, -6.5110e-01,  4.0379e-01,  8.0056e-01,\n",
       "          1.7967e+00, -1.7286e-01,  8.9490e-01,  2.5221e+00,  9.0101e-01,\n",
       "         -9.3334e-01,  2.7572e-01, -1.0808e+00,  1.6058e-01,  1.0822e+00,\n",
       "          1.0253e-01,  4.5653e-01,  9.3660e-02,  6.7689e-01, -9.1576e-01,\n",
       "         -3.6836e-02,  1.1317e+00, -5.4547e-01, -4.2518e-01, -3.2765e-01,\n",
       "          3.4597e-01,  2.4114e-01,  5.9621e-01, -9.1706e-01,  2.7427e+00,\n",
       "          7.0460e-01,  7.9421e-01, -4.0319e-02,  3.0880e-01, -9.0315e-02,\n",
       "         -1.4865e+00, -6.1038e-01,  2.7091e-01, -2.0323e+00,  4.2787e-01,\n",
       "          6.9593e-02, -2.8328e-01,  5.3013e-01,  1.0530e+00,  1.5350e+00,\n",
       "         -5.3865e-01,  5.6356e-01,  1.6915e+00,  1.1779e+00,  1.1519e+00,\n",
       "         -2.3383e-01,  1.7798e-01,  2.3030e-01,  3.2136e-01, -2.3101e-01,\n",
       "          3.6188e-01,  6.9796e-01,  4.3757e-01,  1.1945e+00,  4.1193e-01,\n",
       "          5.6153e-01, -9.0861e-01,  6.1336e-01,  7.2461e-01,  6.0849e-01,\n",
       "          9.6069e-01,  5.3904e-01, -5.8923e-01,  3.8153e-01,  6.4641e-01,\n",
       "          1.8049e-01, -2.9474e-01,  1.0262e-01,  1.0224e+00,  1.7094e-01,\n",
       "          1.6581e+00,  4.4954e-01, -3.4850e-01, -1.0262e-01,  9.1800e-01,\n",
       "          4.3916e-01,  9.7942e-01,  1.3337e+00,  7.1814e-01,  8.1293e-01,\n",
       "          1.0831e+00, -1.5260e-01, -1.2266e+00, -5.7365e-01,  1.1101e+00,\n",
       "          6.9343e-01, -6.3199e-01,  7.6481e-01, -1.8917e-01, -4.7711e-01,\n",
       "         -1.3847e+00,  4.2010e-01,  5.7761e-01,  2.5993e-01, -4.4268e-01,\n",
       "         -3.6427e-01, -7.2298e-01, -3.3101e-01, -1.0469e+00, -6.8637e-01,\n",
       "          1.4998e+00, -3.4543e-01, -3.2169e-01, -1.6048e+00, -4.2532e-02,\n",
       "          4.8982e-01,  7.3723e-02,  2.4397e+00, -1.4931e-01,  5.9426e-02,\n",
       "          1.5988e+00, -2.6175e-01,  8.3588e-01,  1.0288e+00,  3.8859e-01,\n",
       "          6.3030e-01,  6.6436e-01, -2.1051e+00,  5.5732e-01,  6.4834e-01,\n",
       "         -2.4150e-01,  5.6953e-01, -3.1635e-01,  3.0064e-01, -1.7850e-01,\n",
       "         -1.4502e-01, -1.3084e+00,  8.6729e-02,  3.6628e-01,  1.0513e+00,\n",
       "          7.1542e-01, -1.7165e+00, -2.6288e-01, -6.2765e-01, -1.3292e+00,\n",
       "          1.3024e-01,  9.9063e-01, -2.3000e-01,  1.9100e-01,  8.9494e-01,\n",
       "         -9.3877e-01,  9.8972e-01, -1.0331e+00, -1.3647e+00,  9.7453e-01,\n",
       "          1.9128e-01, -3.5180e-02,  5.5247e-01, -7.7066e-01,  5.3998e-02,\n",
       "         -6.7170e-01,  1.3228e+00,  2.0382e-01, -3.3264e-01, -1.1146e+00,\n",
       "          1.2722e-01, -7.5402e-01,  1.4116e+00,  1.6019e-01, -3.0468e-01,\n",
       "         -3.7354e-01, -1.5376e-02, -1.0627e-01,  4.2561e-01, -1.8824e+00,\n",
       "          1.4664e+00,  3.6314e-02,  1.2299e+00, -5.6897e-02,  9.6516e-01,\n",
       "          5.5794e-01,  9.2031e-01,  1.2710e+00,  9.0449e-01,  4.6179e-01,\n",
       "          8.4392e-01,  3.5557e-01, -3.6685e-01,  9.5495e-01, -3.6487e-01,\n",
       "          3.0229e-02,  8.8992e-01,  3.4275e-01, -4.1095e-01, -6.6879e-01,\n",
       "          7.2809e-01, -9.9564e-01, -1.8790e-01, -8.4203e-01, -5.1992e-01,\n",
       "         -1.5432e+00,  5.0339e-01,  4.7730e-01,  7.9181e-01, -1.1965e-01,\n",
       "          1.2073e-01,  6.3031e-01, -2.5428e-01,  8.5587e-03,  1.4234e+00,\n",
       "          1.1224e+00, -9.4434e-01,  6.9661e-01, -1.9215e+00,  9.7057e-01,\n",
       "          9.7977e-01,  1.0569e+00,  1.5133e+00,  1.9372e+00,  1.7260e+00,\n",
       "          1.0939e+00,  4.9478e-01, -9.8562e-02,  5.7189e-01, -1.8576e-01,\n",
       "          1.0061e+00,  2.7609e-02,  2.8225e-01,  7.0827e-01,  1.7172e-02,\n",
       "         -1.1758e-01,  1.1422e+00, -5.4135e-01,  4.5000e-01,  1.3905e+00,\n",
       "          1.4221e-01,  3.4242e-01, -1.1105e+00,  1.2511e-01, -9.2104e-01,\n",
       "         -1.6576e-02, -9.1303e-01,  1.8744e-01,  1.3073e+00,  1.0134e+00,\n",
       "          2.3634e-01,  2.2447e-02, -5.9267e-01,  9.1468e-01, -1.8535e+00,\n",
       "         -7.3108e-01,  6.0575e-01,  2.8953e-01,  6.4549e-01, -7.4716e-02,\n",
       "         -2.9338e-01,  1.8899e+00,  1.2505e-01,  6.1557e-01,  1.5365e-01,\n",
       "          1.0182e+00,  1.1015e+00, -5.8102e-01, -2.0597e-01,  8.3999e-01,\n",
       "          1.5381e+00, -1.7670e+00,  3.4802e-01,  1.5627e+00, -1.5446e+00,\n",
       "          1.0482e+00,  1.4200e+00,  2.1023e+00,  2.1359e-01, -1.5668e-01,\n",
       "         -1.2933e+00,  7.9156e-01, -5.6790e-01,  5.2077e-01,  3.7834e-01,\n",
       "         -4.0609e-02,  3.6113e-01, -3.2922e-01,  1.2945e+00, -4.3657e-01,\n",
       "          1.4449e+00,  6.0915e-01,  2.2024e+00, -1.6672e-01, -1.1214e-01,\n",
       "         -3.2493e-01, -1.6206e-01,  8.0187e-01,  8.0760e-01, -8.3740e-01,\n",
       "          1.1507e+00,  1.0677e+00,  6.2035e-01, -5.0882e-01, -1.7383e+00,\n",
       "          1.5221e+00,  1.0109e-01,  7.6414e-01,  1.0567e+00, -5.6092e-02,\n",
       "         -9.5583e-02, -4.3652e-03, -5.1400e-01,  1.9584e+00, -1.1616e+00,\n",
       "          2.8383e+00,  8.7326e-01,  1.2737e+00, -2.4023e-01,  8.4994e-01,\n",
       "          6.0325e-01,  1.0523e+00,  1.0862e+00]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[\"vision_model_output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3643f89c-ae6a-43f3-8388-d52579bff497",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:41<00:00,  2.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22, 3072)\n",
      "(22,)\n",
      "(62, 3072)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "X_target = []\n",
    "X_background = []\n",
    "y_target = []\n",
    "y_background = []\n",
    "\n",
    "THRESHOLD = 1e-2\n",
    "\n",
    "for i in trange(100):\n",
    "    if cifar10_ds[\"train\"][i][\"label\"] in [0, 1]:\n",
    "        X_target.append(np.asarray(cifar10_ds[\"train\"][i][\"img\"]).flatten())\n",
    "        y_target.append(cifar10_ds[\"train\"][i][\"label\"])\n",
    "    else:\n",
    "        sample_pred = pipe(\n",
    "            cifar10_ds[\"train\"][i][\"img\"],\n",
    "            candidate_labels=[\n",
    "                \"airplane\",\n",
    "                \"automobile\",\n",
    "                \"bird\",\n",
    "                \"cat\",\n",
    "                \"deer\",\n",
    "                \"dog\",\n",
    "                \"frog\",\n",
    "                \"horse\",\n",
    "                \"ship\",\n",
    "                \"truck\",\n",
    "            ],\n",
    "        )\n",
    "        for tmp_dict in sample_pred:\n",
    "            if (\n",
    "                tmp_dict[\"label\"] in [\"airplane\", \"automobile\"]\n",
    "                and tmp_dict[\"score\"] > THRESHOLD\n",
    "            ):\n",
    "                X_background.append(np.asarray(cifar10_ds[\"train\"][i][\"img\"]).flatten())\n",
    "\n",
    "X_target = np.asarray(X_target)\n",
    "y_target = np.asarray(y_target)\n",
    "X_background = np.asarray(X_background)\n",
    "\n",
    "print(X_target.shape)\n",
    "print(y_target.shape)\n",
    "print(X_background.shape)\n",
    "\n",
    "# TODO: save clip embeddings as a file (i.e. pickle or .pt, and then upload + cos similarity) // ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "90733292-e842-4863-b6bf-c9e2707fc45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 22, 2)\n"
     ]
    }
   ],
   "source": [
    "from contrastive import CPCA\n",
    "\n",
    "cpca = CPCA()\n",
    "X_cpca = cpca.fit_transform(X_target[:, 0:1000], X_background[:, 0:1000])\n",
    "X_cpca = np.asarray(X_cpca)\n",
    "print(X_cpca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "92f96d9f-b02d-4055-a190-95b0d35e884f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27\n",
      "0.27\n",
      "0.27\n",
      "0.27\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "for i in range(4):\n",
    "    knn = KNeighborsClassifier()\n",
    "    scores = cross_val_score(knn, X_target, y_target, cv=5)\n",
    "    print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d05029f-bffd-4aea-89eb-b3e84083d52b",
   "metadata": {},
   "source": [
    "### Using Google ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2efd7a75-e363-40f2-8b92-e9139e3c21b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_vit_pipe = pipeline(\"image-classification\", model=\"google/vit-base-patch16-224\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "811221d4-9851-4bc4-9b91-7be2ee854556",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on ImageClassificationPipeline in module transformers.pipelines.image_classification object:\n",
      "\n",
      "class ImageClassificationPipeline(transformers.pipelines.base.Pipeline)\n",
      " |  ImageClassificationPipeline(*args, **kwargs)\n",
      " |  \n",
      " |  Image classification pipeline using any `AutoModelForImageClassification`. This pipeline predicts the class of an\n",
      " |  image.\n",
      " |  \n",
      " |  Example:\n",
      " |  \n",
      " |  ```python\n",
      " |  >>> from transformers import pipeline\n",
      " |  \n",
      " |  >>> classifier = pipeline(model=\"microsoft/beit-base-patch16-224-pt22k-ft22k\")\n",
      " |  >>> classifier(\"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\")\n",
      " |  [{'score': 0.442, 'label': 'macaw'}, {'score': 0.088, 'label': 'popinjay'}, {'score': 0.075, 'label': 'parrot'}, {'score': 0.073, 'label': 'parodist, lampooner'}, {'score': 0.046, 'label': 'poll, poll_parrot'}]\n",
      " |  ```\n",
      " |  \n",
      " |  Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)\n",
      " |  \n",
      " |  This image classification pipeline can currently be loaded from [`pipeline`] using the following task identifier:\n",
      " |  `\"image-classification\"`.\n",
      " |  \n",
      " |  See the list of available models on\n",
      " |  [huggingface.co/models](https://huggingface.co/models?filter=image-classification).\n",
      " |  \n",
      " |  Arguments:\n",
      " |      model ([`PreTrainedModel`] or [`TFPreTrainedModel`]):\n",
      " |          The model that will be used by the pipeline to make predictions. This needs to be a model inheriting from\n",
      " |          [`PreTrainedModel`] for PyTorch and [`TFPreTrainedModel`] for TensorFlow.\n",
      " |      image_processor ([`BaseImageProcessor`]):\n",
      " |          The image processor that will be used by the pipeline to encode data for the model. This object inherits from\n",
      " |          [`BaseImageProcessor`].\n",
      " |      modelcard (`str` or [`ModelCard`], *optional*):\n",
      " |          Model card attributed to the model for this pipeline.\n",
      " |      framework (`str`, *optional*):\n",
      " |          The framework to use, either `\"pt\"` for PyTorch or `\"tf\"` for TensorFlow. The specified framework must be\n",
      " |          installed.\n",
      " |  \n",
      " |          If no framework is specified, will default to the one currently installed. If no framework is specified and\n",
      " |          both frameworks are installed, will default to the framework of the `model`, or to PyTorch if no model is\n",
      " |          provided.\n",
      " |      task (`str`, defaults to `\"\"`):\n",
      " |          A task-identifier for the pipeline.\n",
      " |      num_workers (`int`, *optional*, defaults to 8):\n",
      " |          When the pipeline will use *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number of\n",
      " |          workers to be used.\n",
      " |      batch_size (`int`, *optional*, defaults to 1):\n",
      " |          When the pipeline will use *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of\n",
      " |          the batch to use, for inference this is not always beneficial, please read [Batching with\n",
      " |          pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching) .\n",
      " |      args_parser ([`~pipelines.ArgumentHandler`], *optional*):\n",
      " |          Reference to the object in charge of parsing supplied pipeline parameters.\n",
      " |      device (`int`, *optional*, defaults to -1):\n",
      " |          Device ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model on\n",
      " |          the associated CUDA device id. You can pass native `torch.device` or a `str` too\n",
      " |      torch_dtype (`str` or `torch.dtype`, *optional*):\n",
      " |          Sent directly as `model_kwargs` (just a simpler shortcut) to use the available precision for this model\n",
      " |          (`torch.float16`, `torch.bfloat16`, ... or `\"auto\"`)\n",
      " |      binary_output (`bool`, *optional*, defaults to `False`):\n",
      " |          Flag indicating if the output the pipeline should happen in a serialized format (i.e., pickle) or as\n",
      " |          the raw output data e.g. text.\n",
      " |      function_to_apply (`str`, *optional*, defaults to `\"default\"`):\n",
      " |          The function to apply to the model outputs in order to retrieve the scores. Accepts four different values:\n",
      " |  \n",
      " |          - `\"default\"`: if the model has a single label, will apply the sigmoid function on the output. If the model\n",
      " |            has several labels, will apply the softmax function on the output.\n",
      " |          - `\"sigmoid\"`: Applies the sigmoid function on the output.\n",
      " |          - `\"softmax\"`: Applies the softmax function on the output.\n",
      " |          - `\"none\"`: Does not apply any function on the output.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      ImageClassificationPipeline\n",
      " |      transformers.pipelines.base.Pipeline\n",
      " |      transformers.pipelines.base._ScikitCompat\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __call__(self, images: Union[str, List[str], ForwardRef('Image.Image'), List[ForwardRef('Image.Image')]], **kwargs)\n",
      " |      Assign labels to the image(s) passed as inputs.\n",
      " |      \n",
      " |      Args:\n",
      " |          images (`str`, `List[str]`, `PIL.Image` or `List[PIL.Image]`):\n",
      " |              The pipeline handles three types of images:\n",
      " |      \n",
      " |              - A string containing a http link pointing to an image\n",
      " |              - A string containing a local path to an image\n",
      " |              - An image loaded in PIL directly\n",
      " |      \n",
      " |              The pipeline accepts either a single image or a batch of images, which must then be passed as a string.\n",
      " |              Images in a batch must all be in the same format: all as http links, all as local paths, or all as PIL\n",
      " |              images.\n",
      " |          function_to_apply (`str`, *optional*, defaults to `\"default\"`):\n",
      " |              The function to apply to the model outputs in order to retrieve the scores. Accepts four different\n",
      " |              values:\n",
      " |      \n",
      " |              If this argument is not specified, then it will apply the following functions according to the number\n",
      " |              of labels:\n",
      " |      \n",
      " |              - If the model has a single label, will apply the sigmoid function on the output.\n",
      " |              - If the model has several labels, will apply the softmax function on the output.\n",
      " |      \n",
      " |              Possible values are:\n",
      " |      \n",
      " |              - `\"sigmoid\"`: Applies the sigmoid function on the output.\n",
      " |              - `\"softmax\"`: Applies the softmax function on the output.\n",
      " |              - `\"none\"`: Does not apply any function on the output.\n",
      " |          top_k (`int`, *optional*, defaults to 5):\n",
      " |              The number of top labels that will be returned by the pipeline. If the provided number is higher than\n",
      " |              the number of labels available in the model configuration, it will default to the number of labels.\n",
      " |          timeout (`float`, *optional*, defaults to None):\n",
      " |              The maximum time in seconds to wait for fetching images from the web. If None, no timeout is set and\n",
      " |              the call may block forever.\n",
      " |      \n",
      " |      Return:\n",
      " |          A dictionary or a list of dictionaries containing result. If the input is a single image, will return a\n",
      " |          dictionary, if the input is a list of several images, will return a list of dictionaries corresponding to\n",
      " |          the images.\n",
      " |      \n",
      " |          The dictionaries contain the following keys:\n",
      " |      \n",
      " |          - **label** (`str`) -- The label identified by the model.\n",
      " |          - **score** (`int`) -- The score attributed by the model for that label.\n",
      " |  \n",
      " |  __init__(self, *args, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  postprocess(self, model_outputs, function_to_apply=None, top_k=5)\n",
      " |      Postprocess will receive the raw outputs of the `_forward` method, generally tensors, and reformat them into\n",
      " |      something more friendly. Generally it will output a list or a dict or results (containing just strings and\n",
      " |      numbers).\n",
      " |  \n",
      " |  preprocess(self, image, timeout=None)\n",
      " |      Preprocess will take the `input_` of a specific pipeline and return a dictionary of everything necessary for\n",
      " |      `_forward` to run properly. It should contain at least one tensor, but might have arbitrary other items.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {'function_to_apply': <enum 'ClassificationFunction'...\n",
      " |  \n",
      " |  function_to_apply = <ClassificationFunction.NONE: 'none'>\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.pipelines.base.Pipeline:\n",
      " |  \n",
      " |  check_model_type(self, supported_models: Union[List[str], dict])\n",
      " |      Check if the model class is in supported by the pipeline.\n",
      " |      \n",
      " |      Args:\n",
      " |          supported_models (`List[str]` or `dict`):\n",
      " |              The list of models supported by the pipeline, or a dictionary with model class values.\n",
      " |  \n",
      " |  device_placement(self)\n",
      " |      Context Manager allowing tensor allocation on the user-specified device in framework agnostic way.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Context manager\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      # Explicitly ask for tensor allocation on CUDA device :0\n",
      " |      pipe = pipeline(..., device=0)\n",
      " |      with pipe.device_placement():\n",
      " |          # Every framework specific tensor allocation will be done on the request device\n",
      " |          output = pipe(...)\n",
      " |      ```\n",
      " |  \n",
      " |  ensure_tensor_on_device(self, **inputs)\n",
      " |      Ensure PyTorch tensors are on the specified device.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs (keyword arguments that should be `torch.Tensor`, the rest is ignored):\n",
      " |              The tensors to place on `self.device`.\n",
      " |          Recursive on lists **only**.\n",
      " |      \n",
      " |      Return:\n",
      " |          `Dict[str, torch.Tensor]`: The same as `inputs` but on the proper device.\n",
      " |  \n",
      " |  forward(self, model_inputs, **forward_params)\n",
      " |  \n",
      " |  get_inference_context(self)\n",
      " |  \n",
      " |  get_iterator(self, inputs, num_workers: int, batch_size: int, preprocess_params, forward_params, postprocess_params)\n",
      " |  \n",
      " |  iterate(self, inputs, preprocess_params, forward_params, postprocess_params)\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Scikit / Keras interface to transformers' pipelines. This method will forward to __call__().\n",
      " |  \n",
      " |  run_multi(self, inputs, preprocess_params, forward_params, postprocess_params)\n",
      " |  \n",
      " |  run_single(self, inputs, preprocess_params, forward_params, postprocess_params)\n",
      " |  \n",
      " |  save_pretrained(self, save_directory: str, safe_serialization: bool = True)\n",
      " |      Save the pipeline's model and tokenizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          save_directory (`str`):\n",
      " |              A path to the directory where to saved. It will be created if it doesn't exist.\n",
      " |          safe_serialization (`str`):\n",
      " |              Whether to save the model using `safetensors` or the traditional way for PyTorch or Tensorflow.\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Scikit / Keras interface to transformers' pipelines. This method will forward to __call__().\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from transformers.pipelines.base.Pipeline:\n",
      " |  \n",
      " |  default_input_names = None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from transformers.pipelines.base._ScikitCompat:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(google_vit_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e42daf-0da0-4ab9-af38-5873ecc53f96",
   "metadata": {},
   "source": [
    "HF Blog: https://huggingface.co/blog/MarkusStoll/interactive-hf-space-to-visualize-image-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca78d2d9-e495-4b8f-a0d2-2b0c0c59f4a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.39.3-py3-none-any.whl.metadata (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Downloading transformers-4.39.3-py3-none-any.whl (8.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m774.0/774.0 kB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, regex, tokenizers, transformers\n",
      "Successfully installed regex-2023.12.25 safetensors-0.4.2 tokenizers-0.15.2 transformers-4.39.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee999f79cedf49539067ea68462cf0b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf6bf40717174122a4234bf3645f8a49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "352e8a6a4cf34411b299475e4cfe2be2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"google/vit-base-patch16-224-in21k\"\n",
    "processor = transformers.ViTImageProcessor.from_pretrained(model_name)\n",
    "cls_model = transformers.ViTForImageClassification.from_pretrained(model_name).to(\n",
    "    device\n",
    ")\n",
    "fe_model = transformers.ViTModel.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2d0a9c44-f3c0-4d2e-ad81-83896bd1a578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "def infer(batch):\n",
    "    # print('hi!')\n",
    "    images = [image.convert(\"RGB\") for image in batch]\n",
    "    inputs = processor(images=images, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = cls_model(**inputs)\n",
    "        probs = torch.nn.functional.softmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "        embeddings = fe_model(**inputs).last_hidden_state[:, 0].cpu().numpy()\n",
    "    return {\"embedding\": embeddings}\n",
    "\n",
    "\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef48d7d-b8a0-43e1-9a27-e846e3a140a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images = image.convert(\"RGB\") for image in\n",
    "with torch.no_grad():\n",
    "    outputs = cls_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f209557b-2940-40ae-bb78-eefb1a9b4d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_ds_small = cifar10_ds[\"train\"].select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "68291823-e9b9-4a79-ac6e-022083679c99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "110023a42b9f4a1bb57a55e50cd27c1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds_enrichments = cifar10_ds_small.map(\n",
    "    infer, input_columns=\"img\", batched=True, batch_size=20\n",
    ").remove_columns([\"img\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ecd8f986-8fdf-4fee-a514-09e0a46e96e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.10191544890403748,\n",
       " 0.12705735862255096,\n",
       " -0.3053702712059021,\n",
       " -0.29277658462524414,\n",
       " -0.08739897608757019,\n",
       " -0.2789906859397888,\n",
       " -0.12070721387863159,\n",
       " -0.22004324197769165,\n",
       " -0.09731496125459671,\n",
       " 0.23732300102710724,\n",
       " -0.2460750788450241,\n",
       " 0.2482222318649292,\n",
       " -0.003959480673074722,\n",
       " -0.11801853030920029,\n",
       " 0.09823556989431381,\n",
       " -0.3013449013233185,\n",
       " 0.051646213978528976,\n",
       " -0.021946806460618973,\n",
       " 0.15900658071041107,\n",
       " -0.2672097384929657,\n",
       " 0.12404245883226395,\n",
       " -0.10307228565216064,\n",
       " -0.10493819415569305,\n",
       " 0.01658962108194828,\n",
       " -0.024667609483003616,\n",
       " -0.03955630585551262,\n",
       " 0.1589890420436859,\n",
       " 0.13561886548995972,\n",
       " -0.3056838810443878,\n",
       " 0.015412827022373676,\n",
       " -0.20067720115184784,\n",
       " -0.07610426098108292,\n",
       " 0.08080829679965973,\n",
       " 0.12081564217805862,\n",
       " 0.2308061569929123,\n",
       " 0.2581796944141388,\n",
       " 0.12905488908290863,\n",
       " -0.09925127029418945,\n",
       " -0.04863332211971283,\n",
       " 0.07180126756429672,\n",
       " 0.06924854218959808,\n",
       " -0.12364786118268967,\n",
       " -0.1865655779838562,\n",
       " -0.1618955284357071,\n",
       " -0.054468799382448196,\n",
       " -0.10355595499277115,\n",
       " -0.31729617714881897,\n",
       " 0.022212384268641472,\n",
       " -0.12224380671977997,\n",
       " -0.21713954210281372,\n",
       " -0.12586815655231476,\n",
       " -0.0669897273182869,\n",
       " -0.12626895308494568,\n",
       " -0.07169608771800995,\n",
       " -0.13580016791820526,\n",
       " -0.20175296068191528,\n",
       " -0.20388516783714294,\n",
       " 0.04822881147265434,\n",
       " -0.2941688299179077,\n",
       " -0.08412890136241913,\n",
       " -0.1586059182882309,\n",
       " -0.10959170758724213,\n",
       " -0.4353145956993103,\n",
       " -0.036305319517850876,\n",
       " -0.19745498895645142,\n",
       " 0.02994524873793125,\n",
       " 0.12988656759262085,\n",
       " -0.03819027170538902,\n",
       " 0.0813998356461525,\n",
       " -0.2923937141895294,\n",
       " 0.13247887790203094,\n",
       " -0.002651770133525133,\n",
       " -0.06135711073875427,\n",
       " -0.05313118174672127,\n",
       " 0.1999378353357315,\n",
       " 0.26445430517196655,\n",
       " -0.04206730052828789,\n",
       " 0.017851917073130608,\n",
       " -0.1251647025346756,\n",
       " -0.11341780424118042,\n",
       " 0.04476070776581764,\n",
       " -0.04985591396689415,\n",
       " -0.0034611974842846394,\n",
       " 0.13769756257534027,\n",
       " 0.3096473217010498,\n",
       " -0.14646977186203003,\n",
       " -0.05537639185786247,\n",
       " 0.10204575955867767,\n",
       " 0.01508475374430418,\n",
       " 0.11016780883073807,\n",
       " -0.05810210853815079,\n",
       " -0.3303794860839844,\n",
       " 0.06403711438179016,\n",
       " -0.05050033703446388,\n",
       " 0.06123245507478714,\n",
       " -0.04727184772491455,\n",
       " 0.1436697393655777,\n",
       " -0.3699028789997101,\n",
       " 0.03505100682377815,\n",
       " 0.16675208508968353,\n",
       " 0.14776155352592468,\n",
       " -0.1973818838596344,\n",
       " 0.17936579883098602,\n",
       " -0.002225847914814949,\n",
       " -0.19043761491775513,\n",
       " 0.019394714385271072,\n",
       " 0.251962810754776,\n",
       " 0.02118491940200329,\n",
       " 0.1554640680551529,\n",
       " -0.15536531805992126,\n",
       " 0.12622123956680298,\n",
       " -0.06183808296918869,\n",
       " -0.22646625339984894,\n",
       " -0.2730518579483032,\n",
       " -0.12790891528129578,\n",
       " -0.13466882705688477,\n",
       " -0.12395307421684265,\n",
       " -0.2091272920370102,\n",
       " -0.035160668194293976,\n",
       " -0.10645274072885513,\n",
       " 0.19853907823562622,\n",
       " -0.11381208151578903,\n",
       " -0.45816707611083984,\n",
       " -0.2135993093252182,\n",
       " -0.043998610228300095,\n",
       " -0.14569918811321259,\n",
       " -0.16006740927696228,\n",
       " -0.15990103781223297,\n",
       " 0.12350732088088989,\n",
       " -0.19122622907161713,\n",
       " 0.143777996301651,\n",
       " 0.03288501501083374,\n",
       " 0.03840462863445282,\n",
       " 0.2332594245672226,\n",
       " 0.16736628115177155,\n",
       " -0.046837951987981796,\n",
       " 0.013711268082261086,\n",
       " 0.05047258362174034,\n",
       " 0.1251346617937088,\n",
       " -0.2311481535434723,\n",
       " 0.047613002359867096,\n",
       " -0.1441454291343689,\n",
       " 0.2473459243774414,\n",
       " -0.03008965030312538,\n",
       " 0.08130214363336563,\n",
       " -0.12314032018184662,\n",
       " 0.1181139424443245,\n",
       " 0.026244454085826874,\n",
       " -0.17688821256160736,\n",
       " -0.32190412282943726,\n",
       " -0.1701194941997528,\n",
       " 0.15858285129070282,\n",
       " 0.04407964646816254,\n",
       " 0.027953673154115677,\n",
       " -0.0036451092455536127,\n",
       " -0.25465020537376404,\n",
       " -0.15971320867538452,\n",
       " -0.08450423181056976,\n",
       " 0.20319950580596924,\n",
       " -0.17540906369686127,\n",
       " 0.11137647926807404,\n",
       " 0.14141973853111267,\n",
       " -0.09658000618219376,\n",
       " 0.037591006606817245,\n",
       " 0.08395236730575562,\n",
       " 0.0077025406062603,\n",
       " -0.06831706315279007,\n",
       " -0.01668483577668667,\n",
       " -0.3474496006965637,\n",
       " 0.014084632508456707,\n",
       " -0.26231399178504944,\n",
       " 0.34095945954322815,\n",
       " -0.04935380816459656,\n",
       " -0.01805247738957405,\n",
       " -0.12535642087459564,\n",
       " 0.24859187006950378,\n",
       " -0.08851746469736099,\n",
       " 0.03831550478935242,\n",
       " 0.35313764214515686,\n",
       " 0.0016857987502589822,\n",
       " -0.025433959439396858,\n",
       " 0.05007219687104225,\n",
       " -0.21379265189170837,\n",
       " -0.3037862181663513,\n",
       " -0.3034360110759735,\n",
       " -0.27490952610969543,\n",
       " -0.2240581512451172,\n",
       " -0.11253267526626587,\n",
       " -0.19684627652168274,\n",
       " 0.3413945734500885,\n",
       " 0.11448851227760315,\n",
       " 0.2618420720100403,\n",
       " -7.674985681660473e-05,\n",
       " 0.1680445373058319,\n",
       " 0.13394752144813538,\n",
       " -0.11704336106777191,\n",
       " 0.08980631828308105,\n",
       " 0.04833924397826195,\n",
       " 0.12004975229501724,\n",
       " 0.11179893463850021,\n",
       " 0.1414014846086502,\n",
       " -0.14079788327217102,\n",
       " 0.1302211582660675,\n",
       " -0.4187181890010834,\n",
       " 0.03897566720843315,\n",
       " 0.03774861618876457,\n",
       " -0.20382261276245117,\n",
       " -0.2667449414730072,\n",
       " -0.053679727017879486,\n",
       " -0.10174273699522018,\n",
       " -0.282134085893631,\n",
       " -0.3656032085418701,\n",
       " -0.12574848532676697,\n",
       " -0.014258605428040028,\n",
       " 0.239275261759758,\n",
       " -0.06782174110412598,\n",
       " -0.1395004838705063,\n",
       " -0.23177002370357513,\n",
       " -0.3890003561973572,\n",
       " 0.3278145492076874,\n",
       " -0.15988627076148987,\n",
       " -0.06602326780557632,\n",
       " -0.14941278100013733,\n",
       " -0.11488957703113556,\n",
       " 0.31103935837745667,\n",
       " -0.1467587649822235,\n",
       " 0.04384688660502434,\n",
       " -0.017657173797488213,\n",
       " -0.10149900615215302,\n",
       " 0.11364991962909698,\n",
       " -0.13165877759456635,\n",
       " -0.058627378195524216,\n",
       " -0.0729488953948021,\n",
       " -0.19421231746673584,\n",
       " -0.14023354649543762,\n",
       " -0.17692044377326965,\n",
       " -0.0882643535733223,\n",
       " 0.11479703336954117,\n",
       " 0.0656195878982544,\n",
       " -0.11117043346166611,\n",
       " 0.2688685953617096,\n",
       " -0.12468591332435608,\n",
       " 0.2836993932723999,\n",
       " -0.2815496623516083,\n",
       " 0.046867113560438156,\n",
       " 0.0025605165865272284,\n",
       " -0.050720155239105225,\n",
       " -0.05199361965060234,\n",
       " 0.007741814479231834,\n",
       " -0.12405920773744583,\n",
       " 0.2095034420490265,\n",
       " 0.03837808221578598,\n",
       " -0.030957553535699844,\n",
       " -0.10461931675672531,\n",
       " -0.06761608272790909,\n",
       " 0.003295090515166521,\n",
       " -0.07488078624010086,\n",
       " 0.0357108935713768,\n",
       " -0.09580937772989273,\n",
       " -0.1646336317062378,\n",
       " 0.20674200356006622,\n",
       " -0.14097562432289124,\n",
       " -0.22419364750385284,\n",
       " 0.05991349369287491,\n",
       " 0.31330394744873047,\n",
       " -0.2205689251422882,\n",
       " 0.15804825723171234,\n",
       " 0.10156350582838058,\n",
       " -0.07511455565690994,\n",
       " -0.12809953093528748,\n",
       " -0.08510781079530716,\n",
       " -0.22798267006874084,\n",
       " -0.03491366282105446,\n",
       " 0.016032584011554718,\n",
       " -0.06631505489349365,\n",
       " -0.23245978355407715,\n",
       " -0.09080298244953156,\n",
       " -0.0705026164650917,\n",
       " -0.15970133244991302,\n",
       " -0.08714521676301956,\n",
       " -0.1265394389629364,\n",
       " -0.20600099861621857,\n",
       " 0.06425004452466965,\n",
       " -0.0640910342335701,\n",
       " -0.2542933225631714,\n",
       " -0.2791042625904083,\n",
       " 0.27130866050720215,\n",
       " 0.08759664744138718,\n",
       " 0.10487740486860275,\n",
       " -0.013025003485381603,\n",
       " 0.10018156468868256,\n",
       " 0.011553117074072361,\n",
       " -0.06305776536464691,\n",
       " 0.15207360684871674,\n",
       " 0.00836085807532072,\n",
       " -0.1784430742263794,\n",
       " -0.03746609762310982,\n",
       " -0.0785440132021904,\n",
       " -0.058003854006528854,\n",
       " 0.10199934244155884,\n",
       " -0.22786536812782288,\n",
       " 0.06017690524458885,\n",
       " -0.05739149451255798,\n",
       " -0.2132846862077713,\n",
       " 0.013340593315660954,\n",
       " -0.1737198680639267,\n",
       " -0.003539165947586298,\n",
       " -0.08488363772630692,\n",
       " -0.21849237382411957,\n",
       " 0.2664147913455963,\n",
       " 0.1436997801065445,\n",
       " 0.11918417364358902,\n",
       " 0.12089131772518158,\n",
       " 0.019523251801729202,\n",
       " 0.11660633981227875,\n",
       " -0.17977659404277802,\n",
       " -0.04153301194310188,\n",
       " 0.14618661999702454,\n",
       " 0.41016238927841187,\n",
       " 0.08281733095645905,\n",
       " 0.145405575633049,\n",
       " -0.14976905286312103,\n",
       " 0.24344663321971893,\n",
       " 0.337702214717865,\n",
       " 0.06434661149978638,\n",
       " -0.27205222845077515,\n",
       " -0.1170167624950409,\n",
       " -0.1651107519865036,\n",
       " 0.009844820015132427,\n",
       " -0.09568222612142563,\n",
       " 0.16522642970085144,\n",
       " -0.21702606976032257,\n",
       " 0.04497329145669937,\n",
       " 0.180877223610878,\n",
       " 0.2563354969024658,\n",
       " -0.000277379818726331,\n",
       " -0.09864490479230881,\n",
       " -0.19423849880695343,\n",
       " 0.047597482800483704,\n",
       " 0.04208226874470711,\n",
       " -0.15157552063465118,\n",
       " -0.20247219502925873,\n",
       " 0.005927742458879948,\n",
       " 0.0685252696275711,\n",
       " -0.11773889511823654,\n",
       " -0.06328243762254715,\n",
       " -0.516261637210846,\n",
       " 0.17361928522586823,\n",
       " -0.023194484412670135,\n",
       " 0.21010223031044006,\n",
       " -0.0957832783460617,\n",
       " -0.015127787366509438,\n",
       " -0.15768533945083618,\n",
       " -0.058716289699077606,\n",
       " -0.057152941823005676,\n",
       " 0.33464840054512024,\n",
       " -0.4274452328681946,\n",
       " -0.2318645417690277,\n",
       " 0.1390572190284729,\n",
       " -0.20067185163497925,\n",
       " -0.003462789813056588,\n",
       " -0.11118131875991821,\n",
       " 0.04316404461860657,\n",
       " -0.05010802671313286,\n",
       " 0.1054096519947052,\n",
       " 0.1137508749961853,\n",
       " -0.20892316102981567,\n",
       " 0.30326929688453674,\n",
       " -0.011080619879066944,\n",
       " -0.13102255761623383,\n",
       " 0.11277163028717041,\n",
       " -0.029340432956814766,\n",
       " 0.0483296737074852,\n",
       " -0.057053469121456146,\n",
       " 0.017956333234906197,\n",
       " -0.10276500135660172,\n",
       " 0.20293854176998138,\n",
       " -0.11989115923643112,\n",
       " -0.08276955038309097,\n",
       " 0.30233171582221985,\n",
       " 0.11136012524366379,\n",
       " -0.029217686504125595,\n",
       " -0.03707575425505638,\n",
       " -0.1331559717655182,\n",
       " -0.16006135940551758,\n",
       " 0.0825335830450058,\n",
       " 0.14084994792938232,\n",
       " 0.01971784234046936,\n",
       " 0.11872793734073639,\n",
       " -0.05354888737201691,\n",
       " -0.21854858100414276,\n",
       " -0.03580784052610397,\n",
       " -0.38016295433044434,\n",
       " -0.19994521141052246,\n",
       " 0.16176660358905792,\n",
       " 0.10199541598558426,\n",
       " -0.0881439596414566,\n",
       " 0.08902356773614883,\n",
       " -0.18276190757751465,\n",
       " -0.010507275350391865,\n",
       " -0.20064419507980347,\n",
       " 0.01401037722826004,\n",
       " 0.10705003887414932,\n",
       " 0.019176390022039413,\n",
       " -0.1058947741985321,\n",
       " 0.09491245448589325,\n",
       " -0.11657179147005081,\n",
       " -0.16247813403606415,\n",
       " -0.1489008516073227,\n",
       " 0.12121003121137619,\n",
       " 0.18091119825839996,\n",
       " -0.15706998109817505,\n",
       " 0.03200560063123703,\n",
       " -0.2203763872385025,\n",
       " 0.09284332394599915,\n",
       " 0.07318535447120667,\n",
       " 0.10710279643535614,\n",
       " 0.02258792333304882,\n",
       " 0.0689462274312973,\n",
       " 0.07690916210412979,\n",
       " -0.016752906143665314,\n",
       " 0.14712150394916534,\n",
       " -0.288593053817749,\n",
       " 0.006781802512705326,\n",
       " -0.1471090465784073,\n",
       " 0.34466296434402466,\n",
       " -0.04810800403356552,\n",
       " 0.22643044590950012,\n",
       " 0.0493483692407608,\n",
       " -0.40881502628326416,\n",
       " -0.12901903688907623,\n",
       " -0.012878055684268475,\n",
       " -0.0916937068104744,\n",
       " 0.18375994265079498,\n",
       " -0.27868586778640747,\n",
       " 0.007002880796790123,\n",
       " -0.25277552008628845,\n",
       " 0.09020684659481049,\n",
       " -0.05743171647191048,\n",
       " 0.336559921503067,\n",
       " -0.26518046855926514,\n",
       " 0.05189317464828491,\n",
       " 0.14870911836624146,\n",
       " -0.16100181639194489,\n",
       " 0.030377443879842758,\n",
       " 0.09203042089939117,\n",
       " -0.13973809778690338,\n",
       " -0.16101320087909698,\n",
       " -0.33142274618148804,\n",
       " -0.2525591552257538,\n",
       " -0.060419999063014984,\n",
       " 0.037986405193805695,\n",
       " 0.045677751302719116,\n",
       " 0.005739472806453705,\n",
       " 0.003950446378439665,\n",
       " 0.1787441372871399,\n",
       " 0.37578970193862915,\n",
       " -0.05798128992319107,\n",
       " -0.04432760551571846,\n",
       " 0.0035302340984344482,\n",
       " -0.19949348270893097,\n",
       " 0.08007658272981644,\n",
       " -0.02365180104970932,\n",
       " 0.10765095800161362,\n",
       " 0.262869268655777,\n",
       " 0.064254991710186,\n",
       " -0.10852514952421188,\n",
       " 0.32696226239204407,\n",
       " 0.29504016041755676,\n",
       " -0.19978584349155426,\n",
       " 0.027559539303183556,\n",
       " -0.1038394421339035,\n",
       " -0.014331916347146034,\n",
       " -0.04016411677002907,\n",
       " -0.21428915858268738,\n",
       " 0.09744751453399658,\n",
       " -0.1440076380968094,\n",
       " 0.16191540658473969,\n",
       " 0.3155859708786011,\n",
       " 0.0851358100771904,\n",
       " 0.283796489238739,\n",
       " 0.17140133678913116,\n",
       " 0.10051964968442917,\n",
       " -0.17501196265220642,\n",
       " -0.22185200452804565,\n",
       " 0.061520934104919434,\n",
       " -0.11417972296476364,\n",
       " 0.31320929527282715,\n",
       " 0.1483236700296402,\n",
       " 0.28622499108314514,\n",
       " -0.1734854131937027,\n",
       " -0.09586197137832642,\n",
       " 0.10188853740692139,\n",
       " 0.12212958931922913,\n",
       " 0.11871964484453201,\n",
       " -0.045574236661195755,\n",
       " -0.05751115083694458,\n",
       " -0.2098759412765503,\n",
       " -0.0673743337392807,\n",
       " 0.02224121056497097,\n",
       " 0.04464014619588852,\n",
       " 0.012665391899645329,\n",
       " -0.06699423491954803,\n",
       " -0.2835777997970581,\n",
       " 0.0490601509809494,\n",
       " -0.0894547700881958,\n",
       " 0.21242201328277588,\n",
       " -0.035031579434871674,\n",
       " -0.20261146128177643,\n",
       " -0.12731365859508514,\n",
       " -0.11506803333759308,\n",
       " 0.29573220014572144,\n",
       " -0.01669713482260704,\n",
       " -0.07615263015031815,\n",
       " 0.1162833645939827,\n",
       " -0.180632546544075,\n",
       " 0.2095940113067627,\n",
       " -0.012230325490236282,\n",
       " 0.08342326432466507,\n",
       " 0.0899445042014122,\n",
       " 0.16913209855556488,\n",
       " 0.25012144446372986,\n",
       " 0.16988009214401245,\n",
       " 0.06410591304302216,\n",
       " -0.023727914318442345,\n",
       " 0.057016756385564804,\n",
       " 0.04588116705417633,\n",
       " -0.13752895593643188,\n",
       " 0.11244886368513107,\n",
       " 0.05301765725016594,\n",
       " 0.2836143374443054,\n",
       " -0.16925832629203796,\n",
       " -0.13226525485515594,\n",
       " -0.010229475796222687,\n",
       " 0.020514484494924545,\n",
       " -0.08966624736785889,\n",
       " -0.05390958860516548,\n",
       " -0.10704965889453888,\n",
       " -0.10792213678359985,\n",
       " -0.029791465029120445,\n",
       " 0.04730546846985817,\n",
       " -0.0788455605506897,\n",
       " -0.05359339714050293,\n",
       " 0.09588837623596191,\n",
       " -0.06232697516679764,\n",
       " 0.22645524144172668,\n",
       " -0.030238788574934006,\n",
       " -0.11274755001068115,\n",
       " 0.23515164852142334,\n",
       " 0.2975029945373535,\n",
       " 0.12880054116249084,\n",
       " -0.3470059633255005,\n",
       " 0.12159454822540283,\n",
       " 0.4416790306568146,\n",
       " -0.037165068089962006,\n",
       " 0.15710119903087616,\n",
       " 0.03371236100792885,\n",
       " -0.012567668221890926,\n",
       " -0.03639046847820282,\n",
       " 0.18271058797836304,\n",
       " -0.2254694253206253,\n",
       " -0.18538136780261993,\n",
       " 0.12076069414615631,\n",
       " -0.07101604342460632,\n",
       " 0.22248722612857819,\n",
       " -0.20395450294017792,\n",
       " 0.12776759266853333,\n",
       " 0.009105226956307888,\n",
       " -0.10177980363368988,\n",
       " -0.08370684832334518,\n",
       " 0.32034891843795776,\n",
       " 0.04582977667450905,\n",
       " 0.08962642401456833,\n",
       " 0.14263004064559937,\n",
       " -0.2845616042613983,\n",
       " 0.026661479845643044,\n",
       " -0.09561926871538162,\n",
       " 0.0612034797668457,\n",
       " 0.17889027297496796,\n",
       " -0.1291583925485611,\n",
       " -0.2697334289550781,\n",
       " -0.02707134187221527,\n",
       " 0.03354238346219063,\n",
       " -0.14849938452243805,\n",
       " 0.04133129492402077,\n",
       " 0.04908549413084984,\n",
       " 0.12231236696243286,\n",
       " -0.026065751910209656,\n",
       " 0.16919350624084473,\n",
       " -0.1686159074306488,\n",
       " 0.2536395788192749,\n",
       " 0.0011475783539935946,\n",
       " 0.22112330794334412,\n",
       " -0.2667723596096039,\n",
       " 0.16556353867053986,\n",
       " -0.05668070912361145,\n",
       " 0.174921914935112,\n",
       " 0.3030511140823364,\n",
       " 0.32324308156967163,\n",
       " 0.06471626460552216,\n",
       " -0.14436781406402588,\n",
       " -0.12102466821670532,\n",
       " 0.17746026813983917,\n",
       " -0.2250186800956726,\n",
       " -0.026154955849051476,\n",
       " -0.10214298218488693,\n",
       " 0.16041578352451324,\n",
       " 0.009107152000069618,\n",
       " -0.33135461807250977,\n",
       " -0.21645335853099823,\n",
       " -0.21406380832195282,\n",
       " 0.015695013105869293,\n",
       " -0.16833750903606415,\n",
       " -0.008289966732263565,\n",
       " 0.08131739497184753,\n",
       " -0.03792713209986687,\n",
       " 0.28185153007507324,\n",
       " 0.03202398866415024,\n",
       " -0.01360744796693325,\n",
       " 0.2475365847349167,\n",
       " -0.025686338543891907,\n",
       " -0.04827989637851715,\n",
       " -0.11784851551055908,\n",
       " 0.02349744737148285,\n",
       " -0.25586211681365967,\n",
       " 0.16336478292942047,\n",
       " 0.09723927825689316,\n",
       " -0.20742033421993256,\n",
       " -0.0016995025798678398,\n",
       " 0.015097023919224739,\n",
       " -0.0315064862370491,\n",
       " 0.06387840211391449,\n",
       " 0.0019439273746684194,\n",
       " 0.38171064853668213,\n",
       " -0.028647303581237793,\n",
       " -0.03420344740152359,\n",
       " -0.15229104459285736,\n",
       " -0.23202858865261078,\n",
       " 0.02026555873453617,\n",
       " -0.0016098878113552928,\n",
       " 0.039506200700998306,\n",
       " -0.16470137238502502,\n",
       " -0.02674303762614727,\n",
       " 0.0479896180331707,\n",
       " -0.33909857273101807,\n",
       " -0.13274362683296204,\n",
       " 0.09098503738641739,\n",
       " 0.3092495799064636,\n",
       " 0.14674489200115204,\n",
       " -0.23108021914958954,\n",
       " 0.37257081270217896,\n",
       " 0.09735216200351715,\n",
       " -0.012313324958086014,\n",
       " -0.16658447682857513,\n",
       " -0.10987813770771027,\n",
       " -0.10816285759210587,\n",
       " 0.019238248467445374,\n",
       " 0.06121142953634262,\n",
       " 0.16291631758213043,\n",
       " 0.12446966767311096,\n",
       " -0.004713974427431822,\n",
       " 0.13173624873161316,\n",
       " -0.0420268289744854,\n",
       " 0.19995221495628357,\n",
       " -0.07451939582824707,\n",
       " -0.032388679683208466,\n",
       " -0.17562620341777802,\n",
       " -0.09619129449129105,\n",
       " 0.036486588418483734,\n",
       " -0.17889949679374695,\n",
       " -0.1187240332365036,\n",
       " -0.08489887416362762,\n",
       " -0.2287389039993286,\n",
       " -0.07783684879541397,\n",
       " -0.09188723564147949,\n",
       " -0.15322813391685486,\n",
       " -0.279655784368515,\n",
       " -0.05751792713999748,\n",
       " -0.05635995790362358,\n",
       " 0.19774968922138214,\n",
       " -0.04189758375287056,\n",
       " -0.39056459069252014,\n",
       " -0.10606003552675247,\n",
       " -0.0522497221827507,\n",
       " 0.04431663826107979,\n",
       " 0.09444928914308548,\n",
       " -0.08727063238620758,\n",
       " -0.19051644206047058,\n",
       " -0.002264924580231309,\n",
       " -0.0646088644862175,\n",
       " -0.21799002587795258,\n",
       " 0.027815381065011024,\n",
       " -0.062437232583761215,\n",
       " 0.06549445539712906,\n",
       " -0.14971111714839935,\n",
       " 0.17871548235416412,\n",
       " -0.4916614890098572,\n",
       " -0.059135619550943375,\n",
       " 0.16034148633480072,\n",
       " 0.0005776749458163977,\n",
       " -0.013527516275644302,\n",
       " 0.3965670168399811,\n",
       " -0.004828752018511295,\n",
       " -0.21628348529338837,\n",
       " -0.04264768213033676,\n",
       " 0.1512637585401535,\n",
       " -0.26690226793289185,\n",
       " -0.10939352959394455,\n",
       " 0.16309063136577606,\n",
       " -0.19669342041015625,\n",
       " -0.026963304728269577,\n",
       " -0.2695396840572357,\n",
       " -0.11439072340726852,\n",
       " 0.04887712001800537,\n",
       " -0.02091294899582863,\n",
       " -0.13082477450370789,\n",
       " -0.0387091226875782,\n",
       " -0.10691172629594803,\n",
       " 0.12292640656232834,\n",
       " 0.011841241270303726,\n",
       " -0.054314903914928436,\n",
       " -0.06624417752027512,\n",
       " 0.06813037395477295,\n",
       " -0.13701669871807098,\n",
       " -0.3052479326725006,\n",
       " -0.027519265189766884,\n",
       " -0.14753131568431854,\n",
       " 0.06995263695716858,\n",
       " -0.19896577298641205,\n",
       " -0.23261335492134094,\n",
       " -0.18254661560058594,\n",
       " 0.005507254041731358,\n",
       " 0.3874010443687439,\n",
       " 0.10180048644542694,\n",
       " -0.04248090088367462,\n",
       " 0.2039128541946411,\n",
       " 0.03883352875709534,\n",
       " 0.1875361204147339,\n",
       " 0.1169128343462944,\n",
       " -0.09370125085115433,\n",
       " 0.06952066719532013,\n",
       " 0.03489408642053604,\n",
       " 0.0022455137223005295,\n",
       " -0.09310382604598999,\n",
       " 0.2656046152114868,\n",
       " 0.07591331750154495,\n",
       " 0.15298070013523102,\n",
       " -0.1706022173166275,\n",
       " -0.11958315223455429,\n",
       " -0.014251633547246456,\n",
       " 0.17225728929042816,\n",
       " 0.28560352325439453,\n",
       " -0.029888126999139786,\n",
       " -0.028731750324368477,\n",
       " 0.22302253544330597,\n",
       " -0.029461152851581573,\n",
       " 0.3258120119571686,\n",
       " 0.015578973107039928,\n",
       " -0.17167752981185913,\n",
       " -0.0477878600358963,\n",
       " -0.012043519876897335,\n",
       " -0.027148917317390442,\n",
       " 0.15370061993598938,\n",
       " 0.08279895782470703,\n",
       " -0.4055962562561035,\n",
       " 0.04077833145856857,\n",
       " 0.03701787069439888,\n",
       " 0.06868208944797516]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_enrichments[\"embedding\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "911d7dfe-6fd8-4350-bbe8-485cb45bba4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "74231ea0-c6a2-4019-9b04-487ba2665838",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 6,\n",
       " 0,\n",
       " 2,\n",
       " 7,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 8,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 8,\n",
       " 9,\n",
       " 5,\n",
       " 0,\n",
       " 7,\n",
       " 6,\n",
       " 7,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 4,\n",
       " 9,\n",
       " 1,\n",
       " 2,\n",
       " 6,\n",
       " 5,\n",
       " 9,\n",
       " 4,\n",
       " 2,\n",
       " 5,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 9,\n",
       " 0,\n",
       " 7,\n",
       " 5,\n",
       " 3,\n",
       " 9,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 9,\n",
       " 7,\n",
       " 7,\n",
       " 2,\n",
       " 9,\n",
       " 0,\n",
       " 2,\n",
       " 8,\n",
       " 5,\n",
       " 9,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 9,\n",
       " 2,\n",
       " 7,\n",
       " 1,\n",
       " 0,\n",
       " 5,\n",
       " 1,\n",
       " 8,\n",
       " 1,\n",
       " 6,\n",
       " 5,\n",
       " 9,\n",
       " 7,\n",
       " 0,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 8,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 9,\n",
       " 1,\n",
       " 1,\n",
       " 7,\n",
       " 3,\n",
       " 8,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 9,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 6,\n",
       " 2,\n",
       " 5,\n",
       " 0,\n",
       " 8,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 7,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 9,\n",
       " 5,\n",
       " 7,\n",
       " 8,\n",
       " 1,\n",
       " 1,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 7,\n",
       " 9,\n",
       " 6,\n",
       " 0,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 7,\n",
       " 4,\n",
       " 3,\n",
       " 7,\n",
       " 5,\n",
       " 5,\n",
       " 7,\n",
       " 9,\n",
       " 3,\n",
       " 6,\n",
       " 9,\n",
       " 7,\n",
       " 5,\n",
       " 7,\n",
       " 1,\n",
       " 7,\n",
       " 0,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 3,\n",
       " 2,\n",
       " 9,\n",
       " 7,\n",
       " 8,\n",
       " 1,\n",
       " 3,\n",
       " 9,\n",
       " 8,\n",
       " 6,\n",
       " 9,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 9,\n",
       " 5,\n",
       " 6,\n",
       " 4,\n",
       " 2,\n",
       " 8,\n",
       " 7,\n",
       " 2,\n",
       " 7,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 6,\n",
       " 5,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 7,\n",
       " 7,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 7,\n",
       " 5,\n",
       " 9,\n",
       " 8,\n",
       " 7,\n",
       " 5,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 9,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 1,\n",
       " 8,\n",
       " 0,\n",
       " 7,\n",
       " 0,\n",
       " 7,\n",
       " 8,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 8,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 9,\n",
       " 5,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 7,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 6,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 7,\n",
       " 4,\n",
       " 5,\n",
       " 9,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 7,\n",
       " 5,\n",
       " 6,\n",
       " 9,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 5,\n",
       " 8,\n",
       " 4,\n",
       " 5,\n",
       " 9,\n",
       " 3,\n",
       " 1,\n",
       " 9,\n",
       " 9,\n",
       " 2,\n",
       " 8,\n",
       " 6,\n",
       " 7,\n",
       " 9,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 8,\n",
       " 1,\n",
       " 5,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 9,\n",
       " 8,\n",
       " 5,\n",
       " 7,\n",
       " 0,\n",
       " 4,\n",
       " 7,\n",
       " 3,\n",
       " 5,\n",
       " 8,\n",
       " 3,\n",
       " 6,\n",
       " 2,\n",
       " 9,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 2,\n",
       " 7,\n",
       " 8,\n",
       " 1,\n",
       " 0,\n",
       " 7,\n",
       " 2,\n",
       " 2,\n",
       " 9,\n",
       " 2,\n",
       " 8,\n",
       " 1,\n",
       " 0,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 9,\n",
       " 7,\n",
       " 1,\n",
       " 7,\n",
       " 9,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 7,\n",
       " 3,\n",
       " 8,\n",
       " 7,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 8,\n",
       " 1,\n",
       " 0,\n",
       " 5,\n",
       " 4,\n",
       " 8,\n",
       " 8,\n",
       " 1,\n",
       " 1,\n",
       " 7,\n",
       " 8,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 7,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 8,\n",
       " 3,\n",
       " 9,\n",
       " 1,\n",
       " 4,\n",
       " 9,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 2,\n",
       " 1,\n",
       " 8,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 6,\n",
       " 5,\n",
       " 8,\n",
       " 7,\n",
       " 6,\n",
       " 1,\n",
       " 0,\n",
       " 8,\n",
       " 1,\n",
       " 3,\n",
       " 6,\n",
       " 1,\n",
       " 2,\n",
       " 6,\n",
       " 1,\n",
       " 6,\n",
       " 0,\n",
       " 7,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 9,\n",
       " 8,\n",
       " 2,\n",
       " 5,\n",
       " 4,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 9,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 8,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 8,\n",
       " 1,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 7,\n",
       " 0,\n",
       " 5,\n",
       " 3,\n",
       " 2,\n",
       " 5,\n",
       " 8,\n",
       " 1,\n",
       " 9,\n",
       " 4,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 4,\n",
       " 7,\n",
       " 1,\n",
       " 5,\n",
       " 9,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 7,\n",
       " 6,\n",
       " 0,\n",
       " 9,\n",
       " 9,\n",
       " 5,\n",
       " 7,\n",
       " 6,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 6,\n",
       " 8,\n",
       " 9,\n",
       " 5,\n",
       " 9,\n",
       " 7,\n",
       " 1,\n",
       " 6,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 0,\n",
       " 7,\n",
       " 4,\n",
       " 6,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 9,\n",
       " 7,\n",
       " 4,\n",
       " 8,\n",
       " 2,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 8,\n",
       " 6,\n",
       " 0,\n",
       " 6,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 9,\n",
       " 4,\n",
       " 7,\n",
       " 5,\n",
       " 9,\n",
       " 9,\n",
       " 6,\n",
       " 5,\n",
       " 9,\n",
       " 3,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 2,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 8,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 9,\n",
       " 4,\n",
       " 8,\n",
       " 8,\n",
       " 1,\n",
       " 5,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 1,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 1,\n",
       " 7,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 9,\n",
       " 5,\n",
       " 9,\n",
       " 7,\n",
       " 8,\n",
       " 1,\n",
       " 5,\n",
       " 2,\n",
       " 8,\n",
       " 2,\n",
       " 6,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 7,\n",
       " 7,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 9,\n",
       " 1,\n",
       " 4,\n",
       " 9,\n",
       " 7,\n",
       " 2,\n",
       " 4,\n",
       " 6,\n",
       " 3,\n",
       " 5,\n",
       " 6,\n",
       " 8,\n",
       " 3,\n",
       " 2,\n",
       " 5,\n",
       " 2,\n",
       " 8,\n",
       " 9,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 9,\n",
       " 5,\n",
       " 0,\n",
       " 2,\n",
       " 5,\n",
       " 9,\n",
       " 5,\n",
       " 3,\n",
       " 2,\n",
       " 6,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 9,\n",
       " 5,\n",
       " 4,\n",
       " 7,\n",
       " 3,\n",
       " 9,\n",
       " 5,\n",
       " 6,\n",
       " 4,\n",
       " 2,\n",
       " 6,\n",
       " 9,\n",
       " 9,\n",
       " 0,\n",
       " 2,\n",
       " 5,\n",
       " 9,\n",
       " 4,\n",
       " 5,\n",
       " 9,\n",
       " 3,\n",
       " 1,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 1,\n",
       " 6,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 8,\n",
       " 3,\n",
       " 5,\n",
       " 9,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 5,\n",
       " 9,\n",
       " 2,\n",
       " 5,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 8,\n",
       " 9,\n",
       " 3,\n",
       " 9,\n",
       " 3,\n",
       " 9,\n",
       " 9,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 9,\n",
       " 7,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 9,\n",
       " 2,\n",
       " 2,\n",
       " 8,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 9,\n",
       " 7,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 7,\n",
       " 1,\n",
       " 7,\n",
       " 3,\n",
       " 8,\n",
       " 8,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 6,\n",
       " 6,\n",
       " 1,\n",
       " 4,\n",
       " 6,\n",
       " 3,\n",
       " 2,\n",
       " 5,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 9,\n",
       " 7,\n",
       " 7,\n",
       " 4,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 2,\n",
       " 8,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 6,\n",
       " 9,\n",
       " 8,\n",
       " 7,\n",
       " 8,\n",
       " 5,\n",
       " 3,\n",
       " 2,\n",
       " 9,\n",
       " 1,\n",
       " 7,\n",
       " 9,\n",
       " 8,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 3,\n",
       " 8,\n",
       " 6,\n",
       " 1,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 8,\n",
       " 9,\n",
       " 2,\n",
       " 4,\n",
       " 7,\n",
       " 6,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 8,\n",
       " 7,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 5,\n",
       " 7,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 7,\n",
       " 9,\n",
       " 9,\n",
       " 4,\n",
       " 9,\n",
       " 1,\n",
       " 9,\n",
       " 0,\n",
       " 2,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 0,\n",
       " 6,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 8,\n",
       " 4,\n",
       " 6,\n",
       " 7,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 1,\n",
       " 0,\n",
       " 7,\n",
       " 8,\n",
       " 5,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 9,\n",
       " 3,\n",
       " 9,\n",
       " 7,\n",
       " 9,\n",
       " 8,\n",
       " 6,\n",
       " 7,\n",
       " 2,\n",
       " 8,\n",
       " 4,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 9,\n",
       " 3,\n",
       " 8,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 6,\n",
       " 9,\n",
       " 8,\n",
       " 7,\n",
       " 8,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 8,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 5,\n",
       " 9,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 7,\n",
       " 9,\n",
       " 2,\n",
       " 3,\n",
       " 6,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 2,\n",
       " 7,\n",
       " 6,\n",
       " 3,\n",
       " 7,\n",
       " 9,\n",
       " 3,\n",
       " 7,\n",
       " 2,\n",
       " 5,\n",
       " 8,\n",
       " 7,\n",
       " 1,\n",
       " 7,\n",
       " 5,\n",
       " 3,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar10_ds[\"train\"][\"label\"][0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b848f5fc-dc7f-48b5-b014-4ba992bba1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "x_t = pca.fit_transform(cifar10_ds[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2d3fb632-786b-497b-ad74-25e0393aa66f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x_t[:, 0], x_t[:, 1], c=cifar10_ds[\"train\"][\"label\"][0:100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041ffbf2-6d44-439b-b52e-680e23e9ac6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next step: identify a region of interest, and run cPCA on a background subsample from that region of i"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
