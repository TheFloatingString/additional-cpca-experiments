{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9713bd72-4d21-4068-9b4c-c68a6f7b560f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.24.1)\n",
      "Collecting pyarrow>=12.0.0 (from datasets)\n",
      "  Downloading pyarrow-15.0.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Downloading pandas-2.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
      "Collecting tqdm>=4.62.1 (from datasets)\n",
      "  Downloading tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2023.4.0)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
      "Collecting huggingface-hub>=0.19.4 (from datasets)\n",
      "  Downloading huggingface_hub-0.21.4-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "INFO: pip is looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting huggingface-hub>=0.19.4 (from datasets)\n",
      "  Downloading huggingface_hub-0.21.3-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.21.2-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.21.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.21.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.20.2-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.20.1-py3-none-any.whl.metadata (12 kB)\n",
      "INFO: pip is still looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading huggingface_hub-0.20.0-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting fsspec[http]<=2024.2.0,>=2023.1.0 (from datasets)\n",
      "  Downloading fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.9/170.9 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.21.4-py3-none-any.whl (346 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.4/346.4 kB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-15.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (38.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.3/38.3 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.5/239.5 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m505.5/505.5 kB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.6/301.6 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytz, xxhash, tzdata, tqdm, pyarrow-hotfix, pyarrow, multidict, fsspec, frozenlist, dill, async-timeout, yarl, pandas, multiprocess, huggingface-hub, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "Successfully installed aiohttp-3.9.3 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.18.0 dill-0.3.8 frozenlist-1.4.1 fsspec-2024.2.0 huggingface-hub-0.21.4 multidict-6.0.5 multiprocess-0.70.16 pandas-2.2.1 pyarrow-15.0.1 pyarrow-hotfix-0.6 pytz-2024.1 tqdm-4.66.2 tzdata-2024.1 xxhash-3.4.1 yarl-1.9.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daa01040-5d6f-4d48-801e-4f8b6362c71d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e91927ad3474e4588303795f23443ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/5.16k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 120M/120M [00:04<00:00, 29.7MB/s] \n",
      "Downloading data: 100%|██████████| 23.9M/23.9M [00:00<00:00, 33.0MB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d1d573bf9504761a8cd36d827490da9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b357083e20cd4d52a3c1f27adf938779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "cifar10_ds = load_dataset(\"cifar10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f798c36-75f9-4622-ae0c-a4a459380ddd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting contrastive\n",
      "  Downloading contrastive-1.2.0.tar.gz (9.0 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from contrastive) (1.24.1)\n",
      "Collecting scikit-learn (from contrastive)\n",
      "  Downloading scikit_learn-1.4.1.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting matplotlib (from contrastive)\n",
      "  Downloading matplotlib-3.8.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->contrastive)\n",
      "  Downloading contourpy-1.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->contrastive)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->contrastive)\n",
      "  Downloading fonttools-4.49.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (159 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.1/159.1 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib->contrastive)\n",
      "  Downloading kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->contrastive) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib->contrastive) (9.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib->contrastive) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->contrastive) (2.8.2)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn->contrastive)\n",
      "  Downloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.2.0 (from scikit-learn->contrastive)\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn->contrastive)\n",
      "  Downloading threadpoolctl-3.3.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib->contrastive) (1.16.0)\n",
      "Downloading matplotlib-3.8.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.4.1.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (310 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.7/310.7 kB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.49.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m92.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading threadpoolctl-3.3.0-py3-none-any.whl (17 kB)\n",
      "Building wheels for collected packages: contrastive\n",
      "  Building wheel for contrastive (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for contrastive: filename=contrastive-1.2.0-py3-none-any.whl size=6899 sha256=b68c1eaa583586de70f61db792a0329dd36600c5864994b58f2662e25ea88747\n",
      "  Stored in directory: /root/.cache/pip/wheels/b4/2a/ad/111239c0d6a0b248f0ee4b903b6b44cc9cdfc1cfcff81ebba2\n",
      "Successfully built contrastive\n",
      "Installing collected packages: threadpoolctl, scipy, kiwisolver, joblib, fonttools, cycler, contourpy, scikit-learn, matplotlib, contrastive\n",
      "Successfully installed contourpy-1.2.0 contrastive-1.2.0 cycler-0.12.1 fonttools-4.49.0 joblib-1.3.2 kiwisolver-1.4.5 matplotlib-3.8.3 scikit-learn-1.4.1.post1 scipy-1.12.0 threadpoolctl-3.3.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install contrastive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3581d7f1-7263-4121-8da4-dde4ac7c7d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['img', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['img', 'label'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(cifar10_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "538cabd4-306a-49d4-b3ee-08416214594f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d25529df-0494-425f-8e19-369b478f6a6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=32x32>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = 10\n",
    "cifar10_ds['train'][index]['img'].show()\n",
    "cifar10_ds['train'][index]['img'].size\n",
    "cifar10_ds['train'][index]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cebec81f-7d58-4e3a-a2ae-0776bde09300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3072,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(cifar10_ds['train'][index]['img']).flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b8ac0d3-c7ce-4578-a0e2-7c65969677c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0478b98-05e6-4883-8ec6-741cf07079ee",
   "metadata": {},
   "source": [
    "### Demo Quickstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8574e6f4-a37d-4998-b0b8-fde06837faa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(397, 3072)\n",
      "(397,)\n",
      "(1603, 3072)\n"
     ]
    }
   ],
   "source": [
    "X_target = list()\n",
    "X_label = list()\n",
    "X_background = list()\n",
    "TARGET_LABEL_LIST = [0,1]\n",
    "for i in random.sample(range(10000),2000):\n",
    "    label = cifar10_ds['train'][i]['label']\n",
    "    img_flat = np.asarray(cifar10_ds['train'][i]['img']).flatten()\n",
    "    if label in TARGET_LABEL_LIST:\n",
    "        X_target.append(img_flat)\n",
    "        X_label.append(label)\n",
    "    else:\n",
    "        X_background.append(img_flat)\n",
    "\n",
    "X_target = np.asarray(X_target)\n",
    "X_label = np.asarray(X_label)\n",
    "X_background = np.asarray(X_background)\n",
    "\n",
    "print(X_target.shape)\n",
    "print(X_label.shape)\n",
    "print(X_background.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dba91d9-aa2f-45d3-b84f-eeab63318104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(397, 2)\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_target)\n",
    "print(X_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7b5cf3e-fdd5-484f-88e8-7fc6e46875de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contrastive import CPCA\n",
    "\n",
    "cpca = CPCA()\n",
    "X_cpca = cpca.fit_transform(X_target[:,:], X_background[:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30c0bfda-cd65-4c1a-b50c-6395b1a97a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 397, 2)\n"
     ]
    }
   ],
   "source": [
    "X_cpca = np.real(np.asarray(X_cpca))\n",
    "print(X_cpca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fcd2b95-e8e6-4a11-9c1a-7290b74fa7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.655 +/- 0.026\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "scores = cross_val_score(knn, X_pca, X_label, cv=5)\n",
    "print(f\"Accuracy: {round(np.mean(scores),3)} +/- {round(np.std(scores),3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50e02369-f437-4af5-bdc4-5a0d7b640510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(397, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cpca[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f879f03b-4d63-4615-9993-cbe1529df518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.652 +/- 0.025\n",
      "Accuracy: 0.554 +/- 0.081\n",
      "Accuracy: 0.579 +/- 0.068\n",
      "Accuracy: 0.549 +/- 0.029\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "for i in range(4):\n",
    "    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    scores = cross_val_score(knn, X_cpca[i], X_label, cv=5)\n",
    "    print(f\"Accuracy: {round(np.mean(scores),3)} +/- {round(np.std(scores),3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0afd2a9-9213-4b60-9c0d-40b5fb7bbff6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample complete\n",
      "label: [1, 0]\n",
      "(377, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.631 +/- 0.028\n",
      "PCA scores\n",
      "Accuracy: 0.578 +/- 0.031\n",
      "cPCA scores\n",
      "Accuracy: 0.576 +/- 0.022\n",
      "Accuracy: 0.631 +/- 0.028\n",
      "Accuracy: 0.61 +/- 0.061\n",
      "Accuracy: 0.549 +/- 0.041\n",
      "\n",
      "sample complete\n",
      "label: [2, 0]\n",
      "(406, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.754 +/- 0.052\n",
      "PCA scores\n",
      "Accuracy: 0.626 +/- 0.033\n",
      "cPCA scores\n",
      "Accuracy: 0.631 +/- 0.05\n",
      "Accuracy: 0.564 +/- 0.058\n",
      "Accuracy: 0.502 +/- 0.028\n",
      "Accuracy: 0.51 +/- 0.061\n",
      "\n",
      "sample complete\n",
      "label: [2, 1]\n",
      "(399, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.629 +/- 0.022\n",
      "PCA scores\n",
      "Accuracy: 0.596 +/- 0.03\n",
      "cPCA scores\n",
      "Accuracy: 0.624 +/- 0.041\n",
      "Accuracy: 0.644 +/- 0.044\n",
      "Accuracy: 0.669 +/- 0.043\n",
      "Accuracy: 0.566 +/- 0.088\n",
      "\n",
      "sample complete\n",
      "label: [3, 0]\n",
      "(382, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.754 +/- 0.038\n",
      "PCA scores\n",
      "Accuracy: 0.652 +/- 0.053\n",
      "cPCA scores\n",
      "Accuracy: 0.641 +/- 0.049\n",
      "Accuracy: 0.704 +/- 0.051\n",
      "Accuracy: 0.508 +/- 0.031\n",
      "Accuracy: 0.49 +/- 0.042\n",
      "\n",
      "sample complete\n",
      "label: [3, 1]\n",
      "(386, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.728 +/- 0.069\n",
      "PCA scores\n",
      "Accuracy: 0.591 +/- 0.065\n",
      "cPCA scores\n",
      "Accuracy: 0.625 +/- 0.043\n",
      "Accuracy: 0.562 +/- 0.053\n",
      "Accuracy: 0.63 +/- 0.06\n",
      "Accuracy: 0.526 +/- 0.036\n",
      "\n",
      "sample complete\n",
      "label: [3, 2]\n",
      "(400, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.605 +/- 0.036\n",
      "PCA scores\n",
      "Accuracy: 0.552 +/- 0.051\n",
      "cPCA scores\n",
      "Accuracy: 0.552 +/- 0.063\n",
      "Accuracy: 0.595 +/- 0.044\n",
      "Accuracy: 0.535 +/- 0.032\n",
      "Accuracy: 0.542 +/- 0.038\n",
      "\n",
      "sample complete\n",
      "label: [4, 0]\n",
      "(387, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.773 +/- 0.056\n",
      "PCA scores\n",
      "Accuracy: 0.693 +/- 0.052\n",
      "cPCA scores\n",
      "Accuracy: 0.641 +/- 0.049\n",
      "Accuracy: 0.612 +/- 0.04\n",
      "Accuracy: 0.47 +/- 0.032\n",
      "Accuracy: 0.499 +/- 0.063\n",
      "\n",
      "sample complete\n",
      "label: [4, 1]\n",
      "(367, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.657 +/- 0.048\n",
      "PCA scores\n",
      "Accuracy: 0.687 +/- 0.053\n",
      "cPCA scores\n",
      "Accuracy: 0.69 +/- 0.068\n",
      "Accuracy: 0.755 +/- 0.012\n",
      "Accuracy: 0.575 +/- 0.064\n",
      "Accuracy: 0.537 +/- 0.059\n",
      "\n",
      "sample complete\n",
      "label: [4, 2]\n",
      "(449, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.583 +/- 0.034\n",
      "PCA scores\n",
      "Accuracy: 0.543 +/- 0.023\n",
      "cPCA scores\n",
      "Accuracy: 0.541 +/- 0.064\n",
      "Accuracy: 0.543 +/- 0.024\n",
      "Accuracy: 0.483 +/- 0.028\n",
      "Accuracy: 0.543 +/- 0.052\n",
      "\n",
      "sample complete\n",
      "label: [4, 3]\n",
      "(436, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.617 +/- 0.056\n",
      "PCA scores\n",
      "Accuracy: 0.516 +/- 0.028\n",
      "cPCA scores\n",
      "Accuracy: 0.491 +/- 0.031\n",
      "Accuracy: 0.555 +/- 0.027\n",
      "Accuracy: 0.495 +/- 0.07\n",
      "Accuracy: 0.493 +/- 0.045\n",
      "\n",
      "sample complete\n",
      "label: [5, 0]\n",
      "(383, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.812 +/- 0.039\n",
      "PCA scores\n",
      "Accuracy: 0.737 +/- 0.053\n",
      "cPCA scores\n",
      "Accuracy: 0.729 +/- 0.039\n",
      "Accuracy: 0.621 +/- 0.051\n",
      "Accuracy: 0.517 +/- 0.045\n",
      "Accuracy: 0.473 +/- 0.018\n",
      "\n",
      "sample complete\n",
      "label: [5, 1]\n",
      "(380, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.713 +/- 0.042\n",
      "PCA scores\n",
      "Accuracy: 0.634 +/- 0.047\n",
      "cPCA scores\n",
      "Accuracy: 0.637 +/- 0.038\n",
      "Accuracy: 0.689 +/- 0.047\n",
      "Accuracy: 0.547 +/- 0.056\n",
      "Accuracy: 0.521 +/- 0.046\n",
      "\n",
      "sample complete\n",
      "label: [5, 2]\n",
      "(365, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.614 +/- 0.041\n",
      "PCA scores\n",
      "Accuracy: 0.641 +/- 0.049\n",
      "cPCA scores\n",
      "Accuracy: 0.622 +/- 0.061\n",
      "Accuracy: 0.534 +/- 0.023\n",
      "Accuracy: 0.474 +/- 0.073\n",
      "Accuracy: 0.482 +/- 0.058\n",
      "\n",
      "sample complete\n",
      "label: [5, 3]\n",
      "(382, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.539 +/- 0.034\n",
      "PCA scores\n",
      "Accuracy: 0.453 +/- 0.024\n",
      "cPCA scores\n",
      "Accuracy: 0.474 +/- 0.047\n",
      "Accuracy: 0.49 +/- 0.04\n",
      "Accuracy: 0.479 +/- 0.019\n",
      "Accuracy: 0.506 +/- 0.064\n",
      "\n",
      "sample complete\n",
      "label: [5, 4]\n",
      "(404, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.629 +/- 0.059\n",
      "PCA scores\n",
      "Accuracy: 0.475 +/- 0.024\n",
      "cPCA scores\n",
      "Accuracy: 0.478 +/- 0.05\n",
      "Accuracy: 0.646 +/- 0.033\n",
      "Accuracy: 0.559 +/- 0.033\n",
      "Accuracy: 0.468 +/- 0.042\n",
      "\n",
      "sample complete\n",
      "label: [6, 0]\n",
      "(393, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.842 +/- 0.034\n",
      "PCA scores\n",
      "Accuracy: 0.72 +/- 0.065\n",
      "cPCA scores\n",
      "Accuracy: 0.705 +/- 0.039\n",
      "Accuracy: 0.807 +/- 0.028\n",
      "Accuracy: 0.649 +/- 0.042\n",
      "Accuracy: 0.532 +/- 0.038\n",
      "\n",
      "sample complete\n",
      "label: [6, 1]\n",
      "(397, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.642 +/- 0.008\n",
      "PCA scores\n",
      "Accuracy: 0.658 +/- 0.02\n",
      "cPCA scores\n",
      "Accuracy: 0.642 +/- 0.028\n",
      "Accuracy: 0.71 +/- 0.024\n",
      "Accuracy: 0.59 +/- 0.042\n",
      "Accuracy: 0.531 +/- 0.05\n",
      "\n",
      "sample complete\n",
      "label: [6, 2]\n",
      "(415, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.607 +/- 0.039\n",
      "PCA scores\n",
      "Accuracy: 0.516 +/- 0.034\n",
      "cPCA scores\n",
      "Accuracy: 0.499 +/- 0.016\n",
      "Accuracy: 0.61 +/- 0.04\n",
      "Accuracy: 0.499 +/- 0.02\n",
      "Accuracy: 0.525 +/- 0.029\n",
      "\n",
      "sample complete\n",
      "label: [6, 3]\n",
      "(400, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.61 +/- 0.027\n",
      "PCA scores\n",
      "Accuracy: 0.562 +/- 0.049\n",
      "cPCA scores\n",
      "Accuracy: 0.562 +/- 0.042\n",
      "Accuracy: 0.612 +/- 0.032\n",
      "Accuracy: 0.495 +/- 0.067\n",
      "Accuracy: 0.543 +/- 0.033\n",
      "\n",
      "sample complete\n",
      "label: [6, 4]\n",
      "(393, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.603 +/- 0.049\n",
      "PCA scores\n",
      "Accuracy: 0.562 +/- 0.029\n",
      "cPCA scores\n",
      "Accuracy: 0.608 +/- 0.048\n",
      "Accuracy: 0.519 +/- 0.021\n",
      "Accuracy: 0.606 +/- 0.06\n",
      "Accuracy: 0.534 +/- 0.021\n",
      "\n",
      "sample complete\n",
      "label: [6, 5]\n",
      "(396, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.659 +/- 0.021\n",
      "PCA scores\n",
      "Accuracy: 0.634 +/- 0.075\n",
      "cPCA scores\n",
      "Accuracy: 0.644 +/- 0.033\n",
      "Accuracy: 0.558 +/- 0.045\n",
      "Accuracy: 0.515 +/- 0.04\n",
      "Accuracy: 0.556 +/- 0.033\n",
      "\n",
      "sample complete\n",
      "label: [7, 0]\n",
      "(388, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.699 +/- 0.033\n",
      "PCA scores\n",
      "Accuracy: 0.645 +/- 0.048\n",
      "cPCA scores\n",
      "Accuracy: 0.663 +/- 0.047\n",
      "Accuracy: 0.57 +/- 0.045\n",
      "Accuracy: 0.575 +/- 0.054\n",
      "Accuracy: 0.552 +/- 0.033\n",
      "\n",
      "sample complete\n",
      "label: [7, 1]\n",
      "(390, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.715 +/- 0.038\n",
      "PCA scores\n",
      "Accuracy: 0.528 +/- 0.045\n",
      "cPCA scores\n",
      "Accuracy: 0.556 +/- 0.041\n",
      "Accuracy: 0.577 +/- 0.044\n",
      "Accuracy: 0.567 +/- 0.026\n",
      "Accuracy: 0.518 +/- 0.013\n",
      "\n",
      "sample complete\n",
      "label: [7, 2]\n",
      "(402, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.654 +/- 0.03\n",
      "PCA scores\n",
      "Accuracy: 0.565 +/- 0.022\n",
      "cPCA scores\n",
      "Accuracy: 0.582 +/- 0.027\n",
      "Accuracy: 0.612 +/- 0.045\n",
      "Accuracy: 0.512 +/- 0.04\n",
      "Accuracy: 0.545 +/- 0.07\n",
      "\n",
      "sample complete\n",
      "label: [7, 3]\n",
      "(405, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.644 +/- 0.031\n",
      "PCA scores\n",
      "Accuracy: 0.516 +/- 0.061\n",
      "cPCA scores\n",
      "Accuracy: 0.509 +/- 0.054\n",
      "Accuracy: 0.523 +/- 0.01\n",
      "Accuracy: 0.514 +/- 0.02\n",
      "Accuracy: 0.511 +/- 0.049\n",
      "\n",
      "sample complete\n",
      "label: [7, 4]\n",
      "(398, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.621 +/- 0.027\n",
      "PCA scores\n",
      "Accuracy: 0.555 +/- 0.045\n",
      "cPCA scores\n",
      "Accuracy: 0.565 +/- 0.05\n",
      "Accuracy: 0.548 +/- 0.025\n",
      "Accuracy: 0.548 +/- 0.053\n",
      "Accuracy: 0.518 +/- 0.02\n",
      "\n",
      "sample complete\n",
      "label: [7, 5]\n",
      "(423, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.676 +/- 0.018\n",
      "PCA scores\n",
      "Accuracy: 0.582 +/- 0.059\n",
      "cPCA scores\n",
      "Accuracy: 0.541 +/- 0.041\n",
      "Accuracy: 0.546 +/- 0.03\n",
      "Accuracy: 0.53 +/- 0.041\n",
      "Accuracy: 0.532 +/- 0.056\n",
      "\n",
      "sample complete\n",
      "label: [7, 6]\n",
      "(403, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.643 +/- 0.029\n",
      "PCA scores\n",
      "Accuracy: 0.596 +/- 0.051\n",
      "cPCA scores\n",
      "Accuracy: 0.596 +/- 0.048\n",
      "Accuracy: 0.623 +/- 0.033\n",
      "Accuracy: 0.481 +/- 0.041\n",
      "Accuracy: 0.501 +/- 0.031\n",
      "\n",
      "sample complete\n",
      "label: [8, 0]\n",
      "(401, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.661 +/- 0.064\n",
      "PCA scores\n",
      "Accuracy: 0.604 +/- 0.037\n",
      "cPCA scores\n",
      "Accuracy: 0.606 +/- 0.048\n",
      "Accuracy: 0.584 +/- 0.072\n",
      "Accuracy: 0.491 +/- 0.029\n",
      "Accuracy: 0.486 +/- 0.04\n",
      "\n",
      "sample complete\n",
      "label: [8, 1]\n",
      "(376, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.625 +/- 0.02\n",
      "PCA scores\n",
      "Accuracy: 0.585 +/- 0.031\n",
      "cPCA scores\n",
      "Accuracy: 0.577 +/- 0.033\n",
      "Accuracy: 0.598 +/- 0.055\n",
      "Accuracy: 0.567 +/- 0.036\n",
      "Accuracy: 0.553 +/- 0.037\n",
      "\n",
      "sample complete\n",
      "label: [8, 2]\n",
      "(409, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.831 +/- 0.037\n",
      "PCA scores\n",
      "Accuracy: 0.721 +/- 0.021\n",
      "cPCA scores\n",
      "Accuracy: 0.719 +/- 0.024\n",
      "Accuracy: 0.579 +/- 0.073\n",
      "Accuracy: 0.496 +/- 0.049\n",
      "Accuracy: 0.491 +/- 0.05\n",
      "\n",
      "sample complete\n",
      "label: [8, 3]\n",
      "(393, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.825 +/- 0.027\n",
      "PCA scores\n",
      "Accuracy: 0.741 +/- 0.042\n",
      "cPCA scores\n",
      "Accuracy: 0.723 +/- 0.047\n",
      "Accuracy: 0.799 +/- 0.057\n",
      "Accuracy: 0.53 +/- 0.052\n",
      "Accuracy: 0.534 +/- 0.022\n",
      "\n",
      "sample complete\n",
      "label: [8, 4]\n",
      "(383, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.838 +/- 0.053\n",
      "PCA scores\n",
      "Accuracy: 0.715 +/- 0.057\n",
      "cPCA scores\n",
      "Accuracy: 0.734 +/- 0.072\n",
      "Accuracy: 0.76 +/- 0.051\n",
      "Accuracy: 0.494 +/- 0.038\n",
      "Accuracy: 0.559 +/- 0.047\n",
      "\n",
      "sample complete\n",
      "label: [8, 5]\n",
      "(398, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.852 +/- 0.031\n",
      "PCA scores\n",
      "Accuracy: 0.676 +/- 0.026\n",
      "cPCA scores\n",
      "Accuracy: 0.678 +/- 0.031\n",
      "Accuracy: 0.636 +/- 0.036\n",
      "Accuracy: 0.522 +/- 0.034\n",
      "Accuracy: 0.515 +/- 0.025\n",
      "\n",
      "sample complete\n",
      "label: [8, 6]\n",
      "(428, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.879 +/- 0.034\n",
      "PCA scores\n",
      "Accuracy: 0.792 +/- 0.053\n",
      "cPCA scores\n",
      "Accuracy: 0.785 +/- 0.059\n",
      "Accuracy: 0.554 +/- 0.024\n",
      "Accuracy: 0.561 +/- 0.054\n",
      "Accuracy: 0.516 +/- 0.032\n",
      "\n",
      "sample complete\n",
      "label: [8, 7]\n",
      "(390, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.764 +/- 0.028\n",
      "PCA scores\n",
      "Accuracy: 0.667 +/- 0.026\n",
      "cPCA scores\n",
      "Accuracy: 0.672 +/- 0.033\n",
      "Accuracy: 0.762 +/- 0.037\n",
      "Accuracy: 0.585 +/- 0.019\n",
      "Accuracy: 0.513 +/- 0.047\n",
      "\n",
      "sample complete\n",
      "label: [9, 0]\n",
      "(401, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.628 +/- 0.031\n",
      "PCA scores\n",
      "Accuracy: 0.648 +/- 0.032\n",
      "cPCA scores\n",
      "Accuracy: 0.661 +/- 0.026\n",
      "Accuracy: 0.676 +/- 0.027\n",
      "Accuracy: 0.553 +/- 0.051\n",
      "Accuracy: 0.534 +/- 0.029\n",
      "\n",
      "sample complete\n",
      "label: [9, 1]\n",
      "(396, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.644 +/- 0.049\n",
      "PCA scores\n",
      "Accuracy: 0.614 +/- 0.074\n",
      "cPCA scores\n",
      "Accuracy: 0.614 +/- 0.047\n",
      "Accuracy: 0.568 +/- 0.022\n",
      "Accuracy: 0.54 +/- 0.019\n",
      "Accuracy: 0.52 +/- 0.05\n",
      "\n",
      "sample complete\n",
      "label: [9, 2]\n",
      "(399, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.667 +/- 0.047\n",
      "PCA scores\n",
      "Accuracy: 0.712 +/- 0.029\n",
      "cPCA scores\n",
      "Accuracy: 0.704 +/- 0.046\n",
      "Accuracy: 0.607 +/- 0.02\n",
      "Accuracy: 0.577 +/- 0.04\n",
      "Accuracy: 0.569 +/- 0.041\n",
      "\n",
      "sample complete\n",
      "label: [9, 3]\n",
      "(412, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.755 +/- 0.046\n",
      "PCA scores\n",
      "Accuracy: 0.704 +/- 0.023\n",
      "cPCA scores\n",
      "Accuracy: 0.697 +/- 0.025\n",
      "Accuracy: 0.527 +/- 0.054\n",
      "Accuracy: 0.556 +/- 0.016\n",
      "Accuracy: 0.558 +/- 0.039\n",
      "\n",
      "sample complete\n",
      "label: [9, 4]\n",
      "(377, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.698 +/- 0.031\n",
      "PCA scores\n",
      "Accuracy: 0.703 +/- 0.036\n",
      "cPCA scores\n",
      "Accuracy: 0.735 +/- 0.032\n",
      "Accuracy: 0.703 +/- 0.019\n",
      "Accuracy: 0.592 +/- 0.042\n",
      "Accuracy: 0.586 +/- 0.048\n",
      "\n",
      "sample complete\n",
      "label: [9, 5]\n",
      "(391, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.716 +/- 0.012\n",
      "PCA scores\n",
      "Accuracy: 0.749 +/- 0.037\n",
      "cPCA scores\n",
      "Accuracy: 0.737 +/- 0.041\n",
      "Accuracy: 0.629 +/- 0.022\n",
      "Accuracy: 0.606 +/- 0.033\n",
      "Accuracy: 0.606 +/- 0.041\n",
      "\n",
      "sample complete\n",
      "label: [9, 6]\n",
      "(410, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.698 +/- 0.055\n",
      "PCA scores\n",
      "Accuracy: 0.715 +/- 0.049\n",
      "cPCA scores\n",
      "Accuracy: 0.702 +/- 0.035\n",
      "Accuracy: 0.615 +/- 0.048\n",
      "Accuracy: 0.49 +/- 0.042\n",
      "Accuracy: 0.507 +/- 0.033\n",
      "\n",
      "sample complete\n",
      "label: [9, 7]\n",
      "(385, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.706 +/- 0.039\n",
      "PCA scores\n",
      "Accuracy: 0.657 +/- 0.023\n",
      "cPCA scores\n",
      "Accuracy: 0.688 +/- 0.018\n",
      "Accuracy: 0.681 +/- 0.049\n",
      "Accuracy: 0.525 +/- 0.033\n",
      "Accuracy: 0.545 +/- 0.037\n",
      "\n",
      "sample complete\n",
      "label: [9, 8]\n",
      "(406, 2)\n",
      "PCA complete\n",
      "CPCA complete\n",
      "No preprocessing score\n",
      "Accuracy: 0.611 +/- 0.015\n",
      "PCA scores\n",
      "Accuracy: 0.586 +/- 0.011\n",
      "cPCA scores\n",
      "Accuracy: 0.581 +/- 0.029\n",
      "Accuracy: 0.579 +/- 0.035\n",
      "Accuracy: 0.571 +/- 0.043\n",
      "Accuracy: 0.537 +/- 0.044\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_list = list()\n",
    "\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        tmp_dict = {\n",
    "            'no_preprocessing':None,\n",
    "            'PCA':None,\n",
    "            'cPCA-0':None,\n",
    "            'cPCA-1':None,\n",
    "            'cPCA-2':None,\n",
    "            'cPCA-3':None,\n",
    "            'name':f\"{i} AND {j}\"\n",
    "        }\n",
    "        X_target = list()\n",
    "        X_label = list()\n",
    "        X_background = list()\n",
    "        if i>j:\n",
    "            TARGET_LABEL_LIST = [i,j]\n",
    "            for l in random.sample(range(10000),2000):\n",
    "                label = cifar10_ds['train'][l]['label']\n",
    "                img_flat = np.asarray(cifar10_ds['train'][l]['img']).flatten()\n",
    "                if label in TARGET_LABEL_LIST:\n",
    "                    X_target.append(img_flat)\n",
    "                    X_label.append(label)\n",
    "                else:\n",
    "                    X_background.append(img_flat)\n",
    "            X_target = np.asarray(X_target)\n",
    "            X_label = np.asarray(X_label)\n",
    "            X_background = np.asarray(X_background)\n",
    "            print(\"sample complete\")\n",
    "            print(f\"label: {TARGET_LABEL_LIST}\")\n",
    "            pca = PCA(n_components=2)\n",
    "            X_pca = pca.fit_transform(X_target)\n",
    "            print(X_pca.shape)\n",
    "            print(\"PCA complete\")\n",
    "            cpca = CPCA()\n",
    "            X_cpca = cpca.fit_transform(X_target[:,:], X_background[:,:])\n",
    "            X_cpca = np.real(X_cpca)\n",
    "            print(\"CPCA complete\")\n",
    "            print(\"No preprocessing score\")\n",
    "            knn = KNeighborsClassifier(n_neighbors=5)\n",
    "            scores = cross_val_score(knn, X_target, X_label, cv=5)\n",
    "            print(f\"Accuracy: {round(np.mean(scores),3)} +/- {round(np.std(scores),3)}\")\n",
    "            tmp_dict['no_preprocessing']=f\"{round(np.mean(scores),3)} +/- {round(np.std(scores),3)}\"\n",
    "            print(\"PCA scores\")\n",
    "            knn = KNeighborsClassifier(n_neighbors=5)\n",
    "            scores = cross_val_score(knn, X_pca, X_label, cv=5)\n",
    "            print(f\"Accuracy: {round(np.mean(scores),3)} +/- {round(np.std(scores),3)}\")\n",
    "            tmp_dict['PCA']=f\"{round(np.mean(scores),3)} +/- {round(np.std(scores),3)}\"\n",
    "            print(\"cPCA scores\")\n",
    "            for k in range(4):\n",
    "                knn = KNeighborsClassifier(n_neighbors=5)\n",
    "                scores = cross_val_score(knn, X_cpca[k], X_label, cv=5)\n",
    "                print(f\"Accuracy: {round(np.mean(scores),3)} +/- {round(np.std(scores),3)}\")\n",
    "                tmp_dict['cPCA-'+str(k)]=f\"{round(np.mean(scores),3)} +/- {round(np.std(scores),3)}\"\n",
    "            print()\n",
    "            results_list.append(tmp_dict)\n",
    "            # raise KeyError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "316c7fda-b899-4785-bde9-71acdff22cc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_preprocessing</th>\n",
       "      <th>PCA</th>\n",
       "      <th>cPCA-0</th>\n",
       "      <th>cPCA-1</th>\n",
       "      <th>cPCA-2</th>\n",
       "      <th>cPCA-3</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.631 +/- 0.028</td>\n",
       "      <td>0.578 +/- 0.031</td>\n",
       "      <td>0.576 +/- 0.022</td>\n",
       "      <td>0.631 +/- 0.028</td>\n",
       "      <td>0.61 +/- 0.061</td>\n",
       "      <td>0.549 +/- 0.041</td>\n",
       "      <td>1 AND 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.754 +/- 0.052</td>\n",
       "      <td>0.626 +/- 0.033</td>\n",
       "      <td>0.631 +/- 0.05</td>\n",
       "      <td>0.564 +/- 0.058</td>\n",
       "      <td>0.502 +/- 0.028</td>\n",
       "      <td>0.51 +/- 0.061</td>\n",
       "      <td>2 AND 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.629 +/- 0.022</td>\n",
       "      <td>0.596 +/- 0.03</td>\n",
       "      <td>0.624 +/- 0.041</td>\n",
       "      <td>0.644 +/- 0.044</td>\n",
       "      <td>0.669 +/- 0.043</td>\n",
       "      <td>0.566 +/- 0.088</td>\n",
       "      <td>2 AND 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.754 +/- 0.038</td>\n",
       "      <td>0.652 +/- 0.053</td>\n",
       "      <td>0.641 +/- 0.049</td>\n",
       "      <td>0.704 +/- 0.051</td>\n",
       "      <td>0.508 +/- 0.031</td>\n",
       "      <td>0.49 +/- 0.042</td>\n",
       "      <td>3 AND 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.728 +/- 0.069</td>\n",
       "      <td>0.591 +/- 0.065</td>\n",
       "      <td>0.625 +/- 0.043</td>\n",
       "      <td>0.562 +/- 0.053</td>\n",
       "      <td>0.63 +/- 0.06</td>\n",
       "      <td>0.526 +/- 0.036</td>\n",
       "      <td>3 AND 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.605 +/- 0.036</td>\n",
       "      <td>0.552 +/- 0.051</td>\n",
       "      <td>0.552 +/- 0.063</td>\n",
       "      <td>0.595 +/- 0.044</td>\n",
       "      <td>0.535 +/- 0.032</td>\n",
       "      <td>0.542 +/- 0.038</td>\n",
       "      <td>3 AND 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.773 +/- 0.056</td>\n",
       "      <td>0.693 +/- 0.052</td>\n",
       "      <td>0.641 +/- 0.049</td>\n",
       "      <td>0.612 +/- 0.04</td>\n",
       "      <td>0.47 +/- 0.032</td>\n",
       "      <td>0.499 +/- 0.063</td>\n",
       "      <td>4 AND 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.657 +/- 0.048</td>\n",
       "      <td>0.687 +/- 0.053</td>\n",
       "      <td>0.69 +/- 0.068</td>\n",
       "      <td>0.755 +/- 0.012</td>\n",
       "      <td>0.575 +/- 0.064</td>\n",
       "      <td>0.537 +/- 0.059</td>\n",
       "      <td>4 AND 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.583 +/- 0.034</td>\n",
       "      <td>0.543 +/- 0.023</td>\n",
       "      <td>0.541 +/- 0.064</td>\n",
       "      <td>0.543 +/- 0.024</td>\n",
       "      <td>0.483 +/- 0.028</td>\n",
       "      <td>0.543 +/- 0.052</td>\n",
       "      <td>4 AND 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.617 +/- 0.056</td>\n",
       "      <td>0.516 +/- 0.028</td>\n",
       "      <td>0.491 +/- 0.031</td>\n",
       "      <td>0.555 +/- 0.027</td>\n",
       "      <td>0.495 +/- 0.07</td>\n",
       "      <td>0.493 +/- 0.045</td>\n",
       "      <td>4 AND 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.812 +/- 0.039</td>\n",
       "      <td>0.737 +/- 0.053</td>\n",
       "      <td>0.729 +/- 0.039</td>\n",
       "      <td>0.621 +/- 0.051</td>\n",
       "      <td>0.517 +/- 0.045</td>\n",
       "      <td>0.473 +/- 0.018</td>\n",
       "      <td>5 AND 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.713 +/- 0.042</td>\n",
       "      <td>0.634 +/- 0.047</td>\n",
       "      <td>0.637 +/- 0.038</td>\n",
       "      <td>0.689 +/- 0.047</td>\n",
       "      <td>0.547 +/- 0.056</td>\n",
       "      <td>0.521 +/- 0.046</td>\n",
       "      <td>5 AND 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.614 +/- 0.041</td>\n",
       "      <td>0.641 +/- 0.049</td>\n",
       "      <td>0.622 +/- 0.061</td>\n",
       "      <td>0.534 +/- 0.023</td>\n",
       "      <td>0.474 +/- 0.073</td>\n",
       "      <td>0.482 +/- 0.058</td>\n",
       "      <td>5 AND 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.539 +/- 0.034</td>\n",
       "      <td>0.453 +/- 0.024</td>\n",
       "      <td>0.474 +/- 0.047</td>\n",
       "      <td>0.49 +/- 0.04</td>\n",
       "      <td>0.479 +/- 0.019</td>\n",
       "      <td>0.506 +/- 0.064</td>\n",
       "      <td>5 AND 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.629 +/- 0.059</td>\n",
       "      <td>0.475 +/- 0.024</td>\n",
       "      <td>0.478 +/- 0.05</td>\n",
       "      <td>0.646 +/- 0.033</td>\n",
       "      <td>0.559 +/- 0.033</td>\n",
       "      <td>0.468 +/- 0.042</td>\n",
       "      <td>5 AND 4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.842 +/- 0.034</td>\n",
       "      <td>0.72 +/- 0.065</td>\n",
       "      <td>0.705 +/- 0.039</td>\n",
       "      <td>0.807 +/- 0.028</td>\n",
       "      <td>0.649 +/- 0.042</td>\n",
       "      <td>0.532 +/- 0.038</td>\n",
       "      <td>6 AND 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.642 +/- 0.008</td>\n",
       "      <td>0.658 +/- 0.02</td>\n",
       "      <td>0.642 +/- 0.028</td>\n",
       "      <td>0.71 +/- 0.024</td>\n",
       "      <td>0.59 +/- 0.042</td>\n",
       "      <td>0.531 +/- 0.05</td>\n",
       "      <td>6 AND 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.607 +/- 0.039</td>\n",
       "      <td>0.516 +/- 0.034</td>\n",
       "      <td>0.499 +/- 0.016</td>\n",
       "      <td>0.61 +/- 0.04</td>\n",
       "      <td>0.499 +/- 0.02</td>\n",
       "      <td>0.525 +/- 0.029</td>\n",
       "      <td>6 AND 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.61 +/- 0.027</td>\n",
       "      <td>0.562 +/- 0.049</td>\n",
       "      <td>0.562 +/- 0.042</td>\n",
       "      <td>0.612 +/- 0.032</td>\n",
       "      <td>0.495 +/- 0.067</td>\n",
       "      <td>0.543 +/- 0.033</td>\n",
       "      <td>6 AND 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.603 +/- 0.049</td>\n",
       "      <td>0.562 +/- 0.029</td>\n",
       "      <td>0.608 +/- 0.048</td>\n",
       "      <td>0.519 +/- 0.021</td>\n",
       "      <td>0.606 +/- 0.06</td>\n",
       "      <td>0.534 +/- 0.021</td>\n",
       "      <td>6 AND 4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.659 +/- 0.021</td>\n",
       "      <td>0.634 +/- 0.075</td>\n",
       "      <td>0.644 +/- 0.033</td>\n",
       "      <td>0.558 +/- 0.045</td>\n",
       "      <td>0.515 +/- 0.04</td>\n",
       "      <td>0.556 +/- 0.033</td>\n",
       "      <td>6 AND 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.699 +/- 0.033</td>\n",
       "      <td>0.645 +/- 0.048</td>\n",
       "      <td>0.663 +/- 0.047</td>\n",
       "      <td>0.57 +/- 0.045</td>\n",
       "      <td>0.575 +/- 0.054</td>\n",
       "      <td>0.552 +/- 0.033</td>\n",
       "      <td>7 AND 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.715 +/- 0.038</td>\n",
       "      <td>0.528 +/- 0.045</td>\n",
       "      <td>0.556 +/- 0.041</td>\n",
       "      <td>0.577 +/- 0.044</td>\n",
       "      <td>0.567 +/- 0.026</td>\n",
       "      <td>0.518 +/- 0.013</td>\n",
       "      <td>7 AND 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.654 +/- 0.03</td>\n",
       "      <td>0.565 +/- 0.022</td>\n",
       "      <td>0.582 +/- 0.027</td>\n",
       "      <td>0.612 +/- 0.045</td>\n",
       "      <td>0.512 +/- 0.04</td>\n",
       "      <td>0.545 +/- 0.07</td>\n",
       "      <td>7 AND 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.644 +/- 0.031</td>\n",
       "      <td>0.516 +/- 0.061</td>\n",
       "      <td>0.509 +/- 0.054</td>\n",
       "      <td>0.523 +/- 0.01</td>\n",
       "      <td>0.514 +/- 0.02</td>\n",
       "      <td>0.511 +/- 0.049</td>\n",
       "      <td>7 AND 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.621 +/- 0.027</td>\n",
       "      <td>0.555 +/- 0.045</td>\n",
       "      <td>0.565 +/- 0.05</td>\n",
       "      <td>0.548 +/- 0.025</td>\n",
       "      <td>0.548 +/- 0.053</td>\n",
       "      <td>0.518 +/- 0.02</td>\n",
       "      <td>7 AND 4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.676 +/- 0.018</td>\n",
       "      <td>0.582 +/- 0.059</td>\n",
       "      <td>0.541 +/- 0.041</td>\n",
       "      <td>0.546 +/- 0.03</td>\n",
       "      <td>0.53 +/- 0.041</td>\n",
       "      <td>0.532 +/- 0.056</td>\n",
       "      <td>7 AND 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.643 +/- 0.029</td>\n",
       "      <td>0.596 +/- 0.051</td>\n",
       "      <td>0.596 +/- 0.048</td>\n",
       "      <td>0.623 +/- 0.033</td>\n",
       "      <td>0.481 +/- 0.041</td>\n",
       "      <td>0.501 +/- 0.031</td>\n",
       "      <td>7 AND 6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.661 +/- 0.064</td>\n",
       "      <td>0.604 +/- 0.037</td>\n",
       "      <td>0.606 +/- 0.048</td>\n",
       "      <td>0.584 +/- 0.072</td>\n",
       "      <td>0.491 +/- 0.029</td>\n",
       "      <td>0.486 +/- 0.04</td>\n",
       "      <td>8 AND 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.625 +/- 0.02</td>\n",
       "      <td>0.585 +/- 0.031</td>\n",
       "      <td>0.577 +/- 0.033</td>\n",
       "      <td>0.598 +/- 0.055</td>\n",
       "      <td>0.567 +/- 0.036</td>\n",
       "      <td>0.553 +/- 0.037</td>\n",
       "      <td>8 AND 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.831 +/- 0.037</td>\n",
       "      <td>0.721 +/- 0.021</td>\n",
       "      <td>0.719 +/- 0.024</td>\n",
       "      <td>0.579 +/- 0.073</td>\n",
       "      <td>0.496 +/- 0.049</td>\n",
       "      <td>0.491 +/- 0.05</td>\n",
       "      <td>8 AND 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.825 +/- 0.027</td>\n",
       "      <td>0.741 +/- 0.042</td>\n",
       "      <td>0.723 +/- 0.047</td>\n",
       "      <td>0.799 +/- 0.057</td>\n",
       "      <td>0.53 +/- 0.052</td>\n",
       "      <td>0.534 +/- 0.022</td>\n",
       "      <td>8 AND 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.838 +/- 0.053</td>\n",
       "      <td>0.715 +/- 0.057</td>\n",
       "      <td>0.734 +/- 0.072</td>\n",
       "      <td>0.76 +/- 0.051</td>\n",
       "      <td>0.494 +/- 0.038</td>\n",
       "      <td>0.559 +/- 0.047</td>\n",
       "      <td>8 AND 4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.852 +/- 0.031</td>\n",
       "      <td>0.676 +/- 0.026</td>\n",
       "      <td>0.678 +/- 0.031</td>\n",
       "      <td>0.636 +/- 0.036</td>\n",
       "      <td>0.522 +/- 0.034</td>\n",
       "      <td>0.515 +/- 0.025</td>\n",
       "      <td>8 AND 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.879 +/- 0.034</td>\n",
       "      <td>0.792 +/- 0.053</td>\n",
       "      <td>0.785 +/- 0.059</td>\n",
       "      <td>0.554 +/- 0.024</td>\n",
       "      <td>0.561 +/- 0.054</td>\n",
       "      <td>0.516 +/- 0.032</td>\n",
       "      <td>8 AND 6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.764 +/- 0.028</td>\n",
       "      <td>0.667 +/- 0.026</td>\n",
       "      <td>0.672 +/- 0.033</td>\n",
       "      <td>0.762 +/- 0.037</td>\n",
       "      <td>0.585 +/- 0.019</td>\n",
       "      <td>0.513 +/- 0.047</td>\n",
       "      <td>8 AND 7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.628 +/- 0.031</td>\n",
       "      <td>0.648 +/- 0.032</td>\n",
       "      <td>0.661 +/- 0.026</td>\n",
       "      <td>0.676 +/- 0.027</td>\n",
       "      <td>0.553 +/- 0.051</td>\n",
       "      <td>0.534 +/- 0.029</td>\n",
       "      <td>9 AND 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.644 +/- 0.049</td>\n",
       "      <td>0.614 +/- 0.074</td>\n",
       "      <td>0.614 +/- 0.047</td>\n",
       "      <td>0.568 +/- 0.022</td>\n",
       "      <td>0.54 +/- 0.019</td>\n",
       "      <td>0.52 +/- 0.05</td>\n",
       "      <td>9 AND 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.667 +/- 0.047</td>\n",
       "      <td>0.712 +/- 0.029</td>\n",
       "      <td>0.704 +/- 0.046</td>\n",
       "      <td>0.607 +/- 0.02</td>\n",
       "      <td>0.577 +/- 0.04</td>\n",
       "      <td>0.569 +/- 0.041</td>\n",
       "      <td>9 AND 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.755 +/- 0.046</td>\n",
       "      <td>0.704 +/- 0.023</td>\n",
       "      <td>0.697 +/- 0.025</td>\n",
       "      <td>0.527 +/- 0.054</td>\n",
       "      <td>0.556 +/- 0.016</td>\n",
       "      <td>0.558 +/- 0.039</td>\n",
       "      <td>9 AND 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.698 +/- 0.031</td>\n",
       "      <td>0.703 +/- 0.036</td>\n",
       "      <td>0.735 +/- 0.032</td>\n",
       "      <td>0.703 +/- 0.019</td>\n",
       "      <td>0.592 +/- 0.042</td>\n",
       "      <td>0.586 +/- 0.048</td>\n",
       "      <td>9 AND 4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.716 +/- 0.012</td>\n",
       "      <td>0.749 +/- 0.037</td>\n",
       "      <td>0.737 +/- 0.041</td>\n",
       "      <td>0.629 +/- 0.022</td>\n",
       "      <td>0.606 +/- 0.033</td>\n",
       "      <td>0.606 +/- 0.041</td>\n",
       "      <td>9 AND 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.698 +/- 0.055</td>\n",
       "      <td>0.715 +/- 0.049</td>\n",
       "      <td>0.702 +/- 0.035</td>\n",
       "      <td>0.615 +/- 0.048</td>\n",
       "      <td>0.49 +/- 0.042</td>\n",
       "      <td>0.507 +/- 0.033</td>\n",
       "      <td>9 AND 6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.706 +/- 0.039</td>\n",
       "      <td>0.657 +/- 0.023</td>\n",
       "      <td>0.688 +/- 0.018</td>\n",
       "      <td>0.681 +/- 0.049</td>\n",
       "      <td>0.525 +/- 0.033</td>\n",
       "      <td>0.545 +/- 0.037</td>\n",
       "      <td>9 AND 7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.611 +/- 0.015</td>\n",
       "      <td>0.586 +/- 0.011</td>\n",
       "      <td>0.581 +/- 0.029</td>\n",
       "      <td>0.579 +/- 0.035</td>\n",
       "      <td>0.571 +/- 0.043</td>\n",
       "      <td>0.537 +/- 0.044</td>\n",
       "      <td>9 AND 8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   no_preprocessing              PCA           cPCA-0           cPCA-1  \\\n",
       "0   0.631 +/- 0.028  0.578 +/- 0.031  0.576 +/- 0.022  0.631 +/- 0.028   \n",
       "1   0.754 +/- 0.052  0.626 +/- 0.033   0.631 +/- 0.05  0.564 +/- 0.058   \n",
       "2   0.629 +/- 0.022   0.596 +/- 0.03  0.624 +/- 0.041  0.644 +/- 0.044   \n",
       "3   0.754 +/- 0.038  0.652 +/- 0.053  0.641 +/- 0.049  0.704 +/- 0.051   \n",
       "4   0.728 +/- 0.069  0.591 +/- 0.065  0.625 +/- 0.043  0.562 +/- 0.053   \n",
       "5   0.605 +/- 0.036  0.552 +/- 0.051  0.552 +/- 0.063  0.595 +/- 0.044   \n",
       "6   0.773 +/- 0.056  0.693 +/- 0.052  0.641 +/- 0.049   0.612 +/- 0.04   \n",
       "7   0.657 +/- 0.048  0.687 +/- 0.053   0.69 +/- 0.068  0.755 +/- 0.012   \n",
       "8   0.583 +/- 0.034  0.543 +/- 0.023  0.541 +/- 0.064  0.543 +/- 0.024   \n",
       "9   0.617 +/- 0.056  0.516 +/- 0.028  0.491 +/- 0.031  0.555 +/- 0.027   \n",
       "10  0.812 +/- 0.039  0.737 +/- 0.053  0.729 +/- 0.039  0.621 +/- 0.051   \n",
       "11  0.713 +/- 0.042  0.634 +/- 0.047  0.637 +/- 0.038  0.689 +/- 0.047   \n",
       "12  0.614 +/- 0.041  0.641 +/- 0.049  0.622 +/- 0.061  0.534 +/- 0.023   \n",
       "13  0.539 +/- 0.034  0.453 +/- 0.024  0.474 +/- 0.047    0.49 +/- 0.04   \n",
       "14  0.629 +/- 0.059  0.475 +/- 0.024   0.478 +/- 0.05  0.646 +/- 0.033   \n",
       "15  0.842 +/- 0.034   0.72 +/- 0.065  0.705 +/- 0.039  0.807 +/- 0.028   \n",
       "16  0.642 +/- 0.008   0.658 +/- 0.02  0.642 +/- 0.028   0.71 +/- 0.024   \n",
       "17  0.607 +/- 0.039  0.516 +/- 0.034  0.499 +/- 0.016    0.61 +/- 0.04   \n",
       "18   0.61 +/- 0.027  0.562 +/- 0.049  0.562 +/- 0.042  0.612 +/- 0.032   \n",
       "19  0.603 +/- 0.049  0.562 +/- 0.029  0.608 +/- 0.048  0.519 +/- 0.021   \n",
       "20  0.659 +/- 0.021  0.634 +/- 0.075  0.644 +/- 0.033  0.558 +/- 0.045   \n",
       "21  0.699 +/- 0.033  0.645 +/- 0.048  0.663 +/- 0.047   0.57 +/- 0.045   \n",
       "22  0.715 +/- 0.038  0.528 +/- 0.045  0.556 +/- 0.041  0.577 +/- 0.044   \n",
       "23   0.654 +/- 0.03  0.565 +/- 0.022  0.582 +/- 0.027  0.612 +/- 0.045   \n",
       "24  0.644 +/- 0.031  0.516 +/- 0.061  0.509 +/- 0.054   0.523 +/- 0.01   \n",
       "25  0.621 +/- 0.027  0.555 +/- 0.045   0.565 +/- 0.05  0.548 +/- 0.025   \n",
       "26  0.676 +/- 0.018  0.582 +/- 0.059  0.541 +/- 0.041   0.546 +/- 0.03   \n",
       "27  0.643 +/- 0.029  0.596 +/- 0.051  0.596 +/- 0.048  0.623 +/- 0.033   \n",
       "28  0.661 +/- 0.064  0.604 +/- 0.037  0.606 +/- 0.048  0.584 +/- 0.072   \n",
       "29   0.625 +/- 0.02  0.585 +/- 0.031  0.577 +/- 0.033  0.598 +/- 0.055   \n",
       "30  0.831 +/- 0.037  0.721 +/- 0.021  0.719 +/- 0.024  0.579 +/- 0.073   \n",
       "31  0.825 +/- 0.027  0.741 +/- 0.042  0.723 +/- 0.047  0.799 +/- 0.057   \n",
       "32  0.838 +/- 0.053  0.715 +/- 0.057  0.734 +/- 0.072   0.76 +/- 0.051   \n",
       "33  0.852 +/- 0.031  0.676 +/- 0.026  0.678 +/- 0.031  0.636 +/- 0.036   \n",
       "34  0.879 +/- 0.034  0.792 +/- 0.053  0.785 +/- 0.059  0.554 +/- 0.024   \n",
       "35  0.764 +/- 0.028  0.667 +/- 0.026  0.672 +/- 0.033  0.762 +/- 0.037   \n",
       "36  0.628 +/- 0.031  0.648 +/- 0.032  0.661 +/- 0.026  0.676 +/- 0.027   \n",
       "37  0.644 +/- 0.049  0.614 +/- 0.074  0.614 +/- 0.047  0.568 +/- 0.022   \n",
       "38  0.667 +/- 0.047  0.712 +/- 0.029  0.704 +/- 0.046   0.607 +/- 0.02   \n",
       "39  0.755 +/- 0.046  0.704 +/- 0.023  0.697 +/- 0.025  0.527 +/- 0.054   \n",
       "40  0.698 +/- 0.031  0.703 +/- 0.036  0.735 +/- 0.032  0.703 +/- 0.019   \n",
       "41  0.716 +/- 0.012  0.749 +/- 0.037  0.737 +/- 0.041  0.629 +/- 0.022   \n",
       "42  0.698 +/- 0.055  0.715 +/- 0.049  0.702 +/- 0.035  0.615 +/- 0.048   \n",
       "43  0.706 +/- 0.039  0.657 +/- 0.023  0.688 +/- 0.018  0.681 +/- 0.049   \n",
       "44  0.611 +/- 0.015  0.586 +/- 0.011  0.581 +/- 0.029  0.579 +/- 0.035   \n",
       "\n",
       "             cPCA-2           cPCA-3     name  \n",
       "0    0.61 +/- 0.061  0.549 +/- 0.041  1 AND 0  \n",
       "1   0.502 +/- 0.028   0.51 +/- 0.061  2 AND 0  \n",
       "2   0.669 +/- 0.043  0.566 +/- 0.088  2 AND 1  \n",
       "3   0.508 +/- 0.031   0.49 +/- 0.042  3 AND 0  \n",
       "4     0.63 +/- 0.06  0.526 +/- 0.036  3 AND 1  \n",
       "5   0.535 +/- 0.032  0.542 +/- 0.038  3 AND 2  \n",
       "6    0.47 +/- 0.032  0.499 +/- 0.063  4 AND 0  \n",
       "7   0.575 +/- 0.064  0.537 +/- 0.059  4 AND 1  \n",
       "8   0.483 +/- 0.028  0.543 +/- 0.052  4 AND 2  \n",
       "9    0.495 +/- 0.07  0.493 +/- 0.045  4 AND 3  \n",
       "10  0.517 +/- 0.045  0.473 +/- 0.018  5 AND 0  \n",
       "11  0.547 +/- 0.056  0.521 +/- 0.046  5 AND 1  \n",
       "12  0.474 +/- 0.073  0.482 +/- 0.058  5 AND 2  \n",
       "13  0.479 +/- 0.019  0.506 +/- 0.064  5 AND 3  \n",
       "14  0.559 +/- 0.033  0.468 +/- 0.042  5 AND 4  \n",
       "15  0.649 +/- 0.042  0.532 +/- 0.038  6 AND 0  \n",
       "16   0.59 +/- 0.042   0.531 +/- 0.05  6 AND 1  \n",
       "17   0.499 +/- 0.02  0.525 +/- 0.029  6 AND 2  \n",
       "18  0.495 +/- 0.067  0.543 +/- 0.033  6 AND 3  \n",
       "19   0.606 +/- 0.06  0.534 +/- 0.021  6 AND 4  \n",
       "20   0.515 +/- 0.04  0.556 +/- 0.033  6 AND 5  \n",
       "21  0.575 +/- 0.054  0.552 +/- 0.033  7 AND 0  \n",
       "22  0.567 +/- 0.026  0.518 +/- 0.013  7 AND 1  \n",
       "23   0.512 +/- 0.04   0.545 +/- 0.07  7 AND 2  \n",
       "24   0.514 +/- 0.02  0.511 +/- 0.049  7 AND 3  \n",
       "25  0.548 +/- 0.053   0.518 +/- 0.02  7 AND 4  \n",
       "26   0.53 +/- 0.041  0.532 +/- 0.056  7 AND 5  \n",
       "27  0.481 +/- 0.041  0.501 +/- 0.031  7 AND 6  \n",
       "28  0.491 +/- 0.029   0.486 +/- 0.04  8 AND 0  \n",
       "29  0.567 +/- 0.036  0.553 +/- 0.037  8 AND 1  \n",
       "30  0.496 +/- 0.049   0.491 +/- 0.05  8 AND 2  \n",
       "31   0.53 +/- 0.052  0.534 +/- 0.022  8 AND 3  \n",
       "32  0.494 +/- 0.038  0.559 +/- 0.047  8 AND 4  \n",
       "33  0.522 +/- 0.034  0.515 +/- 0.025  8 AND 5  \n",
       "34  0.561 +/- 0.054  0.516 +/- 0.032  8 AND 6  \n",
       "35  0.585 +/- 0.019  0.513 +/- 0.047  8 AND 7  \n",
       "36  0.553 +/- 0.051  0.534 +/- 0.029  9 AND 0  \n",
       "37   0.54 +/- 0.019    0.52 +/- 0.05  9 AND 1  \n",
       "38   0.577 +/- 0.04  0.569 +/- 0.041  9 AND 2  \n",
       "39  0.556 +/- 0.016  0.558 +/- 0.039  9 AND 3  \n",
       "40  0.592 +/- 0.042  0.586 +/- 0.048  9 AND 4  \n",
       "41  0.606 +/- 0.033  0.606 +/- 0.041  9 AND 5  \n",
       "42   0.49 +/- 0.042  0.507 +/- 0.033  9 AND 6  \n",
       "43  0.525 +/- 0.033  0.545 +/- 0.037  9 AND 7  \n",
       "44  0.571 +/- 0.043  0.537 +/- 0.044  9 AND 8  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(results_list)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd7205e6-69f0-4e44-838d-ee01f9692163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_5e087_row0_col0, #T_5e087_row0_col1, #T_5e087_row0_col2, #T_5e087_row0_col3, #T_5e087_row0_col4, #T_5e087_row0_col5, #T_5e087_row1_col0, #T_5e087_row1_col1, #T_5e087_row1_col2, #T_5e087_row1_col3, #T_5e087_row1_col4, #T_5e087_row1_col5, #T_5e087_row2_col0, #T_5e087_row2_col1, #T_5e087_row2_col2, #T_5e087_row2_col5, #T_5e087_row3_col0, #T_5e087_row3_col1, #T_5e087_row3_col2, #T_5e087_row3_col3, #T_5e087_row3_col4, #T_5e087_row3_col5, #T_5e087_row4_col0, #T_5e087_row4_col1, #T_5e087_row4_col2, #T_5e087_row4_col3, #T_5e087_row4_col4, #T_5e087_row4_col5, #T_5e087_row5_col0, #T_5e087_row5_col1, #T_5e087_row5_col2, #T_5e087_row5_col3, #T_5e087_row5_col4, #T_5e087_row5_col5, #T_5e087_row6_col0, #T_5e087_row6_col1, #T_5e087_row6_col2, #T_5e087_row6_col3, #T_5e087_row6_col4, #T_5e087_row6_col5, #T_5e087_row7_col0, #T_5e087_row7_col4, #T_5e087_row7_col5, #T_5e087_row8_col0, #T_5e087_row8_col1, #T_5e087_row8_col2, #T_5e087_row8_col3, #T_5e087_row8_col4, #T_5e087_row8_col5, #T_5e087_row9_col0, #T_5e087_row9_col1, #T_5e087_row9_col2, #T_5e087_row9_col3, #T_5e087_row9_col4, #T_5e087_row9_col5, #T_5e087_row10_col0, #T_5e087_row10_col1, #T_5e087_row10_col2, #T_5e087_row10_col3, #T_5e087_row10_col4, #T_5e087_row10_col5, #T_5e087_row11_col0, #T_5e087_row11_col1, #T_5e087_row11_col2, #T_5e087_row11_col3, #T_5e087_row11_col4, #T_5e087_row11_col5, #T_5e087_row12_col0, #T_5e087_row12_col3, #T_5e087_row12_col4, #T_5e087_row12_col5, #T_5e087_row13_col0, #T_5e087_row13_col1, #T_5e087_row13_col2, #T_5e087_row13_col3, #T_5e087_row13_col4, #T_5e087_row13_col5, #T_5e087_row14_col0, #T_5e087_row14_col1, #T_5e087_row14_col2, #T_5e087_row14_col4, #T_5e087_row14_col5, #T_5e087_row15_col0, #T_5e087_row15_col1, #T_5e087_row15_col2, #T_5e087_row15_col3, #T_5e087_row15_col4, #T_5e087_row15_col5, #T_5e087_row16_col0, #T_5e087_row16_col2, #T_5e087_row16_col4, #T_5e087_row16_col5, #T_5e087_row17_col0, #T_5e087_row17_col1, #T_5e087_row17_col2, #T_5e087_row17_col4, #T_5e087_row17_col5, #T_5e087_row18_col0, #T_5e087_row18_col1, #T_5e087_row18_col2, #T_5e087_row18_col4, #T_5e087_row18_col5, #T_5e087_row19_col0, #T_5e087_row19_col1, #T_5e087_row19_col3, #T_5e087_row19_col5, #T_5e087_row20_col0, #T_5e087_row20_col1, #T_5e087_row20_col2, #T_5e087_row20_col3, #T_5e087_row20_col4, #T_5e087_row20_col5, #T_5e087_row21_col0, #T_5e087_row21_col1, #T_5e087_row21_col2, #T_5e087_row21_col3, #T_5e087_row21_col4, #T_5e087_row21_col5, #T_5e087_row22_col0, #T_5e087_row22_col1, #T_5e087_row22_col2, #T_5e087_row22_col3, #T_5e087_row22_col4, #T_5e087_row22_col5, #T_5e087_row23_col0, #T_5e087_row23_col1, #T_5e087_row23_col2, #T_5e087_row23_col3, #T_5e087_row23_col4, #T_5e087_row23_col5, #T_5e087_row24_col0, #T_5e087_row24_col1, #T_5e087_row24_col2, #T_5e087_row24_col3, #T_5e087_row24_col4, #T_5e087_row24_col5, #T_5e087_row25_col0, #T_5e087_row25_col1, #T_5e087_row25_col2, #T_5e087_row25_col3, #T_5e087_row25_col4, #T_5e087_row25_col5, #T_5e087_row26_col0, #T_5e087_row26_col1, #T_5e087_row26_col2, #T_5e087_row26_col3, #T_5e087_row26_col4, #T_5e087_row26_col5, #T_5e087_row27_col0, #T_5e087_row27_col1, #T_5e087_row27_col2, #T_5e087_row27_col3, #T_5e087_row27_col4, #T_5e087_row27_col5, #T_5e087_row28_col0, #T_5e087_row28_col1, #T_5e087_row28_col2, #T_5e087_row28_col3, #T_5e087_row28_col4, #T_5e087_row28_col5, #T_5e087_row29_col0, #T_5e087_row29_col1, #T_5e087_row29_col2, #T_5e087_row29_col3, #T_5e087_row29_col4, #T_5e087_row29_col5, #T_5e087_row30_col0, #T_5e087_row30_col1, #T_5e087_row30_col2, #T_5e087_row30_col3, #T_5e087_row30_col4, #T_5e087_row30_col5, #T_5e087_row31_col0, #T_5e087_row31_col1, #T_5e087_row31_col2, #T_5e087_row31_col3, #T_5e087_row31_col4, #T_5e087_row31_col5, #T_5e087_row32_col0, #T_5e087_row32_col1, #T_5e087_row32_col2, #T_5e087_row32_col3, #T_5e087_row32_col4, #T_5e087_row32_col5, #T_5e087_row33_col0, #T_5e087_row33_col1, #T_5e087_row33_col2, #T_5e087_row33_col3, #T_5e087_row33_col4, #T_5e087_row33_col5, #T_5e087_row34_col0, #T_5e087_row34_col1, #T_5e087_row34_col2, #T_5e087_row34_col3, #T_5e087_row34_col4, #T_5e087_row34_col5, #T_5e087_row35_col0, #T_5e087_row35_col1, #T_5e087_row35_col2, #T_5e087_row35_col3, #T_5e087_row35_col4, #T_5e087_row35_col5, #T_5e087_row36_col0, #T_5e087_row36_col4, #T_5e087_row36_col5, #T_5e087_row37_col0, #T_5e087_row37_col1, #T_5e087_row37_col2, #T_5e087_row37_col3, #T_5e087_row37_col4, #T_5e087_row37_col5, #T_5e087_row38_col0, #T_5e087_row38_col3, #T_5e087_row38_col4, #T_5e087_row38_col5, #T_5e087_row39_col0, #T_5e087_row39_col1, #T_5e087_row39_col2, #T_5e087_row39_col3, #T_5e087_row39_col4, #T_5e087_row39_col5, #T_5e087_row40_col0, #T_5e087_row40_col4, #T_5e087_row40_col5, #T_5e087_row41_col0, #T_5e087_row41_col3, #T_5e087_row41_col4, #T_5e087_row41_col5, #T_5e087_row42_col0, #T_5e087_row42_col3, #T_5e087_row42_col4, #T_5e087_row42_col5, #T_5e087_row43_col0, #T_5e087_row43_col1, #T_5e087_row43_col2, #T_5e087_row43_col3, #T_5e087_row43_col4, #T_5e087_row43_col5, #T_5e087_row44_col0, #T_5e087_row44_col1, #T_5e087_row44_col2, #T_5e087_row44_col3, #T_5e087_row44_col4, #T_5e087_row44_col5 {\n",
       "  background-color: white;\n",
       "}\n",
       "#T_5e087_row2_col3, #T_5e087_row2_col4, #T_5e087_row7_col1, #T_5e087_row7_col2, #T_5e087_row7_col3, #T_5e087_row12_col1, #T_5e087_row12_col2, #T_5e087_row14_col3, #T_5e087_row16_col1, #T_5e087_row16_col3, #T_5e087_row17_col3, #T_5e087_row18_col3, #T_5e087_row19_col2, #T_5e087_row19_col4, #T_5e087_row36_col1, #T_5e087_row36_col2, #T_5e087_row36_col3, #T_5e087_row38_col1, #T_5e087_row38_col2, #T_5e087_row40_col1, #T_5e087_row40_col2, #T_5e087_row40_col3, #T_5e087_row41_col1, #T_5e087_row41_col2, #T_5e087_row42_col1, #T_5e087_row42_col2 {\n",
       "  background-color: rgb(100,255,100);\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_5e087\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_5e087_level0_col0\" class=\"col_heading level0 col0\" >no_preprocessing</th>\n",
       "      <th id=\"T_5e087_level0_col1\" class=\"col_heading level0 col1\" >PCA</th>\n",
       "      <th id=\"T_5e087_level0_col2\" class=\"col_heading level0 col2\" >cPCA-0</th>\n",
       "      <th id=\"T_5e087_level0_col3\" class=\"col_heading level0 col3\" >cPCA-1</th>\n",
       "      <th id=\"T_5e087_level0_col4\" class=\"col_heading level0 col4\" >cPCA-2</th>\n",
       "      <th id=\"T_5e087_level0_col5\" class=\"col_heading level0 col5\" >cPCA-3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >name</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row0\" class=\"row_heading level0 row0\" >1 AND 0</th>\n",
       "      <td id=\"T_5e087_row0_col0\" class=\"data row0 col0\" >0.631 +/- 0.028</td>\n",
       "      <td id=\"T_5e087_row0_col1\" class=\"data row0 col1\" >0.578 +/- 0.031</td>\n",
       "      <td id=\"T_5e087_row0_col2\" class=\"data row0 col2\" >0.576 +/- 0.022</td>\n",
       "      <td id=\"T_5e087_row0_col3\" class=\"data row0 col3\" >0.631 +/- 0.028</td>\n",
       "      <td id=\"T_5e087_row0_col4\" class=\"data row0 col4\" >0.61 +/- 0.061</td>\n",
       "      <td id=\"T_5e087_row0_col5\" class=\"data row0 col5\" >0.549 +/- 0.041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row1\" class=\"row_heading level0 row1\" >2 AND 0</th>\n",
       "      <td id=\"T_5e087_row1_col0\" class=\"data row1 col0\" >0.754 +/- 0.052</td>\n",
       "      <td id=\"T_5e087_row1_col1\" class=\"data row1 col1\" >0.626 +/- 0.033</td>\n",
       "      <td id=\"T_5e087_row1_col2\" class=\"data row1 col2\" >0.631 +/- 0.05</td>\n",
       "      <td id=\"T_5e087_row1_col3\" class=\"data row1 col3\" >0.564 +/- 0.058</td>\n",
       "      <td id=\"T_5e087_row1_col4\" class=\"data row1 col4\" >0.502 +/- 0.028</td>\n",
       "      <td id=\"T_5e087_row1_col5\" class=\"data row1 col5\" >0.51 +/- 0.061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row2\" class=\"row_heading level0 row2\" >2 AND 1</th>\n",
       "      <td id=\"T_5e087_row2_col0\" class=\"data row2 col0\" >0.629 +/- 0.022</td>\n",
       "      <td id=\"T_5e087_row2_col1\" class=\"data row2 col1\" >0.596 +/- 0.03</td>\n",
       "      <td id=\"T_5e087_row2_col2\" class=\"data row2 col2\" >0.624 +/- 0.041</td>\n",
       "      <td id=\"T_5e087_row2_col3\" class=\"data row2 col3\" >0.644 +/- 0.044</td>\n",
       "      <td id=\"T_5e087_row2_col4\" class=\"data row2 col4\" >0.669 +/- 0.043</td>\n",
       "      <td id=\"T_5e087_row2_col5\" class=\"data row2 col5\" >0.566 +/- 0.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row3\" class=\"row_heading level0 row3\" >3 AND 0</th>\n",
       "      <td id=\"T_5e087_row3_col0\" class=\"data row3 col0\" >0.754 +/- 0.038</td>\n",
       "      <td id=\"T_5e087_row3_col1\" class=\"data row3 col1\" >0.652 +/- 0.053</td>\n",
       "      <td id=\"T_5e087_row3_col2\" class=\"data row3 col2\" >0.641 +/- 0.049</td>\n",
       "      <td id=\"T_5e087_row3_col3\" class=\"data row3 col3\" >0.704 +/- 0.051</td>\n",
       "      <td id=\"T_5e087_row3_col4\" class=\"data row3 col4\" >0.508 +/- 0.031</td>\n",
       "      <td id=\"T_5e087_row3_col5\" class=\"data row3 col5\" >0.49 +/- 0.042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row4\" class=\"row_heading level0 row4\" >3 AND 1</th>\n",
       "      <td id=\"T_5e087_row4_col0\" class=\"data row4 col0\" >0.728 +/- 0.069</td>\n",
       "      <td id=\"T_5e087_row4_col1\" class=\"data row4 col1\" >0.591 +/- 0.065</td>\n",
       "      <td id=\"T_5e087_row4_col2\" class=\"data row4 col2\" >0.625 +/- 0.043</td>\n",
       "      <td id=\"T_5e087_row4_col3\" class=\"data row4 col3\" >0.562 +/- 0.053</td>\n",
       "      <td id=\"T_5e087_row4_col4\" class=\"data row4 col4\" >0.63 +/- 0.06</td>\n",
       "      <td id=\"T_5e087_row4_col5\" class=\"data row4 col5\" >0.526 +/- 0.036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row5\" class=\"row_heading level0 row5\" >3 AND 2</th>\n",
       "      <td id=\"T_5e087_row5_col0\" class=\"data row5 col0\" >0.605 +/- 0.036</td>\n",
       "      <td id=\"T_5e087_row5_col1\" class=\"data row5 col1\" >0.552 +/- 0.051</td>\n",
       "      <td id=\"T_5e087_row5_col2\" class=\"data row5 col2\" >0.552 +/- 0.063</td>\n",
       "      <td id=\"T_5e087_row5_col3\" class=\"data row5 col3\" >0.595 +/- 0.044</td>\n",
       "      <td id=\"T_5e087_row5_col4\" class=\"data row5 col4\" >0.535 +/- 0.032</td>\n",
       "      <td id=\"T_5e087_row5_col5\" class=\"data row5 col5\" >0.542 +/- 0.038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row6\" class=\"row_heading level0 row6\" >4 AND 0</th>\n",
       "      <td id=\"T_5e087_row6_col0\" class=\"data row6 col0\" >0.773 +/- 0.056</td>\n",
       "      <td id=\"T_5e087_row6_col1\" class=\"data row6 col1\" >0.693 +/- 0.052</td>\n",
       "      <td id=\"T_5e087_row6_col2\" class=\"data row6 col2\" >0.641 +/- 0.049</td>\n",
       "      <td id=\"T_5e087_row6_col3\" class=\"data row6 col3\" >0.612 +/- 0.04</td>\n",
       "      <td id=\"T_5e087_row6_col4\" class=\"data row6 col4\" >0.47 +/- 0.032</td>\n",
       "      <td id=\"T_5e087_row6_col5\" class=\"data row6 col5\" >0.499 +/- 0.063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row7\" class=\"row_heading level0 row7\" >4 AND 1</th>\n",
       "      <td id=\"T_5e087_row7_col0\" class=\"data row7 col0\" >0.657 +/- 0.048</td>\n",
       "      <td id=\"T_5e087_row7_col1\" class=\"data row7 col1\" >0.687 +/- 0.053</td>\n",
       "      <td id=\"T_5e087_row7_col2\" class=\"data row7 col2\" >0.69 +/- 0.068</td>\n",
       "      <td id=\"T_5e087_row7_col3\" class=\"data row7 col3\" >0.755 +/- 0.012</td>\n",
       "      <td id=\"T_5e087_row7_col4\" class=\"data row7 col4\" >0.575 +/- 0.064</td>\n",
       "      <td id=\"T_5e087_row7_col5\" class=\"data row7 col5\" >0.537 +/- 0.059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row8\" class=\"row_heading level0 row8\" >4 AND 2</th>\n",
       "      <td id=\"T_5e087_row8_col0\" class=\"data row8 col0\" >0.583 +/- 0.034</td>\n",
       "      <td id=\"T_5e087_row8_col1\" class=\"data row8 col1\" >0.543 +/- 0.023</td>\n",
       "      <td id=\"T_5e087_row8_col2\" class=\"data row8 col2\" >0.541 +/- 0.064</td>\n",
       "      <td id=\"T_5e087_row8_col3\" class=\"data row8 col3\" >0.543 +/- 0.024</td>\n",
       "      <td id=\"T_5e087_row8_col4\" class=\"data row8 col4\" >0.483 +/- 0.028</td>\n",
       "      <td id=\"T_5e087_row8_col5\" class=\"data row8 col5\" >0.543 +/- 0.052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row9\" class=\"row_heading level0 row9\" >4 AND 3</th>\n",
       "      <td id=\"T_5e087_row9_col0\" class=\"data row9 col0\" >0.617 +/- 0.056</td>\n",
       "      <td id=\"T_5e087_row9_col1\" class=\"data row9 col1\" >0.516 +/- 0.028</td>\n",
       "      <td id=\"T_5e087_row9_col2\" class=\"data row9 col2\" >0.491 +/- 0.031</td>\n",
       "      <td id=\"T_5e087_row9_col3\" class=\"data row9 col3\" >0.555 +/- 0.027</td>\n",
       "      <td id=\"T_5e087_row9_col4\" class=\"data row9 col4\" >0.495 +/- 0.07</td>\n",
       "      <td id=\"T_5e087_row9_col5\" class=\"data row9 col5\" >0.493 +/- 0.045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row10\" class=\"row_heading level0 row10\" >5 AND 0</th>\n",
       "      <td id=\"T_5e087_row10_col0\" class=\"data row10 col0\" >0.812 +/- 0.039</td>\n",
       "      <td id=\"T_5e087_row10_col1\" class=\"data row10 col1\" >0.737 +/- 0.053</td>\n",
       "      <td id=\"T_5e087_row10_col2\" class=\"data row10 col2\" >0.729 +/- 0.039</td>\n",
       "      <td id=\"T_5e087_row10_col3\" class=\"data row10 col3\" >0.621 +/- 0.051</td>\n",
       "      <td id=\"T_5e087_row10_col4\" class=\"data row10 col4\" >0.517 +/- 0.045</td>\n",
       "      <td id=\"T_5e087_row10_col5\" class=\"data row10 col5\" >0.473 +/- 0.018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row11\" class=\"row_heading level0 row11\" >5 AND 1</th>\n",
       "      <td id=\"T_5e087_row11_col0\" class=\"data row11 col0\" >0.713 +/- 0.042</td>\n",
       "      <td id=\"T_5e087_row11_col1\" class=\"data row11 col1\" >0.634 +/- 0.047</td>\n",
       "      <td id=\"T_5e087_row11_col2\" class=\"data row11 col2\" >0.637 +/- 0.038</td>\n",
       "      <td id=\"T_5e087_row11_col3\" class=\"data row11 col3\" >0.689 +/- 0.047</td>\n",
       "      <td id=\"T_5e087_row11_col4\" class=\"data row11 col4\" >0.547 +/- 0.056</td>\n",
       "      <td id=\"T_5e087_row11_col5\" class=\"data row11 col5\" >0.521 +/- 0.046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row12\" class=\"row_heading level0 row12\" >5 AND 2</th>\n",
       "      <td id=\"T_5e087_row12_col0\" class=\"data row12 col0\" >0.614 +/- 0.041</td>\n",
       "      <td id=\"T_5e087_row12_col1\" class=\"data row12 col1\" >0.641 +/- 0.049</td>\n",
       "      <td id=\"T_5e087_row12_col2\" class=\"data row12 col2\" >0.622 +/- 0.061</td>\n",
       "      <td id=\"T_5e087_row12_col3\" class=\"data row12 col3\" >0.534 +/- 0.023</td>\n",
       "      <td id=\"T_5e087_row12_col4\" class=\"data row12 col4\" >0.474 +/- 0.073</td>\n",
       "      <td id=\"T_5e087_row12_col5\" class=\"data row12 col5\" >0.482 +/- 0.058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row13\" class=\"row_heading level0 row13\" >5 AND 3</th>\n",
       "      <td id=\"T_5e087_row13_col0\" class=\"data row13 col0\" >0.539 +/- 0.034</td>\n",
       "      <td id=\"T_5e087_row13_col1\" class=\"data row13 col1\" >0.453 +/- 0.024</td>\n",
       "      <td id=\"T_5e087_row13_col2\" class=\"data row13 col2\" >0.474 +/- 0.047</td>\n",
       "      <td id=\"T_5e087_row13_col3\" class=\"data row13 col3\" >0.49 +/- 0.04</td>\n",
       "      <td id=\"T_5e087_row13_col4\" class=\"data row13 col4\" >0.479 +/- 0.019</td>\n",
       "      <td id=\"T_5e087_row13_col5\" class=\"data row13 col5\" >0.506 +/- 0.064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row14\" class=\"row_heading level0 row14\" >5 AND 4</th>\n",
       "      <td id=\"T_5e087_row14_col0\" class=\"data row14 col0\" >0.629 +/- 0.059</td>\n",
       "      <td id=\"T_5e087_row14_col1\" class=\"data row14 col1\" >0.475 +/- 0.024</td>\n",
       "      <td id=\"T_5e087_row14_col2\" class=\"data row14 col2\" >0.478 +/- 0.05</td>\n",
       "      <td id=\"T_5e087_row14_col3\" class=\"data row14 col3\" >0.646 +/- 0.033</td>\n",
       "      <td id=\"T_5e087_row14_col4\" class=\"data row14 col4\" >0.559 +/- 0.033</td>\n",
       "      <td id=\"T_5e087_row14_col5\" class=\"data row14 col5\" >0.468 +/- 0.042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row15\" class=\"row_heading level0 row15\" >6 AND 0</th>\n",
       "      <td id=\"T_5e087_row15_col0\" class=\"data row15 col0\" >0.842 +/- 0.034</td>\n",
       "      <td id=\"T_5e087_row15_col1\" class=\"data row15 col1\" >0.72 +/- 0.065</td>\n",
       "      <td id=\"T_5e087_row15_col2\" class=\"data row15 col2\" >0.705 +/- 0.039</td>\n",
       "      <td id=\"T_5e087_row15_col3\" class=\"data row15 col3\" >0.807 +/- 0.028</td>\n",
       "      <td id=\"T_5e087_row15_col4\" class=\"data row15 col4\" >0.649 +/- 0.042</td>\n",
       "      <td id=\"T_5e087_row15_col5\" class=\"data row15 col5\" >0.532 +/- 0.038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row16\" class=\"row_heading level0 row16\" >6 AND 1</th>\n",
       "      <td id=\"T_5e087_row16_col0\" class=\"data row16 col0\" >0.642 +/- 0.008</td>\n",
       "      <td id=\"T_5e087_row16_col1\" class=\"data row16 col1\" >0.658 +/- 0.02</td>\n",
       "      <td id=\"T_5e087_row16_col2\" class=\"data row16 col2\" >0.642 +/- 0.028</td>\n",
       "      <td id=\"T_5e087_row16_col3\" class=\"data row16 col3\" >0.71 +/- 0.024</td>\n",
       "      <td id=\"T_5e087_row16_col4\" class=\"data row16 col4\" >0.59 +/- 0.042</td>\n",
       "      <td id=\"T_5e087_row16_col5\" class=\"data row16 col5\" >0.531 +/- 0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row17\" class=\"row_heading level0 row17\" >6 AND 2</th>\n",
       "      <td id=\"T_5e087_row17_col0\" class=\"data row17 col0\" >0.607 +/- 0.039</td>\n",
       "      <td id=\"T_5e087_row17_col1\" class=\"data row17 col1\" >0.516 +/- 0.034</td>\n",
       "      <td id=\"T_5e087_row17_col2\" class=\"data row17 col2\" >0.499 +/- 0.016</td>\n",
       "      <td id=\"T_5e087_row17_col3\" class=\"data row17 col3\" >0.61 +/- 0.04</td>\n",
       "      <td id=\"T_5e087_row17_col4\" class=\"data row17 col4\" >0.499 +/- 0.02</td>\n",
       "      <td id=\"T_5e087_row17_col5\" class=\"data row17 col5\" >0.525 +/- 0.029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row18\" class=\"row_heading level0 row18\" >6 AND 3</th>\n",
       "      <td id=\"T_5e087_row18_col0\" class=\"data row18 col0\" >0.61 +/- 0.027</td>\n",
       "      <td id=\"T_5e087_row18_col1\" class=\"data row18 col1\" >0.562 +/- 0.049</td>\n",
       "      <td id=\"T_5e087_row18_col2\" class=\"data row18 col2\" >0.562 +/- 0.042</td>\n",
       "      <td id=\"T_5e087_row18_col3\" class=\"data row18 col3\" >0.612 +/- 0.032</td>\n",
       "      <td id=\"T_5e087_row18_col4\" class=\"data row18 col4\" >0.495 +/- 0.067</td>\n",
       "      <td id=\"T_5e087_row18_col5\" class=\"data row18 col5\" >0.543 +/- 0.033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row19\" class=\"row_heading level0 row19\" >6 AND 4</th>\n",
       "      <td id=\"T_5e087_row19_col0\" class=\"data row19 col0\" >0.603 +/- 0.049</td>\n",
       "      <td id=\"T_5e087_row19_col1\" class=\"data row19 col1\" >0.562 +/- 0.029</td>\n",
       "      <td id=\"T_5e087_row19_col2\" class=\"data row19 col2\" >0.608 +/- 0.048</td>\n",
       "      <td id=\"T_5e087_row19_col3\" class=\"data row19 col3\" >0.519 +/- 0.021</td>\n",
       "      <td id=\"T_5e087_row19_col4\" class=\"data row19 col4\" >0.606 +/- 0.06</td>\n",
       "      <td id=\"T_5e087_row19_col5\" class=\"data row19 col5\" >0.534 +/- 0.021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row20\" class=\"row_heading level0 row20\" >6 AND 5</th>\n",
       "      <td id=\"T_5e087_row20_col0\" class=\"data row20 col0\" >0.659 +/- 0.021</td>\n",
       "      <td id=\"T_5e087_row20_col1\" class=\"data row20 col1\" >0.634 +/- 0.075</td>\n",
       "      <td id=\"T_5e087_row20_col2\" class=\"data row20 col2\" >0.644 +/- 0.033</td>\n",
       "      <td id=\"T_5e087_row20_col3\" class=\"data row20 col3\" >0.558 +/- 0.045</td>\n",
       "      <td id=\"T_5e087_row20_col4\" class=\"data row20 col4\" >0.515 +/- 0.04</td>\n",
       "      <td id=\"T_5e087_row20_col5\" class=\"data row20 col5\" >0.556 +/- 0.033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row21\" class=\"row_heading level0 row21\" >7 AND 0</th>\n",
       "      <td id=\"T_5e087_row21_col0\" class=\"data row21 col0\" >0.699 +/- 0.033</td>\n",
       "      <td id=\"T_5e087_row21_col1\" class=\"data row21 col1\" >0.645 +/- 0.048</td>\n",
       "      <td id=\"T_5e087_row21_col2\" class=\"data row21 col2\" >0.663 +/- 0.047</td>\n",
       "      <td id=\"T_5e087_row21_col3\" class=\"data row21 col3\" >0.57 +/- 0.045</td>\n",
       "      <td id=\"T_5e087_row21_col4\" class=\"data row21 col4\" >0.575 +/- 0.054</td>\n",
       "      <td id=\"T_5e087_row21_col5\" class=\"data row21 col5\" >0.552 +/- 0.033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row22\" class=\"row_heading level0 row22\" >7 AND 1</th>\n",
       "      <td id=\"T_5e087_row22_col0\" class=\"data row22 col0\" >0.715 +/- 0.038</td>\n",
       "      <td id=\"T_5e087_row22_col1\" class=\"data row22 col1\" >0.528 +/- 0.045</td>\n",
       "      <td id=\"T_5e087_row22_col2\" class=\"data row22 col2\" >0.556 +/- 0.041</td>\n",
       "      <td id=\"T_5e087_row22_col3\" class=\"data row22 col3\" >0.577 +/- 0.044</td>\n",
       "      <td id=\"T_5e087_row22_col4\" class=\"data row22 col4\" >0.567 +/- 0.026</td>\n",
       "      <td id=\"T_5e087_row22_col5\" class=\"data row22 col5\" >0.518 +/- 0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row23\" class=\"row_heading level0 row23\" >7 AND 2</th>\n",
       "      <td id=\"T_5e087_row23_col0\" class=\"data row23 col0\" >0.654 +/- 0.03</td>\n",
       "      <td id=\"T_5e087_row23_col1\" class=\"data row23 col1\" >0.565 +/- 0.022</td>\n",
       "      <td id=\"T_5e087_row23_col2\" class=\"data row23 col2\" >0.582 +/- 0.027</td>\n",
       "      <td id=\"T_5e087_row23_col3\" class=\"data row23 col3\" >0.612 +/- 0.045</td>\n",
       "      <td id=\"T_5e087_row23_col4\" class=\"data row23 col4\" >0.512 +/- 0.04</td>\n",
       "      <td id=\"T_5e087_row23_col5\" class=\"data row23 col5\" >0.545 +/- 0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row24\" class=\"row_heading level0 row24\" >7 AND 3</th>\n",
       "      <td id=\"T_5e087_row24_col0\" class=\"data row24 col0\" >0.644 +/- 0.031</td>\n",
       "      <td id=\"T_5e087_row24_col1\" class=\"data row24 col1\" >0.516 +/- 0.061</td>\n",
       "      <td id=\"T_5e087_row24_col2\" class=\"data row24 col2\" >0.509 +/- 0.054</td>\n",
       "      <td id=\"T_5e087_row24_col3\" class=\"data row24 col3\" >0.523 +/- 0.01</td>\n",
       "      <td id=\"T_5e087_row24_col4\" class=\"data row24 col4\" >0.514 +/- 0.02</td>\n",
       "      <td id=\"T_5e087_row24_col5\" class=\"data row24 col5\" >0.511 +/- 0.049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row25\" class=\"row_heading level0 row25\" >7 AND 4</th>\n",
       "      <td id=\"T_5e087_row25_col0\" class=\"data row25 col0\" >0.621 +/- 0.027</td>\n",
       "      <td id=\"T_5e087_row25_col1\" class=\"data row25 col1\" >0.555 +/- 0.045</td>\n",
       "      <td id=\"T_5e087_row25_col2\" class=\"data row25 col2\" >0.565 +/- 0.05</td>\n",
       "      <td id=\"T_5e087_row25_col3\" class=\"data row25 col3\" >0.548 +/- 0.025</td>\n",
       "      <td id=\"T_5e087_row25_col4\" class=\"data row25 col4\" >0.548 +/- 0.053</td>\n",
       "      <td id=\"T_5e087_row25_col5\" class=\"data row25 col5\" >0.518 +/- 0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row26\" class=\"row_heading level0 row26\" >7 AND 5</th>\n",
       "      <td id=\"T_5e087_row26_col0\" class=\"data row26 col0\" >0.676 +/- 0.018</td>\n",
       "      <td id=\"T_5e087_row26_col1\" class=\"data row26 col1\" >0.582 +/- 0.059</td>\n",
       "      <td id=\"T_5e087_row26_col2\" class=\"data row26 col2\" >0.541 +/- 0.041</td>\n",
       "      <td id=\"T_5e087_row26_col3\" class=\"data row26 col3\" >0.546 +/- 0.03</td>\n",
       "      <td id=\"T_5e087_row26_col4\" class=\"data row26 col4\" >0.53 +/- 0.041</td>\n",
       "      <td id=\"T_5e087_row26_col5\" class=\"data row26 col5\" >0.532 +/- 0.056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row27\" class=\"row_heading level0 row27\" >7 AND 6</th>\n",
       "      <td id=\"T_5e087_row27_col0\" class=\"data row27 col0\" >0.643 +/- 0.029</td>\n",
       "      <td id=\"T_5e087_row27_col1\" class=\"data row27 col1\" >0.596 +/- 0.051</td>\n",
       "      <td id=\"T_5e087_row27_col2\" class=\"data row27 col2\" >0.596 +/- 0.048</td>\n",
       "      <td id=\"T_5e087_row27_col3\" class=\"data row27 col3\" >0.623 +/- 0.033</td>\n",
       "      <td id=\"T_5e087_row27_col4\" class=\"data row27 col4\" >0.481 +/- 0.041</td>\n",
       "      <td id=\"T_5e087_row27_col5\" class=\"data row27 col5\" >0.501 +/- 0.031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row28\" class=\"row_heading level0 row28\" >8 AND 0</th>\n",
       "      <td id=\"T_5e087_row28_col0\" class=\"data row28 col0\" >0.661 +/- 0.064</td>\n",
       "      <td id=\"T_5e087_row28_col1\" class=\"data row28 col1\" >0.604 +/- 0.037</td>\n",
       "      <td id=\"T_5e087_row28_col2\" class=\"data row28 col2\" >0.606 +/- 0.048</td>\n",
       "      <td id=\"T_5e087_row28_col3\" class=\"data row28 col3\" >0.584 +/- 0.072</td>\n",
       "      <td id=\"T_5e087_row28_col4\" class=\"data row28 col4\" >0.491 +/- 0.029</td>\n",
       "      <td id=\"T_5e087_row28_col5\" class=\"data row28 col5\" >0.486 +/- 0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row29\" class=\"row_heading level0 row29\" >8 AND 1</th>\n",
       "      <td id=\"T_5e087_row29_col0\" class=\"data row29 col0\" >0.625 +/- 0.02</td>\n",
       "      <td id=\"T_5e087_row29_col1\" class=\"data row29 col1\" >0.585 +/- 0.031</td>\n",
       "      <td id=\"T_5e087_row29_col2\" class=\"data row29 col2\" >0.577 +/- 0.033</td>\n",
       "      <td id=\"T_5e087_row29_col3\" class=\"data row29 col3\" >0.598 +/- 0.055</td>\n",
       "      <td id=\"T_5e087_row29_col4\" class=\"data row29 col4\" >0.567 +/- 0.036</td>\n",
       "      <td id=\"T_5e087_row29_col5\" class=\"data row29 col5\" >0.553 +/- 0.037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row30\" class=\"row_heading level0 row30\" >8 AND 2</th>\n",
       "      <td id=\"T_5e087_row30_col0\" class=\"data row30 col0\" >0.831 +/- 0.037</td>\n",
       "      <td id=\"T_5e087_row30_col1\" class=\"data row30 col1\" >0.721 +/- 0.021</td>\n",
       "      <td id=\"T_5e087_row30_col2\" class=\"data row30 col2\" >0.719 +/- 0.024</td>\n",
       "      <td id=\"T_5e087_row30_col3\" class=\"data row30 col3\" >0.579 +/- 0.073</td>\n",
       "      <td id=\"T_5e087_row30_col4\" class=\"data row30 col4\" >0.496 +/- 0.049</td>\n",
       "      <td id=\"T_5e087_row30_col5\" class=\"data row30 col5\" >0.491 +/- 0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row31\" class=\"row_heading level0 row31\" >8 AND 3</th>\n",
       "      <td id=\"T_5e087_row31_col0\" class=\"data row31 col0\" >0.825 +/- 0.027</td>\n",
       "      <td id=\"T_5e087_row31_col1\" class=\"data row31 col1\" >0.741 +/- 0.042</td>\n",
       "      <td id=\"T_5e087_row31_col2\" class=\"data row31 col2\" >0.723 +/- 0.047</td>\n",
       "      <td id=\"T_5e087_row31_col3\" class=\"data row31 col3\" >0.799 +/- 0.057</td>\n",
       "      <td id=\"T_5e087_row31_col4\" class=\"data row31 col4\" >0.53 +/- 0.052</td>\n",
       "      <td id=\"T_5e087_row31_col5\" class=\"data row31 col5\" >0.534 +/- 0.022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row32\" class=\"row_heading level0 row32\" >8 AND 4</th>\n",
       "      <td id=\"T_5e087_row32_col0\" class=\"data row32 col0\" >0.838 +/- 0.053</td>\n",
       "      <td id=\"T_5e087_row32_col1\" class=\"data row32 col1\" >0.715 +/- 0.057</td>\n",
       "      <td id=\"T_5e087_row32_col2\" class=\"data row32 col2\" >0.734 +/- 0.072</td>\n",
       "      <td id=\"T_5e087_row32_col3\" class=\"data row32 col3\" >0.76 +/- 0.051</td>\n",
       "      <td id=\"T_5e087_row32_col4\" class=\"data row32 col4\" >0.494 +/- 0.038</td>\n",
       "      <td id=\"T_5e087_row32_col5\" class=\"data row32 col5\" >0.559 +/- 0.047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row33\" class=\"row_heading level0 row33\" >8 AND 5</th>\n",
       "      <td id=\"T_5e087_row33_col0\" class=\"data row33 col0\" >0.852 +/- 0.031</td>\n",
       "      <td id=\"T_5e087_row33_col1\" class=\"data row33 col1\" >0.676 +/- 0.026</td>\n",
       "      <td id=\"T_5e087_row33_col2\" class=\"data row33 col2\" >0.678 +/- 0.031</td>\n",
       "      <td id=\"T_5e087_row33_col3\" class=\"data row33 col3\" >0.636 +/- 0.036</td>\n",
       "      <td id=\"T_5e087_row33_col4\" class=\"data row33 col4\" >0.522 +/- 0.034</td>\n",
       "      <td id=\"T_5e087_row33_col5\" class=\"data row33 col5\" >0.515 +/- 0.025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row34\" class=\"row_heading level0 row34\" >8 AND 6</th>\n",
       "      <td id=\"T_5e087_row34_col0\" class=\"data row34 col0\" >0.879 +/- 0.034</td>\n",
       "      <td id=\"T_5e087_row34_col1\" class=\"data row34 col1\" >0.792 +/- 0.053</td>\n",
       "      <td id=\"T_5e087_row34_col2\" class=\"data row34 col2\" >0.785 +/- 0.059</td>\n",
       "      <td id=\"T_5e087_row34_col3\" class=\"data row34 col3\" >0.554 +/- 0.024</td>\n",
       "      <td id=\"T_5e087_row34_col4\" class=\"data row34 col4\" >0.561 +/- 0.054</td>\n",
       "      <td id=\"T_5e087_row34_col5\" class=\"data row34 col5\" >0.516 +/- 0.032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row35\" class=\"row_heading level0 row35\" >8 AND 7</th>\n",
       "      <td id=\"T_5e087_row35_col0\" class=\"data row35 col0\" >0.764 +/- 0.028</td>\n",
       "      <td id=\"T_5e087_row35_col1\" class=\"data row35 col1\" >0.667 +/- 0.026</td>\n",
       "      <td id=\"T_5e087_row35_col2\" class=\"data row35 col2\" >0.672 +/- 0.033</td>\n",
       "      <td id=\"T_5e087_row35_col3\" class=\"data row35 col3\" >0.762 +/- 0.037</td>\n",
       "      <td id=\"T_5e087_row35_col4\" class=\"data row35 col4\" >0.585 +/- 0.019</td>\n",
       "      <td id=\"T_5e087_row35_col5\" class=\"data row35 col5\" >0.513 +/- 0.047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row36\" class=\"row_heading level0 row36\" >9 AND 0</th>\n",
       "      <td id=\"T_5e087_row36_col0\" class=\"data row36 col0\" >0.628 +/- 0.031</td>\n",
       "      <td id=\"T_5e087_row36_col1\" class=\"data row36 col1\" >0.648 +/- 0.032</td>\n",
       "      <td id=\"T_5e087_row36_col2\" class=\"data row36 col2\" >0.661 +/- 0.026</td>\n",
       "      <td id=\"T_5e087_row36_col3\" class=\"data row36 col3\" >0.676 +/- 0.027</td>\n",
       "      <td id=\"T_5e087_row36_col4\" class=\"data row36 col4\" >0.553 +/- 0.051</td>\n",
       "      <td id=\"T_5e087_row36_col5\" class=\"data row36 col5\" >0.534 +/- 0.029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row37\" class=\"row_heading level0 row37\" >9 AND 1</th>\n",
       "      <td id=\"T_5e087_row37_col0\" class=\"data row37 col0\" >0.644 +/- 0.049</td>\n",
       "      <td id=\"T_5e087_row37_col1\" class=\"data row37 col1\" >0.614 +/- 0.074</td>\n",
       "      <td id=\"T_5e087_row37_col2\" class=\"data row37 col2\" >0.614 +/- 0.047</td>\n",
       "      <td id=\"T_5e087_row37_col3\" class=\"data row37 col3\" >0.568 +/- 0.022</td>\n",
       "      <td id=\"T_5e087_row37_col4\" class=\"data row37 col4\" >0.54 +/- 0.019</td>\n",
       "      <td id=\"T_5e087_row37_col5\" class=\"data row37 col5\" >0.52 +/- 0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row38\" class=\"row_heading level0 row38\" >9 AND 2</th>\n",
       "      <td id=\"T_5e087_row38_col0\" class=\"data row38 col0\" >0.667 +/- 0.047</td>\n",
       "      <td id=\"T_5e087_row38_col1\" class=\"data row38 col1\" >0.712 +/- 0.029</td>\n",
       "      <td id=\"T_5e087_row38_col2\" class=\"data row38 col2\" >0.704 +/- 0.046</td>\n",
       "      <td id=\"T_5e087_row38_col3\" class=\"data row38 col3\" >0.607 +/- 0.02</td>\n",
       "      <td id=\"T_5e087_row38_col4\" class=\"data row38 col4\" >0.577 +/- 0.04</td>\n",
       "      <td id=\"T_5e087_row38_col5\" class=\"data row38 col5\" >0.569 +/- 0.041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row39\" class=\"row_heading level0 row39\" >9 AND 3</th>\n",
       "      <td id=\"T_5e087_row39_col0\" class=\"data row39 col0\" >0.755 +/- 0.046</td>\n",
       "      <td id=\"T_5e087_row39_col1\" class=\"data row39 col1\" >0.704 +/- 0.023</td>\n",
       "      <td id=\"T_5e087_row39_col2\" class=\"data row39 col2\" >0.697 +/- 0.025</td>\n",
       "      <td id=\"T_5e087_row39_col3\" class=\"data row39 col3\" >0.527 +/- 0.054</td>\n",
       "      <td id=\"T_5e087_row39_col4\" class=\"data row39 col4\" >0.556 +/- 0.016</td>\n",
       "      <td id=\"T_5e087_row39_col5\" class=\"data row39 col5\" >0.558 +/- 0.039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row40\" class=\"row_heading level0 row40\" >9 AND 4</th>\n",
       "      <td id=\"T_5e087_row40_col0\" class=\"data row40 col0\" >0.698 +/- 0.031</td>\n",
       "      <td id=\"T_5e087_row40_col1\" class=\"data row40 col1\" >0.703 +/- 0.036</td>\n",
       "      <td id=\"T_5e087_row40_col2\" class=\"data row40 col2\" >0.735 +/- 0.032</td>\n",
       "      <td id=\"T_5e087_row40_col3\" class=\"data row40 col3\" >0.703 +/- 0.019</td>\n",
       "      <td id=\"T_5e087_row40_col4\" class=\"data row40 col4\" >0.592 +/- 0.042</td>\n",
       "      <td id=\"T_5e087_row40_col5\" class=\"data row40 col5\" >0.586 +/- 0.048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row41\" class=\"row_heading level0 row41\" >9 AND 5</th>\n",
       "      <td id=\"T_5e087_row41_col0\" class=\"data row41 col0\" >0.716 +/- 0.012</td>\n",
       "      <td id=\"T_5e087_row41_col1\" class=\"data row41 col1\" >0.749 +/- 0.037</td>\n",
       "      <td id=\"T_5e087_row41_col2\" class=\"data row41 col2\" >0.737 +/- 0.041</td>\n",
       "      <td id=\"T_5e087_row41_col3\" class=\"data row41 col3\" >0.629 +/- 0.022</td>\n",
       "      <td id=\"T_5e087_row41_col4\" class=\"data row41 col4\" >0.606 +/- 0.033</td>\n",
       "      <td id=\"T_5e087_row41_col5\" class=\"data row41 col5\" >0.606 +/- 0.041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row42\" class=\"row_heading level0 row42\" >9 AND 6</th>\n",
       "      <td id=\"T_5e087_row42_col0\" class=\"data row42 col0\" >0.698 +/- 0.055</td>\n",
       "      <td id=\"T_5e087_row42_col1\" class=\"data row42 col1\" >0.715 +/- 0.049</td>\n",
       "      <td id=\"T_5e087_row42_col2\" class=\"data row42 col2\" >0.702 +/- 0.035</td>\n",
       "      <td id=\"T_5e087_row42_col3\" class=\"data row42 col3\" >0.615 +/- 0.048</td>\n",
       "      <td id=\"T_5e087_row42_col4\" class=\"data row42 col4\" >0.49 +/- 0.042</td>\n",
       "      <td id=\"T_5e087_row42_col5\" class=\"data row42 col5\" >0.507 +/- 0.033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row43\" class=\"row_heading level0 row43\" >9 AND 7</th>\n",
       "      <td id=\"T_5e087_row43_col0\" class=\"data row43 col0\" >0.706 +/- 0.039</td>\n",
       "      <td id=\"T_5e087_row43_col1\" class=\"data row43 col1\" >0.657 +/- 0.023</td>\n",
       "      <td id=\"T_5e087_row43_col2\" class=\"data row43 col2\" >0.688 +/- 0.018</td>\n",
       "      <td id=\"T_5e087_row43_col3\" class=\"data row43 col3\" >0.681 +/- 0.049</td>\n",
       "      <td id=\"T_5e087_row43_col4\" class=\"data row43 col4\" >0.525 +/- 0.033</td>\n",
       "      <td id=\"T_5e087_row43_col5\" class=\"data row43 col5\" >0.545 +/- 0.037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e087_level0_row44\" class=\"row_heading level0 row44\" >9 AND 8</th>\n",
       "      <td id=\"T_5e087_row44_col0\" class=\"data row44 col0\" >0.611 +/- 0.015</td>\n",
       "      <td id=\"T_5e087_row44_col1\" class=\"data row44 col1\" >0.586 +/- 0.011</td>\n",
       "      <td id=\"T_5e087_row44_col2\" class=\"data row44 col2\" >0.581 +/- 0.029</td>\n",
       "      <td id=\"T_5e087_row44_col3\" class=\"data row44 col3\" >0.579 +/- 0.035</td>\n",
       "      <td id=\"T_5e087_row44_col4\" class=\"data row44 col4\" >0.571 +/- 0.043</td>\n",
       "      <td id=\"T_5e087_row44_col5\" class=\"data row44 col5\" >0.537 +/- 0.044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fb1cc1efeb0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def color_format(s):\n",
    "    original_acc = get_acc(s['no_preprocessing'])\n",
    "    # print(original_acc)\n",
    "    is_max = s == s.max()\n",
    "    color_list = []\n",
    "    for i in s:\n",
    "        # print(f'hi: {i}')\n",
    "        if get_acc(i) <= original_acc:\n",
    "            color_list.append('background-color: white')\n",
    "        else:\n",
    "            color_list.append('background-color: rgb(100,255,100)')\n",
    "    return color_list\n",
    "\n",
    "def get_acc(x):\n",
    "    return float(x.split('+/-')[0].strip())\n",
    "# results_df = pd.DataFrame(results_list)\n",
    "df.set_index(\"name\", inplace=True)\n",
    "df.style.apply(color_format, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc695da5-9427-49ea-91b8-71fa48813daa",
   "metadata": {},
   "source": [
    "### Using OpenAI Clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce843fd1-fa77-4f37-9362-2de0f212f6e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.7/130.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Downloading transformers-4.38.2-py3-none-any.whl (8.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m774.0/774.0 kB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, regex, tokenizers, transformers\n",
      "Successfully installed regex-2023.12.25 safetensors-0.4.2 tokenizers-0.15.2 transformers-4.38.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbed6484-7b09-42df-b866-fcb27b80a2a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edef4688669942f68edc9a7ea0438c62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/4.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f68cde275ad94f419d01a89a0e7f051f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d35c1b793e664924b73285c1d1532b5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca3a9bcd21944cc883b431eff9348545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/862k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca71237a01d247008dcd131664cf83e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce900e22eec24daa841c0983313a18b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c11626bc45f1434c903d6c624e7a546b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2866dc5047a64daf9e00bc50241800fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"zero-shot-image-classification\", model=\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66b5d3df-e8c3-494a-ad4a-ac8b57657ec8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on ZeroShotImageClassificationPipeline in module transformers.pipelines.zero_shot_image_classification object:\n",
      "\n",
      "class ZeroShotImageClassificationPipeline(transformers.pipelines.base.Pipeline)\n",
      " |  ZeroShotImageClassificationPipeline(**kwargs)\n",
      " |  \n",
      " |  Zero shot image classification pipeline using `CLIPModel`. This pipeline predicts the class of an image when you\n",
      " |  provide an image and a set of `candidate_labels`.\n",
      " |  \n",
      " |  Example:\n",
      " |  \n",
      " |  ```python\n",
      " |  >>> from transformers import pipeline\n",
      " |  \n",
      " |  >>> classifier = pipeline(model=\"google/siglip-so400m-patch14-384\")\n",
      " |  >>> classifier(\n",
      " |  ...     \"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\",\n",
      " |  ...     candidate_labels=[\"animals\", \"humans\", \"landscape\"],\n",
      " |  ... )\n",
      " |  [{'score': 0.965, 'label': 'animals'}, {'score': 0.03, 'label': 'humans'}, {'score': 0.005, 'label': 'landscape'}]\n",
      " |  \n",
      " |  >>> classifier(\n",
      " |  ...     \"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\",\n",
      " |  ...     candidate_labels=[\"black and white\", \"photorealist\", \"painting\"],\n",
      " |  ... )\n",
      " |  [{'score': 0.996, 'label': 'black and white'}, {'score': 0.003, 'label': 'photorealist'}, {'score': 0.0, 'label': 'painting'}]\n",
      " |  ```\n",
      " |  \n",
      " |  Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)\n",
      " |  \n",
      " |  This image classification pipeline can currently be loaded from [`pipeline`] using the following task identifier:\n",
      " |  `\"zero-shot-image-classification\"`.\n",
      " |  \n",
      " |  See the list of available models on\n",
      " |  [huggingface.co/models](https://huggingface.co/models?filter=zero-shot-image-classification).\n",
      " |  \n",
      " |  Arguments:\n",
      " |      model ([`PreTrainedModel`] or [`TFPreTrainedModel`]):\n",
      " |          The model that will be used by the pipeline to make predictions. This needs to be a model inheriting from\n",
      " |          [`PreTrainedModel`] for PyTorch and [`TFPreTrainedModel`] for TensorFlow.\n",
      " |      image_processor ([`BaseImageProcessor`]):\n",
      " |          The image processor that will be used by the pipeline to encode data for the model. This object inherits from\n",
      " |          [`BaseImageProcessor`].\n",
      " |      modelcard (`str` or [`ModelCard`], *optional*):\n",
      " |          Model card attributed to the model for this pipeline.\n",
      " |      framework (`str`, *optional*):\n",
      " |          The framework to use, either `\"pt\"` for PyTorch or `\"tf\"` for TensorFlow. The specified framework must be\n",
      " |          installed.\n",
      " |  \n",
      " |          If no framework is specified, will default to the one currently installed. If no framework is specified and\n",
      " |          both frameworks are installed, will default to the framework of the `model`, or to PyTorch if no model is\n",
      " |          provided.\n",
      " |      task (`str`, defaults to `\"\"`):\n",
      " |          A task-identifier for the pipeline.\n",
      " |      num_workers (`int`, *optional*, defaults to 8):\n",
      " |          When the pipeline will use *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number of\n",
      " |          workers to be used.\n",
      " |      batch_size (`int`, *optional*, defaults to 1):\n",
      " |          When the pipeline will use *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of\n",
      " |          the batch to use, for inference this is not always beneficial, please read [Batching with\n",
      " |          pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching) .\n",
      " |      args_parser ([`~pipelines.ArgumentHandler`], *optional*):\n",
      " |          Reference to the object in charge of parsing supplied pipeline parameters.\n",
      " |      device (`int`, *optional*, defaults to -1):\n",
      " |          Device ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model on\n",
      " |          the associated CUDA device id. You can pass native `torch.device` or a `str` too\n",
      " |      torch_dtype (`str` or `torch.dtype`, *optional*):\n",
      " |          Sent directly as `model_kwargs` (just a simpler shortcut) to use the available precision for this model\n",
      " |          (`torch.float16`, `torch.bfloat16`, ... or `\"auto\"`)\n",
      " |      binary_output (`bool`, *optional*, defaults to `False`):\n",
      " |          Flag indicating if the output the pipeline should happen in a serialized format (i.e., pickle) or as\n",
      " |          the raw output data e.g. text.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      ZeroShotImageClassificationPipeline\n",
      " |      transformers.pipelines.base.Pipeline\n",
      " |      transformers.pipelines.base._ScikitCompat\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __call__(self, images: Union[str, List[str], ForwardRef('Image'), List[ForwardRef('Image')]], **kwargs)\n",
      " |      Assign labels to the image(s) passed as inputs.\n",
      " |      \n",
      " |      Args:\n",
      " |          images (`str`, `List[str]`, `PIL.Image` or `List[PIL.Image]`):\n",
      " |              The pipeline handles three types of images:\n",
      " |      \n",
      " |              - A string containing a http link pointing to an image\n",
      " |              - A string containing a local path to an image\n",
      " |              - An image loaded in PIL directly\n",
      " |      \n",
      " |          candidate_labels (`List[str]`):\n",
      " |              The candidate labels for this image\n",
      " |      \n",
      " |          hypothesis_template (`str`, *optional*, defaults to `\"This is a photo of {}\"`):\n",
      " |              The sentence used in cunjunction with *candidate_labels* to attempt the image classification by\n",
      " |              replacing the placeholder with the candidate_labels. Then likelihood is estimated by using\n",
      " |              logits_per_image\n",
      " |      \n",
      " |          timeout (`float`, *optional*, defaults to None):\n",
      " |              The maximum time in seconds to wait for fetching images from the web. If None, no timeout is set and\n",
      " |              the call may block forever.\n",
      " |      \n",
      " |      Return:\n",
      " |          A list of dictionaries containing result, one dictionary per proposed label. The dictionaries contain the\n",
      " |          following keys:\n",
      " |      \n",
      " |          - **label** (`str`) -- The label identified by the model. It is one of the suggested `candidate_label`.\n",
      " |          - **score** (`float`) -- The score attributed by the model for that label (between 0 and 1).\n",
      " |  \n",
      " |  __init__(self, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  postprocess(self, model_outputs)\n",
      " |      Postprocess will receive the raw outputs of the `_forward` method, generally tensors, and reformat them into\n",
      " |      something more friendly. Generally it will output a list or a dict or results (containing just strings and\n",
      " |      numbers).\n",
      " |  \n",
      " |  preprocess(self, image, candidate_labels=None, hypothesis_template='This is a photo of {}.', timeout=None)\n",
      " |      Preprocess will take the `input_` of a specific pipeline and return a dictionary of everything necessary for\n",
      " |      `_forward` to run properly. It should contain at least one tensor, but might have arbitrary other items.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.pipelines.base.Pipeline:\n",
      " |  \n",
      " |  check_model_type(self, supported_models: Union[List[str], dict])\n",
      " |      Check if the model class is in supported by the pipeline.\n",
      " |      \n",
      " |      Args:\n",
      " |          supported_models (`List[str]` or `dict`):\n",
      " |              The list of models supported by the pipeline, or a dictionary with model class values.\n",
      " |  \n",
      " |  device_placement(self)\n",
      " |      Context Manager allowing tensor allocation on the user-specified device in framework agnostic way.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Context manager\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      # Explicitly ask for tensor allocation on CUDA device :0\n",
      " |      pipe = pipeline(..., device=0)\n",
      " |      with pipe.device_placement():\n",
      " |          # Every framework specific tensor allocation will be done on the request device\n",
      " |          output = pipe(...)\n",
      " |      ```\n",
      " |  \n",
      " |  ensure_tensor_on_device(self, **inputs)\n",
      " |      Ensure PyTorch tensors are on the specified device.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs (keyword arguments that should be `torch.Tensor`, the rest is ignored):\n",
      " |              The tensors to place on `self.device`.\n",
      " |          Recursive on lists **only**.\n",
      " |      \n",
      " |      Return:\n",
      " |          `Dict[str, torch.Tensor]`: The same as `inputs` but on the proper device.\n",
      " |  \n",
      " |  forward(self, model_inputs, **forward_params)\n",
      " |  \n",
      " |  get_inference_context(self)\n",
      " |  \n",
      " |  get_iterator(self, inputs, num_workers: int, batch_size: int, preprocess_params, forward_params, postprocess_params)\n",
      " |  \n",
      " |  iterate(self, inputs, preprocess_params, forward_params, postprocess_params)\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Scikit / Keras interface to transformers' pipelines. This method will forward to __call__().\n",
      " |  \n",
      " |  run_multi(self, inputs, preprocess_params, forward_params, postprocess_params)\n",
      " |  \n",
      " |  run_single(self, inputs, preprocess_params, forward_params, postprocess_params)\n",
      " |  \n",
      " |  save_pretrained(self, save_directory: str, safe_serialization: bool = True)\n",
      " |      Save the pipeline's model and tokenizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          save_directory (`str`):\n",
      " |              A path to the directory where to saved. It will be created if it doesn't exist.\n",
      " |          safe_serialization (`str`):\n",
      " |              Whether to save the model using `safetensors` or the traditional way for PyTorch or Tensorflow.\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Scikit / Keras interface to transformers' pipelines. This method will forward to __call__().\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from transformers.pipelines.base.Pipeline:\n",
      " |  \n",
      " |  default_input_names = None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from transformers.pipelines.base._ScikitCompat:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a0453cfe-b4b1-4044-b493-2e7d80d190ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'airplane', 'score': 0.9744154214859009},\n",
      " {'label': 'bird', 'score': 0.012951435521245003},\n",
      " {'label': 'ship', 'score': 0.008248009718954563},\n",
      " {'label': 'automobile', 'score': 0.0018931111553683877},\n",
      " {'label': 'dog', 'score': 0.000702335499227047},\n",
      " {'label': 'truck', 'score': 0.0006072412943467498},\n",
      " {'label': 'cat', 'score': 0.0004406006191857159},\n",
      " {'label': 'deer', 'score': 0.00033803354017436504},\n",
      " {'label': 'horse', 'score': 0.00031132795265875757},\n",
      " {'label': 'frog', 'score': 9.248455171473324e-05}]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "sample_pred = pipe(cifar10_ds['train'][0]['img'], candidate_labels=['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'])\n",
    "pprint(sample_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0edc22aa-eb04-473b-b6c1-d5ec87ad6485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=32x32 at 0x7F0350D42E00>\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(cifar10_ds['train'][0]['img'])\n",
    "print(cifar10_ds['train'][0]['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04bcc1f-5878-43d7-9108-3be412d39a25",
   "metadata": {},
   "source": [
    "### Binary Classification Using Clip\n",
    "+ Target dataset is either label 0 (airplane) or 1 (automobile)\n",
    "+ For the background dataset, we want to only use instances where the score for label 0 or 1 is larger than a threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a6a39153-aa0a-403f-adeb-84a242099297",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "\n",
    "inputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True)\n",
    "output = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "247f1f16-3d21-4a8d-9f66-b8db81d000be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[49406,   320,  1125,   539,   320,  2368, 49407],\n",
       "        [49406,   320,  1125,   539,   320,  1929, 49407]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ee830f1f-ea1f-4774-b1f7-df394148aa27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits_per_image', 'logits_per_text', 'text_embeds', 'image_embeds', 'text_model_output', 'vision_model_output'])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "18b6b1dd-43de-44da-aa69-608e507812d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPooling(last_hidden_state=tensor([[[-0.1391,  0.1016, -0.2362,  ...,  0.1568,  0.2143,  0.2181],\n",
       "         [ 0.1976,  0.4183, -0.8494,  ...,  0.1030,  1.4616,  0.4321],\n",
       "         [ 0.1630,  0.2004, -1.0608,  ..., -0.0640,  1.3778,  0.5354],\n",
       "         ...,\n",
       "         [ 0.0552, -0.0113, -0.6040,  ...,  0.2935,  1.0303, -0.0316],\n",
       "         [ 0.1673,  0.0491, -0.3833,  ...,  0.3797,  0.9275, -0.0212],\n",
       "         [ 0.1937,  0.2567, -0.4325,  ...,  0.2957,  1.2151, -0.0417]]],\n",
       "       grad_fn=<AddBackward0>), pooler_output=tensor([[-3.5264e-01,  4.3304e-02, -7.5685e-01, -8.6568e-01,  5.1499e-01,\n",
       "         -8.6238e-01,  3.0487e-01,  1.0063e+00,  4.8464e-01,  4.2416e-01,\n",
       "          2.1265e+00, -6.3059e-01, -2.0583e+00,  4.7908e-02,  2.4651e-01,\n",
       "          1.5391e-01,  1.1603e+00,  9.8606e-01,  1.1290e+00, -6.3667e-01,\n",
       "          3.4923e-01,  9.5762e-01,  3.4354e-01,  1.7032e-01,  2.7315e-01,\n",
       "         -1.5358e-01,  3.5460e-01, -3.6673e-01,  2.2665e-01,  1.4825e+00,\n",
       "         -5.5609e+00,  4.7867e-01, -3.6525e-02,  6.9109e-02,  9.1081e-01,\n",
       "          9.7474e-01,  1.3259e+00, -1.4460e-01, -1.0853e+00, -3.7624e-01,\n",
       "          3.5872e-02,  2.3670e+00,  1.3561e-02, -1.2212e-01, -1.5952e+00,\n",
       "          8.5968e-01,  8.5326e-01, -4.8963e-01, -1.0861e+00,  3.1368e-01,\n",
       "          2.6086e-01,  9.9086e-01,  4.4735e-02, -6.0607e-01, -2.5588e-02,\n",
       "         -2.5954e-01,  2.6184e-01,  3.6858e-01,  5.3657e-01, -4.7886e-01,\n",
       "         -9.9796e-01, -5.2461e-01,  7.5558e-01, -8.6403e-01, -8.2988e-02,\n",
       "         -1.2506e-01, -6.2009e-01, -3.4873e-01,  2.8018e-01, -6.4279e-02,\n",
       "          5.8928e-01,  9.2975e-01,  2.3892e-02, -1.7725e+00,  6.8918e-01,\n",
       "          2.3316e-01,  4.8994e-01, -6.8690e-03, -3.9914e-01, -5.4403e-01,\n",
       "          1.2570e+00, -1.6068e-01,  1.0599e+00,  1.1801e+00,  2.0259e-01,\n",
       "          6.1690e-02,  9.2588e-01,  2.9386e-01,  1.1666e-02,  2.0448e-01,\n",
       "          4.6831e-01,  2.3376e-01,  2.5640e+00,  6.9256e-01,  3.3700e-01,\n",
       "          5.3429e-01,  3.2651e-01, -9.4718e-01, -7.1590e-02, -3.3778e-01,\n",
       "         -1.0915e+00,  1.0146e+00,  4.8727e-01,  1.2883e-01,  1.5267e+00,\n",
       "          1.2702e+00,  1.0522e-01,  6.1814e-01,  1.2620e+00, -9.7498e-01,\n",
       "         -8.3319e-01,  1.0517e+00,  1.0112e+00,  5.0059e-01,  6.3184e-01,\n",
       "         -6.1085e-01,  9.8586e-01, -4.7894e+00,  7.0911e-01,  1.2716e+00,\n",
       "          9.4411e-03, -1.3824e+00,  1.3820e+00, -9.4865e-01,  1.3560e+00,\n",
       "         -3.0087e-02,  3.3432e-01,  2.8334e-01, -2.1543e+00, -2.9065e-01,\n",
       "          6.9320e-01,  5.6451e-01,  7.8557e-01,  5.6714e-01,  6.4115e-01,\n",
       "         -4.0370e-01,  9.2214e-01,  3.5810e-01,  4.1939e-02, -5.5594e-01,\n",
       "          7.5722e-01,  2.3432e-01,  5.0104e-01,  5.5496e-02,  2.0550e-01,\n",
       "         -2.1337e-01, -1.9733e-01,  1.1409e+00,  6.9875e-01,  1.7283e+00,\n",
       "         -7.6343e-01, -3.2098e-02, -3.1697e-01, -7.2258e-01,  5.8814e-01,\n",
       "          4.9162e-01,  1.3317e+00, -5.7261e-01,  5.1979e-02,  2.5553e+00,\n",
       "          8.2094e-02,  1.5233e+00, -8.4682e-01, -3.1632e-01,  3.1814e-01,\n",
       "         -1.2002e-02, -4.0892e-01,  2.8272e+00,  9.8166e-01,  1.8623e-03,\n",
       "         -1.1622e+00,  1.7501e-01,  3.4614e-01,  9.1047e-02,  9.7296e-01,\n",
       "          1.0803e-01,  8.7221e-01,  1.5142e+00,  5.8995e-01, -2.5457e-01,\n",
       "          9.0521e-01, -7.8983e-01, -1.6377e+00, -2.0730e+00, -1.4363e-01,\n",
       "          4.6563e-01, -7.8027e-02,  3.7469e-01,  7.6621e-01,  2.4610e-01,\n",
       "          9.8579e-01,  1.2886e+00, -9.8026e-01,  1.2338e+00, -8.0253e-01,\n",
       "          1.0827e-01,  6.4560e-01, -3.6977e-01,  3.5115e-01, -1.9270e-02,\n",
       "          1.8087e+00, -1.1130e+00, -1.0118e+00, -3.1947e-01,  1.2213e-01,\n",
       "         -3.3319e-01, -7.6511e-01, -1.3321e+00,  4.4757e-01, -3.6747e-01,\n",
       "          1.1015e+00,  2.0999e-01,  1.2471e-01, -1.3555e-01, -3.2647e-01,\n",
       "         -5.1078e-01,  1.7271e+00, -1.1488e-01,  1.4335e+00,  2.5693e+00,\n",
       "         -1.3075e+00, -4.7947e-02,  4.6487e-02,  6.6219e-01, -3.8170e-01,\n",
       "          6.4457e-01,  1.4556e+00, -5.9513e-01, -2.5588e-01, -5.0467e-01,\n",
       "         -7.0425e-01, -3.8041e-01,  3.7561e-01, -6.9994e-01, -9.1622e-01,\n",
       "         -1.2066e+00,  1.0132e+00, -1.8947e-01,  1.4076e+00,  3.1970e-01,\n",
       "         -5.2296e-01, -3.1013e-01,  1.1451e+00,  2.1767e-01, -5.1022e-01,\n",
       "          3.6925e-01,  4.8536e-01,  9.7659e-01, -5.2282e-01, -8.7919e-01,\n",
       "         -1.8793e-01,  3.1892e-01, -1.0917e+00,  7.9159e-01, -5.4996e-02,\n",
       "         -1.3775e+00,  8.3843e-01,  1.3465e+00,  1.7894e+00,  3.6971e-02,\n",
       "         -6.7165e-02,  3.2308e-01,  9.1129e-02, -3.3263e-01,  2.9222e-02,\n",
       "          6.2664e-01,  6.3750e-01,  6.3640e-01,  1.4893e+00, -9.3224e-01,\n",
       "          9.9525e-01,  1.9778e+00,  1.1538e+00,  5.7606e-01, -4.5982e-01,\n",
       "         -7.5805e-01,  4.8339e-01, -2.5749e-01,  4.2240e-01, -3.2489e-01,\n",
       "         -1.6969e-01,  1.3746e-01, -9.2024e-01,  1.8619e-03,  1.6877e+00,\n",
       "          1.3015e+00,  6.9480e-01,  6.4740e-01, -1.5567e-02,  3.2907e-01,\n",
       "          3.4862e-01, -1.2985e-01, -4.1677e-01,  7.7095e-02,  2.3429e-01,\n",
       "         -2.8761e-01,  9.5210e-01, -1.0645e-01,  1.8884e-01,  8.5905e-01,\n",
       "          2.1797e-02,  1.9875e+00,  1.3699e+00, -4.0271e-01, -1.5127e-01,\n",
       "          1.5301e+00, -4.9888e-01, -4.6305e-02, -2.3626e+00,  2.0374e+00,\n",
       "          8.4862e-01,  3.5139e-01, -3.7457e-01,  1.6747e+00,  1.1257e+00,\n",
       "          1.8909e-01, -8.5011e-02,  5.7352e-01,  3.5760e-01, -7.8200e-01,\n",
       "         -3.6456e-01,  1.3278e+00,  3.8621e-01,  1.7694e+00, -9.9532e-01,\n",
       "         -7.6960e-01,  2.0353e-01,  8.4319e-01,  5.7375e-01, -1.2118e+00,\n",
       "         -5.8213e-01,  2.2399e-01,  3.1518e-01,  7.8852e-01, -3.0443e-01,\n",
       "          9.1613e-01, -8.4484e+00,  8.0768e-01, -3.1251e-01, -5.2107e-01,\n",
       "          2.2688e+00, -1.0966e+00, -4.2557e-01,  8.1576e-01,  3.9901e-02,\n",
       "          5.6879e-01, -3.4454e-01,  4.9169e-01, -3.7721e-01,  1.9447e+00,\n",
       "         -2.7399e+00, -5.0647e-01, -6.4821e-01, -4.6581e-01, -2.3602e-01,\n",
       "         -4.2327e-01,  2.6264e-01,  1.0275e-01,  5.7667e-01, -3.7875e-01,\n",
       "         -5.4397e-01,  1.5267e+00,  1.6838e+00, -3.2419e-01,  8.0147e-01,\n",
       "         -1.5630e+00, -1.2249e+00,  5.5589e-01,  7.7242e-01,  6.0412e-01,\n",
       "          2.8980e-02, -4.1701e-01,  1.3650e+00,  2.2362e+00,  1.8796e-01,\n",
       "          3.4491e-01, -5.1059e-01, -4.9391e-01,  3.6328e-01,  2.0818e-01,\n",
       "          1.1217e+00, -2.6375e-02,  1.1703e+00, -5.1401e-01,  4.4867e-01,\n",
       "         -1.5572e+00, -9.0921e-01, -3.0844e-01,  3.0260e-01, -3.3211e-02,\n",
       "         -1.7382e-01, -1.1408e+00, -1.6635e-01, -1.3524e+00, -1.8079e-01,\n",
       "          5.8820e-01,  1.7260e+00,  8.5461e-01,  4.3257e-01,  2.5352e-01,\n",
       "          1.0146e+00,  3.3303e-01, -2.0897e+00, -5.4013e-01, -1.5557e-01,\n",
       "         -6.6893e-01,  1.9901e-01,  6.1228e-01,  1.3019e+00,  7.2215e-01,\n",
       "          6.3090e-01, -9.8380e-01, -1.3003e+00, -7.9498e-01,  1.1795e-01,\n",
       "          1.8986e+00,  4.5840e-01,  1.1443e+00, -6.3730e-01, -4.1015e-01,\n",
       "          3.4007e-02,  1.0921e+00,  1.5790e+00,  1.2586e+00, -3.5369e-01,\n",
       "          8.9323e-01, -8.6981e-02,  8.8079e-01,  4.3065e-01,  6.3814e-02,\n",
       "          8.2852e-02, -4.0830e-01,  6.6507e-01,  3.2531e-01, -5.2296e-01,\n",
       "          1.3427e-01,  9.1604e-01,  5.2759e-01,  9.0018e-01,  8.5370e-01,\n",
       "         -8.4900e-01, -4.4072e-01,  4.6849e-01,  1.2493e+00,  7.8004e-01,\n",
       "          1.6397e+00,  6.5921e-01, -2.1974e-01,  6.1917e-01,  2.0731e-01,\n",
       "         -1.7788e-01, -1.4708e-01,  1.3430e+00,  8.6353e-01, -5.2284e-01,\n",
       "          9.7586e-01,  1.9234e-01, -6.5110e-01,  4.0379e-01,  8.0056e-01,\n",
       "          1.7967e+00, -1.7286e-01,  8.9490e-01,  2.5221e+00,  9.0101e-01,\n",
       "         -9.3334e-01,  2.7572e-01, -1.0808e+00,  1.6058e-01,  1.0822e+00,\n",
       "          1.0253e-01,  4.5653e-01,  9.3660e-02,  6.7689e-01, -9.1576e-01,\n",
       "         -3.6836e-02,  1.1317e+00, -5.4547e-01, -4.2518e-01, -3.2765e-01,\n",
       "          3.4597e-01,  2.4114e-01,  5.9621e-01, -9.1706e-01,  2.7427e+00,\n",
       "          7.0460e-01,  7.9421e-01, -4.0319e-02,  3.0880e-01, -9.0315e-02,\n",
       "         -1.4865e+00, -6.1038e-01,  2.7091e-01, -2.0323e+00,  4.2787e-01,\n",
       "          6.9593e-02, -2.8328e-01,  5.3013e-01,  1.0530e+00,  1.5350e+00,\n",
       "         -5.3865e-01,  5.6356e-01,  1.6915e+00,  1.1779e+00,  1.1519e+00,\n",
       "         -2.3383e-01,  1.7798e-01,  2.3030e-01,  3.2136e-01, -2.3101e-01,\n",
       "          3.6188e-01,  6.9796e-01,  4.3757e-01,  1.1945e+00,  4.1193e-01,\n",
       "          5.6153e-01, -9.0861e-01,  6.1336e-01,  7.2461e-01,  6.0849e-01,\n",
       "          9.6069e-01,  5.3904e-01, -5.8923e-01,  3.8153e-01,  6.4641e-01,\n",
       "          1.8049e-01, -2.9474e-01,  1.0262e-01,  1.0224e+00,  1.7094e-01,\n",
       "          1.6581e+00,  4.4954e-01, -3.4850e-01, -1.0262e-01,  9.1800e-01,\n",
       "          4.3916e-01,  9.7942e-01,  1.3337e+00,  7.1814e-01,  8.1293e-01,\n",
       "          1.0831e+00, -1.5260e-01, -1.2266e+00, -5.7365e-01,  1.1101e+00,\n",
       "          6.9343e-01, -6.3199e-01,  7.6481e-01, -1.8917e-01, -4.7711e-01,\n",
       "         -1.3847e+00,  4.2010e-01,  5.7761e-01,  2.5993e-01, -4.4268e-01,\n",
       "         -3.6427e-01, -7.2298e-01, -3.3101e-01, -1.0469e+00, -6.8637e-01,\n",
       "          1.4998e+00, -3.4543e-01, -3.2169e-01, -1.6048e+00, -4.2532e-02,\n",
       "          4.8982e-01,  7.3723e-02,  2.4397e+00, -1.4931e-01,  5.9426e-02,\n",
       "          1.5988e+00, -2.6175e-01,  8.3588e-01,  1.0288e+00,  3.8859e-01,\n",
       "          6.3030e-01,  6.6436e-01, -2.1051e+00,  5.5732e-01,  6.4834e-01,\n",
       "         -2.4150e-01,  5.6953e-01, -3.1635e-01,  3.0064e-01, -1.7850e-01,\n",
       "         -1.4502e-01, -1.3084e+00,  8.6729e-02,  3.6628e-01,  1.0513e+00,\n",
       "          7.1542e-01, -1.7165e+00, -2.6288e-01, -6.2765e-01, -1.3292e+00,\n",
       "          1.3024e-01,  9.9063e-01, -2.3000e-01,  1.9100e-01,  8.9494e-01,\n",
       "         -9.3877e-01,  9.8972e-01, -1.0331e+00, -1.3647e+00,  9.7453e-01,\n",
       "          1.9128e-01, -3.5180e-02,  5.5247e-01, -7.7066e-01,  5.3998e-02,\n",
       "         -6.7170e-01,  1.3228e+00,  2.0382e-01, -3.3264e-01, -1.1146e+00,\n",
       "          1.2722e-01, -7.5402e-01,  1.4116e+00,  1.6019e-01, -3.0468e-01,\n",
       "         -3.7354e-01, -1.5376e-02, -1.0627e-01,  4.2561e-01, -1.8824e+00,\n",
       "          1.4664e+00,  3.6314e-02,  1.2299e+00, -5.6897e-02,  9.6516e-01,\n",
       "          5.5794e-01,  9.2031e-01,  1.2710e+00,  9.0449e-01,  4.6179e-01,\n",
       "          8.4392e-01,  3.5557e-01, -3.6685e-01,  9.5495e-01, -3.6487e-01,\n",
       "          3.0229e-02,  8.8992e-01,  3.4275e-01, -4.1095e-01, -6.6879e-01,\n",
       "          7.2809e-01, -9.9564e-01, -1.8790e-01, -8.4203e-01, -5.1992e-01,\n",
       "         -1.5432e+00,  5.0339e-01,  4.7730e-01,  7.9181e-01, -1.1965e-01,\n",
       "          1.2073e-01,  6.3031e-01, -2.5428e-01,  8.5587e-03,  1.4234e+00,\n",
       "          1.1224e+00, -9.4434e-01,  6.9661e-01, -1.9215e+00,  9.7057e-01,\n",
       "          9.7977e-01,  1.0569e+00,  1.5133e+00,  1.9372e+00,  1.7260e+00,\n",
       "          1.0939e+00,  4.9478e-01, -9.8562e-02,  5.7189e-01, -1.8576e-01,\n",
       "          1.0061e+00,  2.7609e-02,  2.8225e-01,  7.0827e-01,  1.7172e-02,\n",
       "         -1.1758e-01,  1.1422e+00, -5.4135e-01,  4.5000e-01,  1.3905e+00,\n",
       "          1.4221e-01,  3.4242e-01, -1.1105e+00,  1.2511e-01, -9.2104e-01,\n",
       "         -1.6576e-02, -9.1303e-01,  1.8744e-01,  1.3073e+00,  1.0134e+00,\n",
       "          2.3634e-01,  2.2447e-02, -5.9267e-01,  9.1468e-01, -1.8535e+00,\n",
       "         -7.3108e-01,  6.0575e-01,  2.8953e-01,  6.4549e-01, -7.4716e-02,\n",
       "         -2.9338e-01,  1.8899e+00,  1.2505e-01,  6.1557e-01,  1.5365e-01,\n",
       "          1.0182e+00,  1.1015e+00, -5.8102e-01, -2.0597e-01,  8.3999e-01,\n",
       "          1.5381e+00, -1.7670e+00,  3.4802e-01,  1.5627e+00, -1.5446e+00,\n",
       "          1.0482e+00,  1.4200e+00,  2.1023e+00,  2.1359e-01, -1.5668e-01,\n",
       "         -1.2933e+00,  7.9156e-01, -5.6790e-01,  5.2077e-01,  3.7834e-01,\n",
       "         -4.0609e-02,  3.6113e-01, -3.2922e-01,  1.2945e+00, -4.3657e-01,\n",
       "          1.4449e+00,  6.0915e-01,  2.2024e+00, -1.6672e-01, -1.1214e-01,\n",
       "         -3.2493e-01, -1.6206e-01,  8.0187e-01,  8.0760e-01, -8.3740e-01,\n",
       "          1.1507e+00,  1.0677e+00,  6.2035e-01, -5.0882e-01, -1.7383e+00,\n",
       "          1.5221e+00,  1.0109e-01,  7.6414e-01,  1.0567e+00, -5.6092e-02,\n",
       "         -9.5583e-02, -4.3652e-03, -5.1400e-01,  1.9584e+00, -1.1616e+00,\n",
       "          2.8383e+00,  8.7326e-01,  1.2737e+00, -2.4023e-01,  8.4994e-01,\n",
       "          6.0325e-01,  1.0523e+00,  1.0862e+00]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['vision_model_output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3643f89c-ae6a-43f3-8388-d52579bff497",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:41<00:00,  2.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22, 3072)\n",
      "(22,)\n",
      "(62, 3072)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "X_target = []\n",
    "X_background = []\n",
    "y_target = []\n",
    "y_background = []\n",
    "\n",
    "THRESHOLD = 1e-2\n",
    "\n",
    "for i in trange(100):\n",
    "    if cifar10_ds['train'][i]['label'] in [0,1]:\n",
    "        X_target.append(np.asarray(cifar10_ds['train'][i]['img']).flatten())\n",
    "        y_target.append(cifar10_ds['train'][i]['label'])\n",
    "    else:\n",
    "        sample_pred = pipe(cifar10_ds['train'][i]['img'], candidate_labels=['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'])\n",
    "        for tmp_dict in sample_pred:\n",
    "            if tmp_dict['label'] in ['airplane', 'automobile'] and tmp_dict['score'] > THRESHOLD:\n",
    "                X_background.append(np.asarray(cifar10_ds['train'][i]['img']).flatten())\n",
    "\n",
    "X_target = np.asarray(X_target)\n",
    "y_target = np.asarray(y_target)\n",
    "X_background = np.asarray(X_background)\n",
    "\n",
    "print(X_target.shape)\n",
    "print(y_target.shape)\n",
    "print(X_background.shape)\n",
    "\n",
    "# TODO: save clip embeddings as a file (i.e. pickle or .pt, and then upload + cos similarity) // ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "90733292-e842-4863-b6bf-c9e2707fc45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 22, 2)\n"
     ]
    }
   ],
   "source": [
    "from contrastive import CPCA\n",
    "\n",
    "cpca = CPCA()\n",
    "X_cpca = cpca.fit_transform(X_target[:,0:1000], X_background[:,0:1000])\n",
    "X_cpca = np.asarray(X_cpca)\n",
    "print(X_cpca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "92f96d9f-b02d-4055-a190-95b0d35e884f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27\n",
      "0.27\n",
      "0.27\n",
      "0.27\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "for i in range(4):\n",
    "    knn = KNeighborsClassifier()\n",
    "    scores = cross_val_score(knn, X_target, y_target, cv=5)\n",
    "    print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d05029f-bffd-4aea-89eb-b3e84083d52b",
   "metadata": {},
   "source": [
    "### Using Google ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2efd7a75-e363-40f2-8b92-e9139e3c21b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_vit_pipe = pipeline(\"image-classification\", model=\"google/vit-base-patch16-224\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "811221d4-9851-4bc4-9b91-7be2ee854556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on ImageClassificationPipeline in module transformers.pipelines.image_classification object:\n",
      "\n",
      "class ImageClassificationPipeline(transformers.pipelines.base.Pipeline)\n",
      " |  ImageClassificationPipeline(*args, **kwargs)\n",
      " |  \n",
      " |  Image classification pipeline using any `AutoModelForImageClassification`. This pipeline predicts the class of an\n",
      " |  image.\n",
      " |  \n",
      " |  Example:\n",
      " |  \n",
      " |  ```python\n",
      " |  >>> from transformers import pipeline\n",
      " |  \n",
      " |  >>> classifier = pipeline(model=\"microsoft/beit-base-patch16-224-pt22k-ft22k\")\n",
      " |  >>> classifier(\"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\")\n",
      " |  [{'score': 0.442, 'label': 'macaw'}, {'score': 0.088, 'label': 'popinjay'}, {'score': 0.075, 'label': 'parrot'}, {'score': 0.073, 'label': 'parodist, lampooner'}, {'score': 0.046, 'label': 'poll, poll_parrot'}]\n",
      " |  ```\n",
      " |  \n",
      " |  Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)\n",
      " |  \n",
      " |  This image classification pipeline can currently be loaded from [`pipeline`] using the following task identifier:\n",
      " |  `\"image-classification\"`.\n",
      " |  \n",
      " |  See the list of available models on\n",
      " |  [huggingface.co/models](https://huggingface.co/models?filter=image-classification).\n",
      " |  \n",
      " |  Arguments:\n",
      " |      model ([`PreTrainedModel`] or [`TFPreTrainedModel`]):\n",
      " |          The model that will be used by the pipeline to make predictions. This needs to be a model inheriting from\n",
      " |          [`PreTrainedModel`] for PyTorch and [`TFPreTrainedModel`] for TensorFlow.\n",
      " |      image_processor ([`BaseImageProcessor`]):\n",
      " |          The image processor that will be used by the pipeline to encode data for the model. This object inherits from\n",
      " |          [`BaseImageProcessor`].\n",
      " |      modelcard (`str` or [`ModelCard`], *optional*):\n",
      " |          Model card attributed to the model for this pipeline.\n",
      " |      framework (`str`, *optional*):\n",
      " |          The framework to use, either `\"pt\"` for PyTorch or `\"tf\"` for TensorFlow. The specified framework must be\n",
      " |          installed.\n",
      " |  \n",
      " |          If no framework is specified, will default to the one currently installed. If no framework is specified and\n",
      " |          both frameworks are installed, will default to the framework of the `model`, or to PyTorch if no model is\n",
      " |          provided.\n",
      " |      task (`str`, defaults to `\"\"`):\n",
      " |          A task-identifier for the pipeline.\n",
      " |      num_workers (`int`, *optional*, defaults to 8):\n",
      " |          When the pipeline will use *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number of\n",
      " |          workers to be used.\n",
      " |      batch_size (`int`, *optional*, defaults to 1):\n",
      " |          When the pipeline will use *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of\n",
      " |          the batch to use, for inference this is not always beneficial, please read [Batching with\n",
      " |          pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching) .\n",
      " |      args_parser ([`~pipelines.ArgumentHandler`], *optional*):\n",
      " |          Reference to the object in charge of parsing supplied pipeline parameters.\n",
      " |      device (`int`, *optional*, defaults to -1):\n",
      " |          Device ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model on\n",
      " |          the associated CUDA device id. You can pass native `torch.device` or a `str` too\n",
      " |      torch_dtype (`str` or `torch.dtype`, *optional*):\n",
      " |          Sent directly as `model_kwargs` (just a simpler shortcut) to use the available precision for this model\n",
      " |          (`torch.float16`, `torch.bfloat16`, ... or `\"auto\"`)\n",
      " |      binary_output (`bool`, *optional*, defaults to `False`):\n",
      " |          Flag indicating if the output the pipeline should happen in a serialized format (i.e., pickle) or as\n",
      " |          the raw output data e.g. text.\n",
      " |      function_to_apply (`str`, *optional*, defaults to `\"default\"`):\n",
      " |          The function to apply to the model outputs in order to retrieve the scores. Accepts four different values:\n",
      " |  \n",
      " |          - `\"default\"`: if the model has a single label, will apply the sigmoid function on the output. If the model\n",
      " |            has several labels, will apply the softmax function on the output.\n",
      " |          - `\"sigmoid\"`: Applies the sigmoid function on the output.\n",
      " |          - `\"softmax\"`: Applies the softmax function on the output.\n",
      " |          - `\"none\"`: Does not apply any function on the output.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      ImageClassificationPipeline\n",
      " |      transformers.pipelines.base.Pipeline\n",
      " |      transformers.pipelines.base._ScikitCompat\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __call__(self, images: Union[str, List[str], ForwardRef('Image.Image'), List[ForwardRef('Image.Image')]], **kwargs)\n",
      " |      Assign labels to the image(s) passed as inputs.\n",
      " |      \n",
      " |      Args:\n",
      " |          images (`str`, `List[str]`, `PIL.Image` or `List[PIL.Image]`):\n",
      " |              The pipeline handles three types of images:\n",
      " |      \n",
      " |              - A string containing a http link pointing to an image\n",
      " |              - A string containing a local path to an image\n",
      " |              - An image loaded in PIL directly\n",
      " |      \n",
      " |              The pipeline accepts either a single image or a batch of images, which must then be passed as a string.\n",
      " |              Images in a batch must all be in the same format: all as http links, all as local paths, or all as PIL\n",
      " |              images.\n",
      " |          function_to_apply (`str`, *optional*, defaults to `\"default\"`):\n",
      " |              The function to apply to the model outputs in order to retrieve the scores. Accepts four different\n",
      " |              values:\n",
      " |      \n",
      " |              If this argument is not specified, then it will apply the following functions according to the number\n",
      " |              of labels:\n",
      " |      \n",
      " |              - If the model has a single label, will apply the sigmoid function on the output.\n",
      " |              - If the model has several labels, will apply the softmax function on the output.\n",
      " |      \n",
      " |              Possible values are:\n",
      " |      \n",
      " |              - `\"sigmoid\"`: Applies the sigmoid function on the output.\n",
      " |              - `\"softmax\"`: Applies the softmax function on the output.\n",
      " |              - `\"none\"`: Does not apply any function on the output.\n",
      " |          top_k (`int`, *optional*, defaults to 5):\n",
      " |              The number of top labels that will be returned by the pipeline. If the provided number is higher than\n",
      " |              the number of labels available in the model configuration, it will default to the number of labels.\n",
      " |          timeout (`float`, *optional*, defaults to None):\n",
      " |              The maximum time in seconds to wait for fetching images from the web. If None, no timeout is set and\n",
      " |              the call may block forever.\n",
      " |      \n",
      " |      Return:\n",
      " |          A dictionary or a list of dictionaries containing result. If the input is a single image, will return a\n",
      " |          dictionary, if the input is a list of several images, will return a list of dictionaries corresponding to\n",
      " |          the images.\n",
      " |      \n",
      " |          The dictionaries contain the following keys:\n",
      " |      \n",
      " |          - **label** (`str`) -- The label identified by the model.\n",
      " |          - **score** (`int`) -- The score attributed by the model for that label.\n",
      " |  \n",
      " |  __init__(self, *args, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  postprocess(self, model_outputs, function_to_apply=None, top_k=5)\n",
      " |      Postprocess will receive the raw outputs of the `_forward` method, generally tensors, and reformat them into\n",
      " |      something more friendly. Generally it will output a list or a dict or results (containing just strings and\n",
      " |      numbers).\n",
      " |  \n",
      " |  preprocess(self, image, timeout=None)\n",
      " |      Preprocess will take the `input_` of a specific pipeline and return a dictionary of everything necessary for\n",
      " |      `_forward` to run properly. It should contain at least one tensor, but might have arbitrary other items.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {'function_to_apply': <enum 'ClassificationFunction'...\n",
      " |  \n",
      " |  function_to_apply = <ClassificationFunction.NONE: 'none'>\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.pipelines.base.Pipeline:\n",
      " |  \n",
      " |  check_model_type(self, supported_models: Union[List[str], dict])\n",
      " |      Check if the model class is in supported by the pipeline.\n",
      " |      \n",
      " |      Args:\n",
      " |          supported_models (`List[str]` or `dict`):\n",
      " |              The list of models supported by the pipeline, or a dictionary with model class values.\n",
      " |  \n",
      " |  device_placement(self)\n",
      " |      Context Manager allowing tensor allocation on the user-specified device in framework agnostic way.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Context manager\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      # Explicitly ask for tensor allocation on CUDA device :0\n",
      " |      pipe = pipeline(..., device=0)\n",
      " |      with pipe.device_placement():\n",
      " |          # Every framework specific tensor allocation will be done on the request device\n",
      " |          output = pipe(...)\n",
      " |      ```\n",
      " |  \n",
      " |  ensure_tensor_on_device(self, **inputs)\n",
      " |      Ensure PyTorch tensors are on the specified device.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs (keyword arguments that should be `torch.Tensor`, the rest is ignored):\n",
      " |              The tensors to place on `self.device`.\n",
      " |          Recursive on lists **only**.\n",
      " |      \n",
      " |      Return:\n",
      " |          `Dict[str, torch.Tensor]`: The same as `inputs` but on the proper device.\n",
      " |  \n",
      " |  forward(self, model_inputs, **forward_params)\n",
      " |  \n",
      " |  get_inference_context(self)\n",
      " |  \n",
      " |  get_iterator(self, inputs, num_workers: int, batch_size: int, preprocess_params, forward_params, postprocess_params)\n",
      " |  \n",
      " |  iterate(self, inputs, preprocess_params, forward_params, postprocess_params)\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Scikit / Keras interface to transformers' pipelines. This method will forward to __call__().\n",
      " |  \n",
      " |  run_multi(self, inputs, preprocess_params, forward_params, postprocess_params)\n",
      " |  \n",
      " |  run_single(self, inputs, preprocess_params, forward_params, postprocess_params)\n",
      " |  \n",
      " |  save_pretrained(self, save_directory: str, safe_serialization: bool = True)\n",
      " |      Save the pipeline's model and tokenizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          save_directory (`str`):\n",
      " |              A path to the directory where to saved. It will be created if it doesn't exist.\n",
      " |          safe_serialization (`str`):\n",
      " |              Whether to save the model using `safetensors` or the traditional way for PyTorch or Tensorflow.\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Scikit / Keras interface to transformers' pipelines. This method will forward to __call__().\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from transformers.pipelines.base.Pipeline:\n",
      " |  \n",
      " |  default_input_names = None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from transformers.pipelines.base._ScikitCompat:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(google_vit_pipe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
